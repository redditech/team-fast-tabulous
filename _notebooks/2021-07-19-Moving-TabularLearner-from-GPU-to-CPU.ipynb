{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ffc481f",
   "metadata": {},
   "source": [
    "# Moving TabularLearner from GPU to CPU\n",
    "> Once the TabularLearner which contains a TabularModel has been trained on a GPU, we no longer require the GPU as predictions work on much smaller amounts of data. However, moving the TabularLearner to a CPU is not straightforward. This post shows you how.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- author: Tim Cummings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c8682f",
   "metadata": {},
   "source": [
    "## Creating TabularLearner on GPU\n",
    "You may recall from https://redditech.github.io/team-fast-tabulous/kaggle/fastai/2021/07/08/HomeSite-Quote-A-Fastai-Tabular-Approach.html that we created a TabularLearner using the following steps\n",
    "\n",
    "    df = pd.read_csv('train.csv', ...)  # plus some EDA\n",
    "    y_names = 'QuoteConversion_Flag'\n",
    "    cont_names, cat_names = cont_cat_split(df, dep_var=y_names)\n",
    "    # create TabularPandas\n",
    "    to = TabularPandas(df, procs, cat_names, cont_names, y_names, y_block, splits)\n",
    "    # create DataLoaders\n",
    "    dls = to.dataloaders(bs=4096, val_bs=512, layers=[10000,500], embed_ps=0.02, ps=[0.001,0.01])\n",
    "    # create TabularLearner \n",
    "    learn = tabular_learner(dls, metrics=RocAucBinary())\n",
    "    # train the TabularLearner\n",
    "    learn.fit_one_cycle(n_epoch=5, lr_max=1e-2, wd=0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3875971",
   "metadata": {},
   "source": [
    "## Moving TabularLearner from GPU to GPU\n",
    "Moving the TabularLearner from one GPU to another GPU was easy\n",
    "\n",
    "    # GPU 1\n",
    "    save_pickle(\"learner.pkl\", learn)\n",
    "    \n",
    "    # GPU 2\n",
    "    learn = load_pickle(\"learner.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a90cdf",
   "metadata": {},
   "source": [
    "## Moving TabularLearner from GPU to CPU\n",
    "However `load_pickle(\"learner.pkl\")` on a CPU will raise an exception because the pickle file is of a `TabularLearner` created for a GPU. The solution is to rebuild the `TabularLearner` on the CPU from the `DataLoaders` and the `TabularModel`. But first you have to convert the `DataLoaders` and the `TabularModel` to CPU versions and you have to do that while on the GPU or they won't load on the CPU. Use the `to()` method on both objects which converts in place and returns the converted object.\n",
    "\n",
    "    # GPU\n",
    "    save_pickle(\"dataloaders_cpu.pkl\", learn.dls.to(\"cpu\"))\n",
    "    save_pickle(\"tabularmodel_cpu.pkl\", learn.model.to(\"cpu\"))\n",
    "    \n",
    "    # CPU\n",
    "    dls = load_pickle(\"dataloaders_cpu.pkl\")\n",
    "    mdl = load_pickle(\"tabularmodel_cpu.pkl\")\n",
    "    learn = TabularLearner(dls=dls, model=mdl)\n",
    "\n",
    "To check it loaded correctly on CPU, calculate some predictions and calculate the roc_auc_score\n",
    "\n",
    "    preds, targs = learn.get_preds()\n",
    "    print(f\"Trained deep learning model has a roc_auc_score of {roc_auc_score(to_np(targs), to_np(preds[:,1]))}\")\n",
    "\n",
    "And if correct it should print the same roc_auc_score calculated on the GPU.\n",
    "\n",
    "    Trained deep learning model has a roc_auc_score of 0.9630509311593953"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f5d5f9",
   "metadata": {},
   "source": [
    "## Alternative\n",
    "If you don't have your dataloaders converted to CPU and then pickled, but you do have your TabularPandas pickled, then because TabularPandas doesn't need to be converted to CPU you can do the following\n",
    "\n",
    "    # GPU\n",
    "    save_pickle(\"tabularpandas.pkl\", to)\n",
    "    save_pickle(\"tabularmodel_cpu.pkl\", learn.model.to(\"cpu\"))\n",
    "    \n",
    "    # CPU\n",
    "    to = load_pickle(\"tabularpandas.pkl\")\n",
    "    dls = to.dataloaders(bs=4096, val_bs=512, layers=[10000,500], embed_ps=0.02, ps=[0.001,0.01])\n",
    "    mdl = load_pickle(\"tabularmodel_cpu.pkl\")\n",
    "    learn = TabularLearner(dls=dls, model=mdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78196eba",
   "metadata": {},
   "source": [
    "## Lightweight Alternative\n",
    "If you don't need the original DataLoaders in your model you can export the model with an empty DataLoaders. This has the advantage of producing a very compact exported model in one step on the GPU.\n",
    "\n",
    "    # GPU\n",
    "    learn.export(fname=\"learn_empty_dls.pkl\")\n",
    "    \n",
    "    # CPU\n",
    "    learn = load_learner(\"learn_empty_dls.pkl\")\n",
    "\n",
    "The disadvantage is you can't test that the model came across properly using `learn.get_preds()` with no parameters, because it has no DataLoaders. However, you can still do inference using the following:\n",
    "\n",
    "    # Pandas Series of independent variables (e.g. sr_row)\n",
    "    pred = learn.predict(sr_row)\n",
    "    \n",
    "    # Pandas DataFrame with many rows of independent variables (e.g. df_rows)\n",
    "    dl = learn.dls.test_dl(df_rows)\n",
    "    dl.dataset.conts = dl.dataset.conts.astype(np.float32)\n",
    "    inp,preds,_,dec_preds = learn.get_preds(dl=dl, with_input=True, with_decoded=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d78990",
   "metadata": {},
   "source": [
    "## Alternatives which don't work\n",
    "The error message when loading a GPU TabularLearner on a CPU is:\n",
    "\n",
    "    RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
    "\n",
    "The error message suggests that there is something that can be done on the CPU which can solve the problem. However I tried doing what it suggested.\n",
    "\n",
    "    # GPU \n",
    "    save_pickle(\"tabularlearner.pkl\", learn)\n",
    "    save_pickle(\"tabularpandas.pkl\", to)\n",
    "    \n",
    "    # CPU\n",
    "    learn = load_pickle(\"tabularlearner.pkl\")  # RuntimeError above\n",
    "    learn = torch.load(\"tabularlearner.pkl\", map_location=torch.device(\"cpu\"))  # RuntimeError above\n",
    "\n",
    "Using the lightweight alternative described above `load_learner(filepath)` with a TabularLearner saved using `save_pickle` also produces the same error.\n",
    "\n",
    "Another suggestion was using `learn.save(\"learn_save\")` on the GPU which saves a file called `learn_save.pth`. This file can be loaded on CPU with `torch.load(\"learn_save.pth\", map_location=torch.device(\"cpu\"))` but it just gives a OrderDict with keys [\"model\", \"opt\"]. Looking online suggested I save the model state dict and use `load_state-dict` method on an empty model to load it back in. It looks like `learn_save.pth` maybe such a state dict but I didn't know how to create an empty `TabularModel` to call method `load_state_dict` on.\n",
    "\n",
    "Finally I thought that because the `to(\"cpu\")` method works in place, I could convert `learn` to CPU version on GPU and only need to pickle one file. However none of the following attempts on the GPU created file which could be loaded on CPU.\n",
    "\n",
    "    # Failure 1\n",
    "    learn.to(\"cpu\")\n",
    "    save_pickle(\"fail.pkl\", learn)\n",
    "    \n",
    "    # Failure 2\n",
    "    learn.model.to(\"cpu\")\n",
    "    learn.dls.to(\"cpu\")\n",
    "    save_pickle(\"fail.pkl\", learn)\n",
    "    \n",
    "    # Failure 3\n",
    "    learn.model = learn.model.to(\"cpu\")\n",
    "    learn.dls = learn.dls.to(\"cpu\")\n",
    "    save_pickle(\"fail.pkl\", learn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7685e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
