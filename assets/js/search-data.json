{
  
    
        "post0": {
            "title": "First Pass Using Fastai Tabular For Homesite Competition",
            "content": "Introduction . Here I will submit an entry to the Homesite Competition on Kaggle Competition using Google Colab to get a first pass submission going, but mainly ignore what was discovered in the initial exploratory data analysis at this time, since we want a baseline for comparison after applying what we learn there to see how it improves (or not) our submission then. . Setup fastai and Google drive . !pip install -Uqq fastai . from fastai.tabular.all import * . The snippet below is only useful in Colab for accessing my Google Drive and is straight out the fastbook source code in Github . global gdrive gdrive = Path(&#39;/content/gdrive/My Drive&#39;) from google.colab import drive if not gdrive.exists(): drive.mount(str(gdrive.parent)) . Only add the Kaggle bits below if I&#39;m running locally, in Collab they&#39;re already here . . !ls /content/gdrive/MyDrive/Kaggle/kaggle.json . /content/gdrive/MyDrive/Kaggle/kaggle.json . Useful links here: . Documentation on Path library | Documentation on fastai extensions to Path library | . Path.cwd() . Path(&#39;/content&#39;) . Setup kaggle environment parameters . !mkdir -p ~/.kaggle !cp /content/gdrive/MyDrive/Kaggle/kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . from kaggle import api . path = Path.cwd() path.ls() . (#4) [Path(&#39;/content/.config&#39;),Path(&#39;/content/models&#39;),Path(&#39;/content/gdrive&#39;),Path(&#39;/content/sample_data&#39;)] . path = path/&quot;gdrive/MyDrive/Kaggle/homesite_data&quot; path.mkdir(exist_ok=True) Path.BASE_PATH = path api.competition_download_cli(&#39;homesite-quote-conversion&#39;, path=path) file_extract(path/&quot;homesite-quote-conversion.zip&quot;) file_extract(path/&quot;train.csv.zip&quot;) file_extract(path/&quot;test.csv.zip&quot;) . homesite-quote-conversion.zip: Skipping, found more recently modified local copy (use --force to force download) . path . Path(&#39;.&#39;) . path.ls() . (#9) [Path(&#39;homesite-quote-conversion.zip&#39;),Path(&#39;models&#39;),Path(&#39;sample_submission.csv.zip&#39;),Path(&#39;test.csv.zip&#39;),Path(&#39;train.csv.zip&#39;),Path(&#39;train.csv&#39;),Path(&#39;test.csv&#39;),Path(&#39;sample_submission.csv&#39;),Path(&#39;submission.csv&#39;)] . Exploring the Homesite data . First set the random seed so that the results are reproducible . set_seed(42) . df_train = pd.read_csv(path/&quot;train.csv&quot;, low_memory=False) df_train.head() . QuoteNumber Original_Quote_Date QuoteConversion_Flag Field6 Field7 Field8 Field9 Field10 Field11 Field12 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField3 SalesField4 SalesField5 SalesField6 SalesField7 SalesField8 SalesField9 SalesField10 SalesField11 SalesField12 ... GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56A GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60A GeographicField60B GeographicField61A GeographicField61B GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 1 | 2013-08-16 | 0 | B | 23 | 0.9403 | 0.0006 | 965 | 1.0200 | N | 17 | 23 | 17 | 23 | 15 | 22 | 16 | 22 | 13 | 22 | 13 | 23 | T | D | 2 | 1 | 7 | 18 | 3 | 8 | 0 | 5 | 5 | 24 | V | 48649 | 0 | 0 | 0 | 0 | ... | 8 | 4 | 20 | 22 | 10 | 8 | 6 | 5 | 15 | 13 | 19 | 18 | 16 | 14 | 21 | 23 | 21 | 23 | 16 | 11 | 22 | 24 | 7 | 14 | -1 | 17 | 15 | 17 | 14 | 18 | 9 | 9 | -1 | 8 | -1 | 18 | -1 | 10 | N | CA | . 1 2 | 2014-04-22 | 0 | F | 7 | 1.0006 | 0.0040 | 548 | 1.2433 | N | 6 | 8 | 6 | 8 | 5 | 7 | 5 | 8 | 13 | 22 | 13 | 23 | T | E | 5 | 9 | 5 | 14 | 6 | 18 | 1 | 5 | 5 | 11 | P | 26778 | 0 | 0 | 1 | 1 | ... | 23 | 24 | 11 | 15 | 21 | 24 | 6 | 11 | 21 | 21 | 18 | 15 | 20 | 20 | 13 | 12 | 12 | 12 | 15 | 9 | 13 | 11 | 11 | 20 | -1 | 9 | 18 | 21 | 8 | 7 | 10 | 10 | -1 | 11 | -1 | 17 | -1 | 20 | N | NJ | . 2 4 | 2014-08-25 | 0 | F | 7 | 1.0006 | 0.0040 | 548 | 1.2433 | N | 7 | 12 | 7 | 12 | 6 | 10 | 7 | 11 | 25 | 25 | 13 | 23 | T | J | 4 | 6 | 3 | 10 | 4 | 11 | 1 | 5 | 5 | 11 | K | 8751 | 0 | 0 | 2 | 2 | ... | 21 | 22 | 24 | 25 | 20 | 22 | 7 | 13 | 23 | 23 | 20 | 19 | 20 | 20 | 18 | 20 | 19 | 21 | 20 | 19 | 11 | 8 | 3 | 3 | -1 | 5 | 21 | 24 | 12 | 15 | 15 | 18 | -1 | 21 | -1 | 11 | -1 | 8 | N | NJ | . 3 6 | 2013-04-15 | 0 | J | 10 | 0.9769 | 0.0004 | 1,165 | 1.2665 | N | 3 | 2 | 3 | 2 | 2 | 2 | 3 | 2 | 13 | 22 | 13 | 23 | Y | F | 15 | 23 | 8 | 19 | 14 | 24 | 0 | 5 | 5 | 23 | V | 43854 | 0 | 0 | 0 | 0 | ... | 3 | 1 | 14 | 22 | 6 | 2 | 7 | 14 | 11 | 8 | 19 | 18 | 18 | 16 | 13 | 12 | 13 | 12 | 17 | 13 | 5 | 2 | 3 | 4 | -1 | 7 | 14 | 14 | 14 | 18 | 6 | 5 | -1 | 10 | -1 | 9 | -1 | 21 | N | TX | . 4 8 | 2014-01-25 | 0 | E | 23 | 0.9472 | 0.0006 | 1,487 | 1.3045 | N | 8 | 13 | 8 | 13 | 7 | 11 | 7 | 13 | 13 | 22 | 13 | 23 | T | F | 4 | 6 | 3 | 6 | 3 | 6 | 1 | 5 | 5 | 7 | R | 12505 | 1 | 0 | 0 | 0 | ... | 24 | 25 | 9 | 11 | 25 | 25 | 5 | 3 | 22 | 22 | 21 | 21 | 17 | 15 | 25 | 25 | 25 | 25 | 17 | 13 | 13 | 11 | 3 | 4 | -1 | 7 | 11 | 9 | 10 | 10 | 18 | 22 | -1 | 10 | -1 | 11 | -1 | 12 | N | IL | . 5 rows × 299 columns . df_train.shape . (260753, 299) . df_test = pd.read_csv(path/&quot;test.csv&quot;, low_memory=False) df_test.head() . QuoteNumber Original_Quote_Date Field6 Field7 Field8 Field9 Field10 Field11 Field12 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField3 SalesField4 SalesField5 SalesField6 SalesField7 SalesField8 SalesField9 SalesField10 SalesField11 SalesField12 SalesField13 ... GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56A GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60A GeographicField60B GeographicField61A GeographicField61B GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 3 | 2014-08-12 | E | 16 | 0.9364 | 0.0006 | 1,487 | 1.3045 | N | 4 | 4 | 4 | 4 | 3 | 3 | 3 | 4 | 13 | 22 | 13 | 23 | Y | K | 13 | 22 | 6 | 16 | 9 | 21 | 0 | 5 | 5 | 11 | P | 67052 | 0 | 0 | 0 | 0 | 0 | ... | 22 | 23 | 9 | 12 | 25 | 25 | 6 | 9 | 4 | 2 | 16 | 12 | 20 | 20 | 2 | 2 | 2 | 1 | 1 | 1 | 10 | 7 | 25 | 25 | -1 | 19 | 19 | 22 | 12 | 15 | 1 | 1 | -1 | 1 | -1 | 20 | -1 | 25 | Y | IL | . 1 5 | 2013-09-07 | F | 11 | 0.9919 | 0.0038 | 564 | 1.1886 | N | 8 | 14 | 8 | 14 | 7 | 12 | 8 | 13 | 13 | 22 | 13 | 23 | T | E | 4 | 5 | 3 | 6 | 3 | 6 | 1 | 5 | 5 | 4 | R | 27288 | 1 | 0 | 0 | 0 | 0 | ... | 23 | 24 | 12 | 21 | 23 | 25 | 7 | 11 | 16 | 14 | 13 | 6 | 17 | 15 | 7 | 5 | 7 | 5 | 13 | 7 | 14 | 14 | 7 | 14 | -1 | 4 | 1 | 1 | 5 | 3 | 10 | 10 | -1 | 5 | -1 | 5 | -1 | 21 | N | NJ | . 2 7 | 2013-03-29 | F | 15 | 0.8945 | 0.0038 | 564 | 1.0670 | N | 11 | 18 | 11 | 18 | 10 | 16 | 10 | 18 | 13 | 22 | 13 | 23 | T | E | 3 | 3 | 5 | 14 | 3 | 9 | 1 | 5 | 5 | 23 | V | 65264 | 0 | 1 | 2 | 2 | 0 | ... | 16 | 18 | 9 | 10 | 14 | 16 | 6 | 8 | 20 | 19 | 17 | 14 | 16 | 13 | 20 | 22 | 20 | 22 | 20 | 18 | 10 | 7 | 4 | 7 | -1 | 11 | 13 | 12 | 18 | 22 | 10 | 11 | -1 | 20 | -1 | 22 | -1 | 11 | N | NJ | . 3 9 | 2015-03-21 | K | 21 | 0.8870 | 0.0004 | 1,113 | 1.2665 | Y | 14 | 22 | 15 | 22 | 13 | 20 | 22 | 25 | 13 | 22 | 13 | 23 | Y | F | 5 | 9 | 9 | 20 | 5 | 16 | 1 | 5 | 5 | 11 | R | 32725 | 1 | 1 | 1 | 1 | 0 | ... | 11 | 11 | 9 | 10 | 11 | 13 | 15 | 21 | 14 | 12 | 17 | 13 | 10 | 6 | 20 | 22 | 20 | 22 | 19 | 16 | 12 | 11 | 4 | 6 | -1 | 13 | 10 | 8 | 5 | 3 | 8 | 8 | -1 | 13 | -1 | 8 | -1 | 21 | N | TX | . 4 10 | 2014-12-10 | B | 25 | 0.9153 | 0.0007 | 935 | 1.0200 | N | 4 | 5 | 4 | 5 | 4 | 4 | 4 | 5 | 13 | 22 | 13 | 23 | Y | D | 12 | 21 | 1 | 1 | 3 | 6 | 0 | 5 | 5 | 11 | T | 56025 | 0 | 1 | 1 | 1 | 0 | ... | 9 | 8 | 25 | 25 | 9 | 3 | 9 | 18 | 7 | 4 | 16 | 12 | 13 | 9 | 8 | 6 | 8 | 6 | 11 | 5 | 19 | 21 | 13 | 21 | -1 | 23 | 11 | 8 | 5 | 3 | 7 | 7 | -1 | 3 | -1 | 22 | -1 | 21 | N | CA | . 5 rows × 298 columns . df_test.shape . (173836, 298) . y_column = df_train.columns.difference(df_test.columns) . y_column . Index([&#39;QuoteConversion_Flag&#39;], dtype=&#39;object&#39;) . From this it looks like QuoteConversion_Flag is the value we want to predict. Let&#39;s take a look at this . type(df_train.QuoteConversion_Flag) . pandas.core.series.Series . df_train.QuoteConversion_Flag.unique() . array([0, 1]) . type(df_train.QuoteConversion_Flag.unique()[0]) . numpy.int64 . Make this a boolean for the purpose of generating predictions as a binary classification . df_train.QuoteConversion_Flag = df_train.QuoteConversion_Flag.astype(dtype=&#39;boolean&#39;) . Let&#39;s see how the training data outcomes are balanced . df_train.QuoteConversion_Flag.describe() . count 260753 unique 2 top False freq 211859 Name: QuoteConversion_Flag, dtype: object . train_data_balance = pd.DataFrame(df_train[&quot;QuoteConversion_Flag&quot;]).groupby(&quot;QuoteConversion_Flag&quot;) . train_data_balance[&quot;QuoteConversion_Flag&quot;].describe() . count unique top freq . QuoteConversion_Flag . False 211859 | 1 | False | 211859 | . True 48894 | 1 | True | 48894 | . We have about 5 times as many &quot;No Sale&quot; data rows as we do data that shows a successful sale happened. This data bias may have an impact on the effectiveness of our model to predict positive sales results . First things first . Learning from my colleague Tim&#39;s work already we know: . Quotenumber is unique so we can make it the index | Original_Quote_Date column should be set as a date type | . Additionally, we should make sure to apply any changes to data types to both train and test data so predictions don&#39;t fail later on . df_train = df_train.set_index(&#39;QuoteNumber&#39;) df_test = df_test.set_index(&#39;QuoteNumber&#39;) . We may have some NaN values for Original_Quote_Date in either the training or test dataset, but let&#39;s confirm there are none. . df_train[&#39;Original_Quote_Date&#39;].isna().sum(), df_test[&#39;Original_Quote_Date&#39;].isna().sum() . (0, 0) . df_train[&#39;Original_Quote_Date&#39;] = pd.to_datetime(df_train[&#39;Original_Quote_Date&#39;]) df_test[&#39;Original_Quote_Date&#39;] = pd.to_datetime(df_test[&#39;Original_Quote_Date&#39;]) . Goal: Quick and dirty model training, assuming fastai defaults for everything . y_names = [y_column[0]] y_names . [&#39;QuoteConversion_Flag&#39;] . cont_names, cat_names = cont_cat_split(df_train, dep_var=y_names) len(cont_names), len(cat_names) . (151, 146) . &quot;QuoteConversion_Flag&quot; in cont_names, &quot;QuoteConversion_Flag&quot; in cat_names #Make sure we&#39;ve gotten our y-column excluded . (False, False) . procs = [Categorify, FillMissing, Normalize] splits = RandomSplitter()(range_of(df_train)) . to = TabularPandas(df=df_train, procs=procs, cat_names=cat_names, cont_names=cont_names, y_names=y_names, splits=splits) dls = to.dataloaders() dls.valid.show_batch() . Original_Quote_Date Field6 Field10 Field12 CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 SalesField3 SalesField4 SalesField5 SalesField7 SalesField9 SalesField10 SalesField11 SalesField13 SalesField14 SalesField15 PersonalField1 PersonalField2 PersonalField5 PersonalField6 PersonalField7 PersonalField8 PersonalField9 PersonalField11 PersonalField12 PersonalField13 PersonalField16 PersonalField17 PersonalField18 PersonalField19 PersonalField22 PersonalField23 PersonalField24 PersonalField25 PersonalField26 PersonalField27 PersonalField28 PersonalField29 PersonalField30 PersonalField31 PersonalField32 PersonalField33 PersonalField34 PersonalField35 PersonalField36 PersonalField37 PersonalField38 PersonalField39 PersonalField40 PersonalField41 PersonalField42 PersonalField43 PersonalField44 PersonalField45 PersonalField46 PersonalField47 PersonalField48 PersonalField49 PersonalField50 PersonalField51 PersonalField52 PersonalField53 PersonalField54 PersonalField55 PersonalField56 PersonalField57 PersonalField58 PersonalField59 PersonalField60 PersonalField61 PersonalField62 PersonalField63 PersonalField64 PersonalField65 PersonalField66 PersonalField67 PersonalField68 PersonalField69 PersonalField70 PersonalField71 PersonalField72 PersonalField73 PersonalField74 PersonalField75 PersonalField76 PersonalField77 PersonalField78 PersonalField79 PersonalField80 PersonalField81 PersonalField82 PersonalField83 PropertyField2A PropertyField3 PropertyField4 PropertyField5 PropertyField6 PropertyField7 PropertyField8 PropertyField9 PropertyField10 PropertyField11A PropertyField11B PropertyField12 PropertyField13 PropertyField14 PropertyField15 PropertyField17 PropertyField18 PropertyField19 PropertyField20 PropertyField22 PropertyField23 PropertyField27 PropertyField28 PropertyField30 PropertyField31 PropertyField32 PropertyField33 PropertyField34 PropertyField35 PropertyField36 PropertyField37 PropertyField38 GeographicField5A GeographicField5B GeographicField10A GeographicField10B GeographicField14A GeographicField14B GeographicField18A GeographicField21A GeographicField22A GeographicField22B GeographicField23A GeographicField56A GeographicField60A GeographicField61A GeographicField62A GeographicField62B GeographicField63 GeographicField64 PersonalField84_na PropertyField29_na Field7 Field8 Field9 Field11 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField6 SalesField8 SalesField12 PersonalField4A PersonalField4B PersonalField10A PersonalField10B PersonalField14 PersonalField15 PersonalField84 PropertyField1A PropertyField1B PropertyField2B PropertyField16A PropertyField16B PropertyField21A PropertyField21B PropertyField24A PropertyField24B PropertyField25 PropertyField26A PropertyField26B PropertyField29 PropertyField39A PropertyField39B GeographicField1A GeographicField1B GeographicField2A GeographicField2B GeographicField3A GeographicField3B GeographicField4A GeographicField4B GeographicField6A GeographicField6B GeographicField7A GeographicField7B GeographicField8A GeographicField8B GeographicField9A GeographicField9B GeographicField11A GeographicField11B GeographicField12A GeographicField12B GeographicField13A GeographicField13B GeographicField15A GeographicField15B GeographicField16A GeographicField16B GeographicField17A GeographicField17B GeographicField18B GeographicField19A GeographicField19B GeographicField20A GeographicField20B GeographicField21B GeographicField23B GeographicField24A GeographicField24B GeographicField25A GeographicField25B GeographicField26A GeographicField26B GeographicField27A GeographicField27B GeographicField28A GeographicField28B GeographicField29A GeographicField29B GeographicField30A GeographicField30B GeographicField31A GeographicField31B GeographicField32A GeographicField32B GeographicField33A GeographicField33B GeographicField34A GeographicField34B GeographicField35A GeographicField35B GeographicField36A GeographicField36B GeographicField37A GeographicField37B GeographicField38A GeographicField38B GeographicField39A GeographicField39B GeographicField40A GeographicField40B GeographicField41A GeographicField41B GeographicField42A GeographicField42B GeographicField43A GeographicField43B GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60B GeographicField61B QuoteConversion_Flag . 0 2015-01-02 | J | 1,113 | N | 1 | 2 | 1 | 6 | X | C | 0 | 5 | 5 | V | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | XB | YJ | ZP | YJ | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | J | 1 | 0 | 1 | -1 | 21 | 4 | 2 | C | 2 | 1 | 2 | 0 | 0 | 2 | 1 | 6 | B | Y | K | Y | G | N | 2 | N | N | N | -1 | 13 | -1 | 25 | -1 | 24 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 15 | N | TX | True | True | 26.0 | 0.8870 | 0.0004 | 1.2665 | 5.0 | 7.0 | 5.0 | 7.0 | 5.000000 | 6.0 | 5.0 | 7.0 | 25.0 | 25.0 | 25.0 | 25.0 | 25.0 | 25.0 | 11.000000 | 6253.999581 | 1.000000e+00 | 8.0 | 10.0 | 6.0 | 10.0 | 2.0 | 19.0 | 2.0 | 8.000000 | 12.0 | 24.0 | 3.0 | 4.0 | 5.0 | 7.0 | 6.0 | 9.0 | 1.0 | 12.000000 | 18.0 | -7.737822e-13 | 13.0 | 16.0 | 8.0 | 17.0 | -1.0 | -1.0 | 6.0 | 6.0 | 18.0 | 20.0 | 23.0 | 24.0 | 24.0 | 25.0 | 23.0 | 24.0 | 20.0 | 23.0 | 24.999999 | 25.0 | 25.0 | 25.0 | 25.0 | 25.0 | 24.0 | 24.0 | 19.0 | 24.0 | 2.000000 | 2.0 | 9.0 | 11.0 | 13.0 | 5.0 | 15.0 | 14.0 | 22.0 | 24.0 | 25.0 | 19.000000 | 21.0 | 10.0 | 14.0 | 24.0 | 25.0 | 5.0 | 7.0 | 4.0 | 3.0 | 6.0 | 7.0 | 5.0 | 4.0 | 12.0 | 20.0 | 4.000000 | 3.0 | 7.0 | 6.0 | 3.0 | 3.0 | 5.0 | 4.0 | 5.0 | 11.0 | 4.0 | 3.0 | 4.0 | 5.0 | 15.0 | 23.0 | 14.0 | 10.0 | 9.0 | 10.0 | 11.0 | 15.0 | 11.0 | 11.0 | 9.0 | 11.0 | 12.0 | 14.0 | 13.0 | 21.0 | -1.0 | -1.0 | -1.0 | -1.0 | -1.000000 | -1.0 | -1.0 | -1.0 | -1.000001 | -1.0 | -1.0 | -1.0 | -1.0 | -1.0 | -1.0 | -1.0 | -1.0 | 13.0 | 13.0 | 9.000000 | 8.0 | 6.0 | 4.0 | 5.0 | 12.0 | False | . 1 2013-05-21 | B | 965 | N | 13 | 22 | 13 | 23 | T | D | 0 | 5 | 5 | P | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | D | 0 | 0 | 1 | -1 | 21 | 2 | 2 | C | 4 | 4 | 3 | 0 | 0 | 2 | 1 | 13 | B | N | K | N | G | Y | 2 | N | N | N | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 24 | N | CA | False | False | 23.0 | 0.9403 | 0.0006 | 1.0200 | 25.0 | 25.0 | 25.0 | 25.0 | 22.999999 | 25.0 | 25.0 | 25.0 | 1.0 | 1.0 | 17.0 | 24.0 | 5.0 | 17.0 | 15.000000 | 8001.001088 | 2.523887e-08 | -1.0 | -1.0 | 8.0 | 16.0 | 4.0 | 24.0 | 2.0 | 15.000000 | 20.0 | 12.0 | 24.0 | 25.0 | 25.0 | 25.0 | 15.0 | 21.0 | 3.0 | 5.000000 | 5.0 | -7.737822e-13 | 1.0 | 1.0 | 8.0 | 17.0 | 25.0 | 25.0 | 19.0 | 21.0 | 10.0 | 9.0 | 2.0 | 6.0 | 4.0 | 6.0 | 2.0 | 6.0 | 2.0 | 8.0 | 2.000000 | 4.0 | 3.0 | 5.0 | 2.0 | 5.0 | 6.0 | 6.0 | 2.0 | 6.0 | 21.000000 | 25.0 | 21.0 | 3.0 | 5.0 | 4.0 | 9.0 | 9.0 | 7.0 | 17.0 | 16.0 | 19.000000 | 22.0 | 16.0 | 19.0 | 21.0 | 20.0 | 25.0 | 25.0 | 22.0 | 25.0 | 9.0 | 15.0 | 12.0 | 19.0 | 6.0 | 10.0 | 20.999999 | 24.0 | 25.0 | 25.0 | 17.0 | 24.0 | 11.0 | 17.0 | 4.0 | 9.0 | 7.0 | 8.0 | 9.0 | 17.0 | 2.0 | 5.0 | 14.0 | 11.0 | 7.0 | 8.0 | 5.0 | 5.0 | 7.0 | 3.0 | 22.0 | 23.0 | 10.0 | 10.0 | 9.0 | 18.0 | 24.0 | 24.0 | 21.0 | 22.0 | 23.000000 | 24.0 | 24.0 | 25.0 | 25.000000 | 25.0 | 22.0 | 23.0 | 14.0 | 14.0 | 3.0 | 4.0 | 10.0 | 22.0 | 24.0 | 1.000000 | 1.0 | 14.0 | 18.0 | 23.0 | 2.0 | False | . 2 2013-12-26 | J | 1,165 | N | 13 | 22 | 13 | 23 | Y | A | 1 | 4 | 3 | Q | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | XQ | ZK | XF | ZN | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | O | 1 | 0 | 1 | -1 | 21 | 4 | 3 | C | 2 | 0 | 4 | 0 | 0 | 2 | 2 | 10 | B | N | M | Y | G | N | 2 | N | N | N | -1 | 23 | -1 | 25 | -1 | 10 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 21 | N | TX | True | True | 1.0 | 0.8922 | 0.0004 | 1.2665 | 11.0 | 19.0 | 11.0 | 19.0 | 15.000000 | 22.0 | 11.0 | 18.0 | 6.0 | 12.0 | 7.0 | 19.0 | 5.0 | 16.0 | 20.000000 | 42071.999796 | 2.523887e-08 | 8.0 | 11.0 | 7.0 | 14.0 | 1.0 | 10.0 | 2.0 | 2.000000 | 1.0 | 4.0 | 8.0 | 16.0 | 11.0 | 19.0 | 19.0 | 23.0 | 1.0 | 24.999999 | 25.0 | -7.737822e-13 | 18.0 | 22.0 | 3.0 | 6.0 | 10.0 | 7.0 | 11.0 | 12.0 | 20.0 | 22.0 | 3.0 | 10.0 | 23.0 | 23.0 | 6.0 | 16.0 | 6.0 | 14.0 | 3.000000 | 10.0 | 23.0 | 22.0 | 5.0 | 16.0 | 24.0 | 23.0 | 7.0 | 15.0 | 2.000000 | 9.0 | 13.0 | 7.0 | 9.0 | 7.0 | 17.0 | 5.0 | 10.0 | 21.0 | 23.0 | 19.000000 | 21.0 | 10.0 | 13.0 | 24.0 | 25.0 | 4.0 | 4.0 | 5.0 | 4.0 | 4.0 | 3.0 | 7.0 | 9.0 | 4.0 | 5.0 | 5.000000 | 4.0 | 3.0 | 2.0 | 4.0 | 4.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 1.0 | 4.0 | 5.0 | 5.0 | 10.0 | 17.0 | 15.0 | 7.0 | 7.0 | 10.0 | 14.0 | 9.0 | 6.0 | 1.0 | 1.0 | 4.0 | 1.0 | 6.0 | 5.0 | 13.0 | 11.0 | 22.0 | 23.0 | 23.000000 | 23.0 | 14.0 | 14.0 | 14.000000 | 15.0 | 19.0 | 16.0 | 1.0 | 1.0 | 5.0 | 9.0 | 5.0 | 10.0 | 7.0 | 16.000000 | 20.0 | 7.0 | 6.0 | 9.0 | 16.0 | False | . 3 2014-06-12 | E | 1,487 | N | 1 | 2 | 1 | 6 | T | F | 1 | 5 | 5 | Q | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 3 | 0 | 5 | 2 | XR | ZQ | ZW | XC | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | R | 0 | 0 | 1 | -1 | 21 | 3 | 1 | A | 9 | 1 | 1 | 0 | 0 | 2 | 1 | 4 | B | N | O | N | G | N | 0 | N | Y | N | -1 | 13 | -1 | 25 | -1 | 17 | -1 | -1 | 25 | 25 | -1 | -1 | -1 | -1 | -1 | 11 | N | IL | True | True | 16.0 | 0.9392 | 0.0006 | 1.3045 | 8.0 | 13.0 | 8.0 | 13.0 | 11.000000 | 18.0 | 7.0 | 12.0 | 4.0 | 6.0 | 4.0 | 11.0 | 4.0 | 12.0 | 11.000000 | 12451.999162 | 2.523887e-08 | 5.0 | 5.0 | -1.0 | -1.0 | 2.0 | 6.0 | 2.0 | 24.999999 | 25.0 | 12.0 | 5.0 | 10.0 | 8.0 | 13.0 | 5.0 | 6.0 | 2.0 | 3.000000 | 3.0 | -7.737822e-13 | 5.0 | 4.0 | 6.0 | 13.0 | 18.0 | 19.0 | 14.0 | 16.0 | 14.0 | 14.0 | 9.0 | 17.0 | 11.0 | 17.0 | 9.0 | 17.0 | 14.0 | 17.0 | 9.000000 | 17.0 | 13.0 | 17.0 | 9.0 | 17.0 | 9.0 | 12.0 | 7.0 | 16.0 | 2.000000 | 7.0 | 16.0 | 19.0 | 17.0 | 4.0 | 11.0 | 23.0 | 18.0 | 1.0 | 1.0 | 0.999999 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 5.0 | 6.0 | 8.0 | 10.0 | 6.0 | 9.0 | 13.0 | 20.0 | 9.0 | 15.0 | 7.000000 | 10.0 | 10.0 | 11.0 | 10.0 | 18.0 | 6.0 | 5.0 | 3.0 | 7.0 | 6.0 | 7.0 | 3.0 | 4.0 | 7.0 | 11.0 | 18.0 | 17.0 | 17.0 | 22.0 | 12.0 | 18.0 | 20.0 | 21.0 | 12.0 | 21.0 | 18.0 | 19.0 | 6.0 | 7.0 | 20.0 | 20.0 | 21.0 | 21.0 | 17.000000 | 14.0 | 20.0 | 22.0 | 20.000000 | 22.0 | 21.0 | 20.0 | 14.0 | 13.0 | 3.0 | 5.0 | 7.0 | 14.0 | 14.0 | 10.000000 | 11.0 | 15.0 | 18.0 | 19.0 | 7.0 | False | . 4 2013-09-05 | B | 965 | N | 13 | 22 | 1 | 6 | Y | E | 1 | 4 | 3 | Q | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | O | 1 | 0 | 1 | -1 | 21 | 4 | 2 | C | 4 | 1 | 3 | 0 | 0 | 2 | 4 | 4 | B | N | N | Y | H | Y | 1 | N | N | N | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 14 | N | CA | False | True | 25.0 | 0.9403 | 0.0006 | 1.0200 | 11.0 | 18.0 | 11.0 | 19.0 | 15.000000 | 22.0 | 11.0 | 18.0 | 6.0 | 12.0 | 6.0 | 16.0 | 4.0 | 13.0 | 21.000000 | 44150.000527 | 2.523887e-08 | 18.0 | 22.0 | 25.0 | 25.0 | 2.0 | 24.0 | 2.0 | 5.000000 | 7.0 | 19.0 | 10.0 | 19.0 | 11.0 | 18.0 | 12.0 | 18.0 | 2.0 | 8.000000 | 11.0 | -7.737822e-13 | 11.0 | 12.0 | 10.0 | 20.0 | 12.0 | 11.0 | 12.0 | 14.0 | 18.0 | 21.0 | 2.0 | 4.0 | 2.0 | 2.0 | 1.0 | 1.0 | 2.0 | 3.0 | 2.000000 | 6.0 | 1.0 | 1.0 | 2.0 | 2.0 | 1.0 | 2.0 | 1.0 | 2.0 | 22.000001 | 25.0 | 24.0 | 2.0 | 3.0 | 3.0 | 6.0 | 6.0 | 4.0 | 18.0 | 18.0 | 23.000000 | 24.0 | 22.0 | 24.0 | 22.0 | 22.0 | 9.0 | 14.0 | 15.0 | 22.0 | 5.0 | 6.0 | 9.0 | 13.0 | 25.0 | 25.0 | 10.000000 | 16.0 | 8.0 | 7.0 | 8.0 | 14.0 | 7.0 | 8.0 | 5.0 | 11.0 | 16.0 | 23.0 | 4.0 | 6.0 | 2.0 | 1.0 | 2.0 | 4.0 | 7.0 | 7.0 | 8.0 | 7.0 | 11.0 | 10.0 | 3.0 | 5.0 | 10.0 | 9.0 | 22.0 | 24.0 | 15.0 | 13.0 | 16.0 | 11.0 | 11.000000 | 6.0 | 20.0 | 22.0 | 19.000000 | 22.0 | 16.0 | 11.0 | 17.0 | 18.0 | 9.0 | 17.0 | 16.0 | 10.0 | 7.0 | 20.000000 | 23.0 | 8.0 | 8.0 | 10.0 | 5.0 | False | . 5 2014-05-13 | J | 1,113 | N | 13 | 22 | 25 | 25 | Y | A | 1 | 5 | 5 | P | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | Y | 1 | 2 | 0 | 1 | 2 | XS | XV | ZW | YN | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 25 | N | N | Y | 0 | I | 1 | 0 | 1 | -1 | 21 | 4 | 0 | B | 2 | 2 | 3 | 1 | 0 | 2 | 9 | 10 | B | N | O | N | H | N | 0 | N | N | N | -1 | 22 | -1 | 25 | -1 | 21 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | 25 | -1 | 16 | N | TX | True | True | 26.0 | 0.8928 | 0.0004 | 1.2665 | 13.0 | 21.0 | 13.0 | 21.0 | 12.000000 | 19.0 | 13.0 | 20.0 | 5.0 | 10.0 | 10.0 | 22.0 | 6.0 | 18.0 | 11.000000 | 61651.999632 | 2.523887e-08 | 9.0 | 12.0 | 17.0 | 22.0 | 5.0 | 10.0 | 2.0 | 11.000000 | 15.0 | 25.0 | 7.0 | 14.0 | 13.0 | 21.0 | 21.0 | 24.0 | 1.0 | 24.999999 | 25.0 | -7.737822e-13 | 17.0 | 21.0 | 2.0 | 1.0 | 8.0 | 6.0 | 14.0 | 16.0 | 6.0 | 3.0 | 14.0 | 22.0 | 23.0 | 23.0 | 15.0 | 22.0 | 20.0 | 22.0 | 15.000000 | 22.0 | 22.0 | 22.0 | 16.0 | 22.0 | 24.0 | 23.0 | 13.0 | 22.0 | 2.000000 | 2.0 | 2.0 | 10.0 | 12.0 | 2.0 | 3.0 | 4.0 | 23.0 | 24.0 | 25.0 | 15.000000 | 13.0 | 8.0 | 11.0 | 20.0 | 18.0 | 7.0 | 10.0 | 5.0 | 5.0 | 8.0 | 14.0 | 6.0 | 7.0 | 6.0 | 10.0 | 3.000000 | 2.0 | 10.0 | 11.0 | 3.0 | 3.0 | 12.0 | 19.0 | 1.0 | 1.0 | 9.0 | 14.0 | 10.0 | 19.0 | 10.0 | 17.0 | 8.0 | 6.0 | 13.0 | 17.0 | 3.0 | 4.0 | 9.0 | 8.0 | 11.0 | 16.0 | 16.0 | 18.0 | 10.0 | 18.0 | 9.0 | 6.0 | 18.0 | 16.0 | 18.000000 | 17.0 | 4.0 | 3.0 | 5.000000 | 3.0 | 20.0 | 17.0 | 11.0 | 8.0 | 5.0 | 9.0 | 19.0 | 13.0 | 12.0 | 25.000001 | 25.0 | 1.0 | 1.0 | 15.0 | 25.0 | False | . 6 2014-05-28 | J | 1,113 | N | 13 | 22 | 13 | 23 | T | F | 1 | 5 | 5 | P | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 3 | 0 | 5 | 2 | XO | XE | YF | XX | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | I | 1 | 0 | 1 | -1 | 21 | 4 | 2 | C | 1 | 0 | 2 | 1 | 0 | 2 | 1 | 17 | B | N | K | N | E | N | 0 | N | Y | N | -1 | 24 | -1 | 25 | -1 | 16 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 15 | N | TX | True | True | 26.0 | 0.8928 | 0.0004 | 1.2665 | 5.0 | 8.0 | 6.0 | 8.0 | 5.000000 | 7.0 | 5.0 | 7.0 | 5.0 | 9.0 | 11.0 | 22.0 | 13.0 | 24.0 | 11.000000 | 3548.000679 | 2.523887e-08 | 1.0 | 1.0 | -1.0 | -1.0 | 1.0 | 13.0 | 2.0 | 3.000000 | 3.0 | 10.0 | 7.0 | 15.0 | 5.0 | 8.0 | 5.0 | 7.0 | 3.0 | 1.000000 | 1.0 | -7.737822e-13 | 11.0 | 12.0 | 10.0 | 20.0 | 11.0 | 10.0 | 14.0 | 16.0 | 1.0 | 1.0 | 12.0 | 20.0 | 23.0 | 22.0 | 13.0 | 20.0 | 15.0 | 19.0 | 14.000000 | 20.0 | 22.0 | 22.0 | 14.0 | 20.0 | 23.0 | 20.0 | 8.0 | 18.0 | 2.000000 | 6.0 | 16.0 | 9.0 | 11.0 | 3.0 | 6.0 | 5.0 | 21.0 | 19.0 | 20.0 | 13.000000 | 10.0 | 7.0 | 10.0 | 16.0 | 14.0 | 8.0 | 13.0 | 11.0 | 17.0 | 8.0 | 14.0 | 6.0 | 7.0 | 3.0 | 2.0 | 13.000000 | 20.0 | 6.0 | 5.0 | 25.0 | 25.0 | 10.0 | 14.0 | 6.0 | 14.0 | 6.0 | 7.0 | 5.0 | 9.0 | 8.0 | 12.0 | 13.0 | 9.0 | 12.0 | 15.0 | 21.0 | 23.0 | 8.0 | 5.0 | 11.0 | 18.0 | 14.0 | 17.0 | 8.0 | 16.0 | 12.0 | 10.0 | 22.0 | 23.0 | 23.000000 | 24.0 | 15.0 | 15.0 | 16.000000 | 17.0 | 24.0 | 24.0 | 22.0 | 24.0 | 1.0 | 1.0 | 4.0 | 23.0 | 24.0 | 8.000000 | 8.0 | 6.0 | 4.0 | 23.0 | 2.0 | False | . 7 2013-07-12 | F | 564 | N | 13 | 22 | 13 | 23 | T | E | 1 | 5 | 5 | T | 0 | 1 | 4 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | ZT | XR | YQ | ZV | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | R | 1 | 0 | 1 | -1 | 21 | 1 | 1 | A | 4 | 0 | 1 | 1 | 0 | 2 | 1 | 4 | B | N | K | Y | E | Y | 1 | N | N | N | -1 | 17 | -1 | 25 | -1 | 15 | -1 | -1 | -1 | 17 | -1 | -1 | -1 | -1 | -1 | 21 | N | NJ | True | True | 11.0 | 0.9919 | 0.0038 | 1.1886 | 4.0 | 5.0 | 4.0 | 5.0 | 4.000000 | 4.0 | 4.0 | 5.0 | 6.0 | 11.0 | 4.0 | 11.0 | 5.0 | 17.0 | 0.999999 | 27491.000189 | 4.000000e+00 | 9.0 | 12.0 | 10.0 | 19.0 | 2.0 | 16.0 | 2.0 | 23.000000 | 24.0 | 10.0 | 4.0 | 6.0 | 4.0 | 5.0 | 4.0 | 5.0 | 2.0 | 3.000000 | 2.0 | -7.737822e-13 | 12.0 | 13.0 | 8.0 | 17.0 | 11.0 | 9.0 | 2.0 | 2.0 | 14.0 | 15.0 | 5.0 | 14.0 | 10.0 | 16.0 | 5.0 | 15.0 | 6.0 | 15.0 | 4.000000 | 15.0 | 10.0 | 16.0 | 5.0 | 16.0 | 11.0 | 17.0 | 6.0 | 15.0 | 2.000000 | 12.0 | 18.0 | 22.0 | 22.0 | 22.0 | 24.0 | 17.0 | 18.0 | 21.0 | 22.0 | 18.000000 | 19.0 | 13.0 | 17.0 | 22.0 | 21.0 | 8.0 | 13.0 | 6.0 | 6.0 | 12.0 | 21.0 | 5.0 | 5.0 | 10.0 | 16.0 | 7.000000 | 9.0 | 13.0 | 17.0 | 10.0 | 17.0 | 11.0 | 18.0 | 9.0 | 19.0 | 14.0 | 22.0 | 4.0 | 5.0 | 11.0 | 18.0 | 22.0 | 19.0 | 14.0 | 19.0 | 11.0 | 15.0 | 20.0 | 21.0 | 8.0 | 9.0 | 20.0 | 21.0 | 6.0 | 7.0 | 11.0 | 8.0 | 6.0 | 2.0 | 6.000000 | 3.0 | 9.0 | 7.0 | 9.000000 | 7.0 | 14.0 | 8.0 | 15.0 | 15.0 | 9.0 | 17.0 | 14.0 | 11.0 | 9.0 | 15.000000 | 19.0 | 4.0 | 3.0 | 1.0 | 21.0 | False | . 8 2014-01-27 | F | 548 | N | 13 | 22 | 13 | 23 | T | E | 0 | 5 | 5 | K | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 3 | 0 | 5 | 2 | XR | ZQ | ZW | XC | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | R | 1 | 0 | 1 | -1 | 21 | 4 | 1 | C | 1 | 1 | 2 | 5 | 0 | 2 | 1 | 15 | A | N | O | N | G | Y | 1 | N | Y | N | -1 | 18 | -1 | 25 | -1 | 13 | -1 | -1 | -1 | 19 | -1 | -1 | -1 | -1 | -1 | 9 | N | NJ | True | True | 11.0 | 0.9838 | 0.0040 | 1.1886 | 7.0 | 12.0 | 7.0 | 12.0 | 6.000000 | 10.0 | 7.0 | 11.0 | 4.0 | 6.0 | 3.0 | 7.0 | 3.0 | 8.0 | 7.000000 | 36990.000050 | 2.523887e-08 | 5.0 | 5.0 | -1.0 | -1.0 | 1.0 | 6.0 | 2.0 | 8.000000 | 12.0 | 24.0 | 6.0 | 11.0 | 7.0 | 12.0 | 5.0 | 6.0 | 2.0 | 4.000000 | 3.0 | -7.737822e-13 | 7.0 | 5.0 | 5.0 | 9.0 | 19.0 | 19.0 | 18.0 | 21.0 | 14.0 | 15.0 | 4.0 | 13.0 | 9.0 | 13.0 | 5.0 | 13.0 | 5.0 | 12.0 | 4.000000 | 15.0 | 8.0 | 13.0 | 5.0 | 14.0 | 9.0 | 11.0 | 5.0 | 12.0 | 2.000000 | 12.0 | 19.0 | 20.0 | 19.0 | 18.0 | 21.0 | 19.0 | 11.0 | 10.0 | 4.0 | 8.000000 | 3.0 | 3.0 | 2.0 | 11.0 | 3.0 | 4.0 | 4.0 | 7.0 | 8.0 | 6.0 | 6.0 | 9.0 | 13.0 | 8.0 | 13.0 | 4.000000 | 3.0 | 6.0 | 4.0 | 8.0 | 14.0 | 9.0 | 12.0 | 2.0 | 1.0 | 5.0 | 5.0 | 2.0 | 2.0 | 10.0 | 16.0 | 23.0 | 21.0 | 9.0 | 11.0 | 11.0 | 15.0 | 18.0 | 19.0 | 10.0 | 14.0 | 15.0 | 17.0 | 6.0 | 11.0 | 20.0 | 20.0 | 17.0 | 14.0 | 20.000000 | 20.0 | 14.0 | 14.0 | 14.000000 | 14.0 | 19.0 | 15.0 | 10.0 | 7.0 | 7.0 | 13.0 | 4.0 | 6.0 | 3.0 | 1.000000 | 1.0 | 12.0 | 15.0 | 21.0 | 5.0 | False | . 9 2013-01-15 | E | 1,480 | N | 13 | 22 | 25 | 25 | Y | A | 1 | 3 | 4 | P | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | XB | XV | YG | XX | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | J | 1 | 0 | 1 | -1 | 21 | 1 | 3 | C | 4 | 1 | 2 | 1 | 0 | 2 | 1 | 3 | B | N | O | Y | H | N | 2 | N | N | N | -1 | 13 | -1 | 25 | -1 | 22 | -1 | 25 | -1 | 24 | -1 | -1 | -1 | -1 | -1 | 24 | Y | IL | True | True | 8.0 | 0.9368 | 0.0006 | 1.2714 | 14.0 | 21.0 | 14.0 | 21.0 | 12.000000 | 20.0 | 13.0 | 21.0 | 5.0 | 9.0 | 6.0 | 17.0 | 3.0 | 11.0 | 23.000000 | 18181.999560 | 2.523887e-08 | 13.0 | 16.0 | 6.0 | 12.0 | 5.0 | 14.0 | 2.0 | 3.000000 | 3.0 | 12.0 | 8.0 | 17.0 | 14.0 | 21.0 | 17.0 | 22.0 | 2.0 | 11.000000 | 16.0 | -7.737822e-13 | 12.0 | 13.0 | 7.0 | 16.0 | 8.0 | 5.0 | 9.0 | 9.0 | 9.0 | 7.0 | 10.0 | 19.0 | 12.0 | 18.0 | 10.0 | 19.0 | 16.0 | 19.0 | 9.000000 | 18.0 | 13.0 | 18.0 | 10.0 | 18.0 | 10.0 | 15.0 | 11.0 | 22.0 | 2.000000 | 8.0 | 11.0 | 20.0 | 19.0 | 5.0 | 13.0 | 25.0 | 15.0 | 16.0 | 15.0 | 17.000000 | 17.0 | 10.0 | 13.0 | 21.0 | 20.0 | 3.0 | 2.0 | 5.0 | 5.0 | 4.0 | 3.0 | 10.0 | 15.0 | 4.0 | 5.0 | 10.000000 | 15.0 | 11.0 | 14.0 | 8.0 | 13.0 | 6.0 | 6.0 | 10.0 | 20.0 | 6.0 | 8.0 | 8.0 | 16.0 | 7.0 | 11.0 | 17.0 | 15.0 | 13.0 | 17.0 | 11.0 | 17.0 | 19.0 | 20.0 | 10.0 | 13.0 | 9.0 | 5.0 | 6.0 | 5.0 | 10.0 | 7.0 | 8.0 | 2.0 | 1.999999 | 1.0 | 25.0 | 25.0 | 25.000000 | 25.0 | 18.0 | 15.0 | 22.0 | 24.0 | 3.0 | 5.0 | 2.0 | 18.0 | 21.0 | 16.000000 | 21.0 | 18.0 | 22.0 | 10.0 | 8.0 | False | . roc_auc_binary = RocAucBinary() learn = tabular_learner(dls, metrics=roc_auc_binary) . type(roc_auc_binary) . fastai.metrics.AccumMetric . learn.lr_find() . SuggestedLRs(valley=tensor(0.0025)) . Reference to why we use fit_one_cycle . Note I ran fit_one_cycle with a value of 10 when prepping this notebook for publishing, but the test results came out suspiciously high on 1 outputs, given that the test submission I ran before was heavily weighted with 0 outputs and got a 0.83 score when I ran with 5 but I didn&#39;t set the random seed value then, so hopefully by setting the random seed this learning rate recommendation won&#39;t change when I run this notebook again later. . learn.fit_one_cycle(5, 0.0025) . epoch train_loss valid_loss roc_auc_score time . 0 | 0.191083 | 0.190373 | 0.956380 | 01:57 | . 1 | 0.182042 | 1.114478 | 0.961028 | 01:58 | . 2 | 0.170142 | 0.192774 | 0.961783 | 01:58 | . 3 | 0.160787 | 0.401845 | 0.963326 | 01:58 | . 4 | 0.153714 | 0.460429 | 0.962740 | 01:58 | . Referenced another Kaggle notebook for this, we don&#39;t need it but it&#39;s good to see what fastai metrics is actually packaging up for you . preds, targs = learn.get_preds() . from sklearn.metrics import roc_auc_score valid_score = roc_auc_score(to_np(targs), to_np(preds[:,1])) valid_score . 0.9627402127903286 . to_test = TabularPandas(df=df_test, procs=procs, cat_names=cat_names, cont_names=cont_names) test_dls = to_test.dataloaders() test_dls.show_batch() . Original_Quote_Date Field6 Field10 Field12 CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 SalesField3 SalesField4 SalesField5 SalesField7 SalesField9 SalesField10 SalesField11 SalesField13 SalesField14 SalesField15 PersonalField1 PersonalField2 PersonalField5 PersonalField6 PersonalField7 PersonalField8 PersonalField9 PersonalField11 PersonalField12 PersonalField13 PersonalField16 PersonalField17 PersonalField18 PersonalField19 PersonalField22 PersonalField23 PersonalField24 PersonalField25 PersonalField26 PersonalField27 PersonalField28 PersonalField29 PersonalField30 PersonalField31 PersonalField32 PersonalField33 PersonalField34 PersonalField35 PersonalField36 PersonalField37 PersonalField38 PersonalField39 PersonalField40 PersonalField41 PersonalField42 PersonalField43 PersonalField44 PersonalField45 PersonalField46 PersonalField47 PersonalField48 PersonalField49 PersonalField50 PersonalField51 PersonalField52 PersonalField53 PersonalField54 PersonalField55 PersonalField56 PersonalField57 PersonalField58 PersonalField59 PersonalField60 PersonalField61 PersonalField62 PersonalField63 PersonalField64 PersonalField65 PersonalField66 PersonalField67 PersonalField68 PersonalField69 PersonalField70 PersonalField71 PersonalField72 PersonalField73 PersonalField74 PersonalField75 PersonalField76 PersonalField77 PersonalField78 PersonalField79 PersonalField80 PersonalField81 PersonalField82 PersonalField83 PropertyField2A PropertyField3 PropertyField4 PropertyField5 PropertyField6 PropertyField7 PropertyField8 PropertyField9 PropertyField10 PropertyField11A PropertyField11B PropertyField12 PropertyField13 PropertyField14 PropertyField15 PropertyField17 PropertyField18 PropertyField19 PropertyField20 PropertyField22 PropertyField23 PropertyField27 PropertyField28 PropertyField30 PropertyField31 PropertyField32 PropertyField33 PropertyField34 PropertyField35 PropertyField36 PropertyField37 PropertyField38 GeographicField5A GeographicField5B GeographicField10A GeographicField10B GeographicField14A GeographicField14B GeographicField18A GeographicField21A GeographicField22A GeographicField22B GeographicField23A GeographicField56A GeographicField60A GeographicField61A GeographicField62A GeographicField62B GeographicField63 GeographicField64 PersonalField84_na PropertyField29_na Field7 Field8 Field9 Field11 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField6 SalesField8 SalesField12 PersonalField4A PersonalField4B PersonalField10A PersonalField10B PersonalField14 PersonalField15 PersonalField84 PropertyField1A PropertyField1B PropertyField2B PropertyField16A PropertyField16B PropertyField21A PropertyField21B PropertyField24A PropertyField24B PropertyField25 PropertyField26A PropertyField26B PropertyField29 PropertyField39A PropertyField39B GeographicField1A GeographicField1B GeographicField2A GeographicField2B GeographicField3A GeographicField3B GeographicField4A GeographicField4B GeographicField6A GeographicField6B GeographicField7A GeographicField7B GeographicField8A GeographicField8B GeographicField9A GeographicField9B GeographicField11A GeographicField11B GeographicField12A GeographicField12B GeographicField13A GeographicField13B GeographicField15A GeographicField15B GeographicField16A GeographicField16B GeographicField17A GeographicField17B GeographicField18B GeographicField19A GeographicField19B GeographicField20A GeographicField20B GeographicField21B GeographicField23B GeographicField24A GeographicField24B GeographicField25A GeographicField25B GeographicField26A GeographicField26B GeographicField27A GeographicField27B GeographicField28A GeographicField28B GeographicField29A GeographicField29B GeographicField30A GeographicField30B GeographicField31A GeographicField31B GeographicField32A GeographicField32B GeographicField33A GeographicField33B GeographicField34A GeographicField34B GeographicField35A GeographicField35B GeographicField36A GeographicField36B GeographicField37A GeographicField37B GeographicField38A GeographicField38B GeographicField39A GeographicField39B GeographicField40A GeographicField40B GeographicField41A GeographicField41B GeographicField42A GeographicField42B GeographicField43A GeographicField43B GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60B GeographicField61B . 0 2015-01-27 | C | 1,487 | Y | 25 | 25 | 13 | 23 | T | A | 1 | 3 | 4 | P | 0 | 1 | 2 | 0 | 1 | 1 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | XD | XS | YP | XC | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | R | 1 | 0 | 1 | -1 | 21 | 2 | 2 | C | 4 | 1 | 3 | 0 | 0 | 2 | 1 | 13 | B | N | O | Y | H | N | 2 | N | N | N | -1 | 13 | -1 | 25 | -1 | 22 | -1 | 25 | -1 | 24 | -1 | -1 | -1 | -1 | -1 | 8 | N | IL | True | False | 23.0 | 0.9108 | 0.0006 | 1.3045 | 7.0 | 11.0 | 7.0 | 11.0 | 10.0 | 16.0 | 11.0 | 19.0 | 4.000000 | 7.0 | 4.0 | 11.0 | 4.0 | 13.0 | 11.0 | 32021.999980 | 2.000000e+00 | 23.0 | 24.0 | 6.0 | 12.0 | 2.0 | 1.0 | 2.0 | 10.000000 | 14.0 | 17.0 | 5.0 | 10.0 | 7.0 | 11.0 | 8.0 | 11.0 | 2.0 | 5.0 | 5.0 | 3.896914e-12 | 12.0 | 13.0 | 7.0 | 15.0 | 12.0 | 10.0 | 9.0 | 10.0 | 21.0 | 23.0 | 10.000000 | 18.0 | 12.0 | 18.0 | 10.000000 | 18.0 | 15.000000 | 18.0 | 9.0 | 18.0 | 13.0 | 18.0 | 9.0 | 18.0 | 10.0 | 13.0 | 11.000000 | 20.0 | 2.0 | 8.0 | 18.0 | 20.0 | 19.0 | 4.0 | 11.0 | 25.0 | 15.0 | 16.000000 | 15.0 | 17.000000 | 17.0 | 10.0 | 13.0 | 21.000000 | 20.0 | 12.0 | 19.0 | 7.0 | 8.0 | 8.0 | 13.0 | 15.0 | 22.0 | 7.000000 | 11.0 | 7.0 | 9.0 | 11.0 | 13.0 | 11.0 | 20.0 | 10.0 | 14.0 | 3.0 | 7.0 | 9.0 | 13.0 | 13.0 | 22.0 | 9.0 | 13.0 | 18.0 | 18.0 | 16.0 | 21.0 | 11.0 | 17.0 | 17.0 | 19.0 | 10.0 | 14.0 | 18.0 | 19.0 | 6.0 | 7.0 | 14.0 | 12.0 | 16.0 | 11.0 | 8.0 | 4.0 | 16.0 | 17.0 | 15.0 | 16.0 | 15.0 | 9.0 | 10.0 | 7.0 | 9.0 | 17.0 | 10.0 | 14.0 | 15.0 | 16.000000 | 21.0 | 16.0 | 20.0 | 8.0 | 25.0 | . 1 2013-06-03 | J | 1,165 | N | 13 | 22 | 25 | 25 | X | G | 1 | 4 | 3 | V | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 6 | 1 | N | 1 | 2 | 0 | 1 | 2 | ZF | XS | YP | XC | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 1 | 0 | 0 | 0 | 0 | 3 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 3 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 3 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | D | 1 | 0 | 1 | -1 | 21 | 2 | 2 | A | 4 | 0 | 3 | 0 | 0 | 2 | 1 | 10 | B | N | K | Y | E | N | 1 | N | N | N | -1 | 16 | -1 | 25 | -1 | 23 | -1 | -1 | -1 | 15 | 25 | -1 | -1 | -1 | -1 | 22 | N | TX | False | True | 10.0 | 0.9691 | 0.0004 | 1.2665 | 7.0 | 12.0 | 7.0 | 12.0 | 7.0 | 10.0 | 7.0 | 12.0 | 21.999999 | 25.0 | 8.0 | 20.0 | 8.0 | 20.0 | 17.0 | 34853.000023 | -3.006702e-08 | 14.0 | 18.0 | 19.0 | 23.0 | 3.0 | 4.0 | 2.0 | 24.999999 | 25.0 | 4.0 | 3.0 | 5.0 | 7.0 | 12.0 | 16.0 | 21.0 | 1.0 | 25.0 | 25.0 | 3.896914e-12 | 23.0 | 25.0 | 2.0 | 2.0 | 5.0 | 3.0 | 1.0 | 1.0 | 19.0 | 22.0 | 20.000000 | 23.0 | 21.0 | 20.0 | 21.000000 | 23.0 | 20.000000 | 23.0 | 21.0 | 23.0 | 21.0 | 21.0 | 21.0 | 23.0 | 23.0 | 20.0 | 19.000000 | 23.0 | 2.0 | 5.0 | 4.0 | 13.0 | 15.0 | 3.0 | 7.0 | 15.0 | 25.0 | 16.000000 | 14.0 | 19.000000 | 23.0 | 8.0 | 11.0 | 25.000000 | 25.0 | 4.0 | 4.0 | 9.0 | 13.0 | 6.0 | 8.0 | 5.0 | 4.0 | 2.000000 | 2.0 | 3.0 | 2.0 | 6.0 | 4.0 | 3.0 | 2.0 | 11.0 | 17.0 | 3.0 | 4.0 | 1.0 | 1.0 | 4.0 | 5.0 | 7.0 | 11.0 | 14.0 | 12.0 | 8.0 | 9.0 | 9.0 | 10.0 | 12.0 | 12.0 | 10.0 | 13.0 | 6.0 | 2.0 | 7.0 | 13.0 | 5.0 | 2.0 | 18.0 | 16.0 | 16.0 | 14.0 | 3.0 | 2.0 | 3.0 | 2.0 | 23.0 | 24.0 | 1.0 | 1.0 | 7.0 | 15.0 | 24.0 | 18.0 | 21.0 | 10.000000 | 11.0 | 3.0 | 2.0 | 2.0 | 18.0 | . 2 2013-08-12 | B | 965 | N | 25 | 25 | 1 | 6 | T | D | 1 | 5 | 5 | K | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | N | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | Y | Y | 0 | O | 0 | 0 | 1 | -1 | 21 | 4 | 2 | C | 1 | 1 | 2 | 1 | 0 | 2 | 1 | 4 | B | N | M | Y | F | Y | 2 | N | N | N | -1 | 13 | -1 | 25 | -1 | 10 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 9 | N | CA | False | True | 24.0 | 0.9403 | 0.0006 | 1.0200 | 11.0 | 19.0 | 11.0 | 19.0 | 15.0 | 22.0 | 11.0 | 18.0 | 3.000000 | 3.0 | 3.0 | 8.0 | 2.0 | 3.0 | 21.0 | 44821.999987 | -3.006702e-08 | 14.0 | 18.0 | 7.0 | 16.0 | 4.0 | 24.0 | 2.0 | 17.000000 | 21.0 | 20.0 | 9.0 | 18.0 | 11.0 | 19.0 | 10.0 | 16.0 | 2.0 | 7.0 | 8.0 | 3.896914e-12 | 8.0 | 7.0 | 11.0 | 21.0 | 13.0 | 13.0 | 3.0 | 2.0 | 17.0 | 19.0 | 2.000000 | 9.0 | 8.0 | 9.0 | 2.000000 | 9.0 | 2.000000 | 4.0 | 2.0 | 9.0 | 5.0 | 8.0 | 2.0 | 9.0 | 10.0 | 15.0 | 3.000000 | 9.0 | 3.0 | 18.0 | 15.0 | 4.0 | 7.0 | 5.0 | 15.0 | 4.0 | 1.0 | 15.000000 | 13.0 | 16.000000 | 14.0 | 16.0 | 21.0 | 15.000000 | 10.0 | 11.0 | 18.0 | 6.0 | 5.0 | 6.0 | 8.0 | 5.0 | 4.0 | 6.000000 | 10.0 | 12.0 | 19.0 | 11.0 | 13.0 | 10.0 | 17.0 | 9.0 | 12.0 | 7.0 | 16.0 | 11.0 | 18.0 | 3.0 | 4.0 | 3.0 | 8.0 | 3.0 | 4.0 | 7.0 | 7.0 | 8.0 | 8.0 | 14.0 | 14.0 | 1.0 | 1.0 | 11.0 | 12.0 | 10.0 | 18.0 | 12.0 | 10.0 | 15.0 | 9.0 | 6.0 | 2.0 | 11.0 | 9.0 | 10.0 | 9.0 | 16.0 | 11.0 | 14.0 | 13.0 | 8.0 | 15.0 | 23.0 | 14.0 | 14.0 | 7.000000 | 6.0 | 10.0 | 11.0 | 6.0 | 2.0 | . 3 2013-09-09 | J | 1,165 | N | 13 | 22 | 13 | 23 | T | G | 1 | 5 | 5 | P | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | N | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | Y | Y | 0 | J | 0 | 0 | 1 | -1 | 21 | 4 | 2 | C | 1 | 1 | 2 | 1 | 0 | 2 | 1 | 10 | B | N | O | N | H | N | 0 | N | N | N | -1 | 14 | -1 | 25 | 25 | 25 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 19 | N | TX | False | False | 23.0 | 0.9497 | 0.0004 | 1.2665 | 11.0 | 18.0 | 11.0 | 18.0 | 10.0 | 16.0 | 10.0 | 18.0 | 3.000000 | 3.0 | 9.0 | 21.0 | 7.0 | 19.0 | 4.0 | 60604.999581 | -3.006702e-08 | 17.0 | 21.0 | 5.0 | 6.0 | 2.0 | 24.0 | 2.0 | 9.000000 | 13.0 | 15.0 | 4.0 | 7.0 | 11.0 | 18.0 | 11.0 | 17.0 | 1.0 | 19.0 | 23.0 | 3.896914e-12 | 9.0 | 10.0 | 4.0 | 8.0 | 17.0 | 17.0 | 24.0 | 25.0 | 6.0 | 3.0 | 24.000001 | 25.0 | 23.0 | 23.0 | 23.999999 | 25.0 | 24.999999 | 25.0 | 24.0 | 24.0 | 23.0 | 23.0 | 24.0 | 24.0 | 23.0 | 22.0 | 23.999999 | 25.0 | 2.0 | 5.0 | 13.0 | 11.0 | 14.0 | 6.0 | 16.0 | 15.0 | 22.0 | 11.000000 | 5.0 | 10.000000 | 4.0 | 5.0 | 4.0 | 13.000000 | 5.0 | 11.0 | 18.0 | 7.0 | 8.0 | 6.0 | 8.0 | 16.0 | 23.0 | 25.000001 | 25.0 | 10.0 | 15.0 | 12.0 | 15.0 | 4.0 | 5.0 | 10.0 | 14.0 | 13.0 | 23.0 | 10.0 | 15.0 | 6.0 | 12.0 | 17.0 | 23.0 | 18.0 | 16.0 | 16.0 | 22.0 | 15.0 | 22.0 | 17.0 | 18.0 | 10.0 | 13.0 | 13.0 | 15.0 | 12.0 | 19.0 | 18.0 | 17.0 | 20.0 | 20.0 | 17.0 | 16.0 | 17.0 | 18.0 | 17.0 | 19.0 | 21.0 | 20.0 | 3.0 | 1.0 | 4.0 | 6.0 | 4.0 | 15.0 | 17.0 | 6.000000 | 4.0 | 15.0 | 19.0 | 24.0 | 9.0 | . 4 2014-09-16 | B | 935 | N | 13 | 22 | 13 | 23 | Y | D | 1 | 5 | 5 | P | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | O | 0 | 0 | 1 | -1 | 21 | 4 | 0 | B | 4 | 0 | 3 | 0 | 0 | 2 | 1 | 6 | D | N | O | Y | G | Y | 2 | N | N | N | -1 | 13 | -1 | 25 | -1 | 10 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | 25 | 25 | N | CA | False | True | 25.0 | 0.9153 | 0.0007 | 1.0200 | 5.0 | 8.0 | 1.0 | 1.0 | 2.0 | 1.0 | 5.0 | 7.0 | 10.000000 | 20.0 | 2.0 | 4.0 | 3.0 | 10.0 | 11.0 | 23616.000116 | -3.006702e-08 | 13.0 | 17.0 | 14.0 | 21.0 | 5.0 | 24.0 | 2.0 | 10.000000 | 14.0 | 20.0 | 8.0 | 17.0 | 5.0 | 8.0 | 3.0 | 2.0 | 2.0 | 2.0 | 1.0 | 3.896914e-12 | 5.0 | 4.0 | 22.0 | 25.0 | 10.0 | 8.0 | 12.0 | 13.0 | 14.0 | 15.0 | 2.000000 | 9.0 | 8.0 | 9.0 | 2.000000 | 9.0 | 2.000000 | 4.0 | 2.0 | 9.0 | 5.0 | 8.0 | 2.0 | 9.0 | 10.0 | 15.0 | 3.000000 | 9.0 | 3.0 | 19.0 | 24.0 | 9.0 | 11.0 | 14.0 | 19.0 | 4.0 | 1.0 | 15.000000 | 13.0 | 16.000000 | 14.0 | 16.0 | 21.0 | 15.000000 | 10.0 | 7.0 | 12.0 | 11.0 | 17.0 | 9.0 | 17.0 | 9.0 | 14.0 | 6.000000 | 9.0 | 15.0 | 22.0 | 12.0 | 14.0 | 10.0 | 18.0 | 18.0 | 24.0 | 15.0 | 23.0 | 10.0 | 16.0 | 3.0 | 4.0 | 3.0 | 7.0 | 3.0 | 5.0 | 5.0 | 4.0 | 7.0 | 6.0 | 14.0 | 14.0 | 2.0 | 1.0 | 9.0 | 4.0 | 10.0 | 18.0 | 7.0 | 4.0 | 11.0 | 5.0 | 10.0 | 5.0 | 5.0 | 3.0 | 5.0 | 3.0 | 17.0 | 12.0 | 19.0 | 21.0 | 12.0 | 21.0 | 22.0 | 12.0 | 11.0 | 13.000000 | 15.0 | 8.0 | 7.0 | 8.0 | 11.0 | . 5 2013-06-13 | J | 1,165 | N | 13 | 22 | 13 | 23 | T | G | 0 | 5 | 5 | Q | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 0 | N | 1 | 3 | 0 | 5 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | Y | Y | 0 | J | 0 | 0 | 1 | -1 | 21 | 4 | 2 | A | 4 | 1 | 3 | 1 | 0 | 2 | 1 | 4 | B | N | K | Y | G | N | 2 | N | Y | N | -1 | 24 | -1 | 25 | -1 | 15 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 20 | N | TX | False | False | 23.0 | 0.9691 | 0.0004 | 1.2665 | 11.0 | 18.0 | 11.0 | 18.0 | 9.0 | 16.0 | 10.0 | 17.0 | 3.000000 | 4.0 | 24.0 | 25.0 | 18.0 | 24.0 | 1.0 | 31184.999971 | -3.006702e-08 | -1.0 | -1.0 | -1.0 | -1.0 | 3.0 | 24.0 | 2.0 | 3.000000 | 3.0 | 10.0 | 12.0 | 21.0 | 11.0 | 18.0 | 18.0 | 23.0 | 3.0 | 7.0 | 8.0 | 3.896914e-12 | 19.0 | 22.0 | 9.0 | 18.0 | 10.0 | 8.0 | 10.0 | 11.0 | 10.0 | 8.0 | 12.000000 | 20.0 | 23.0 | 22.0 | 14.000000 | 20.0 | 21.000000 | 23.0 | 14.0 | 21.0 | 23.0 | 23.0 | 15.0 | 21.0 | 23.0 | 20.0 | 8.000000 | 17.0 | 2.0 | 5.0 | 14.0 | 9.0 | 11.0 | 3.0 | 9.0 | 5.0 | 21.0 | 19.000000 | 20.0 | 13.000000 | 10.0 | 7.0 | 10.0 | 16.000000 | 14.0 | 18.0 | 23.0 | 13.0 | 19.0 | 10.0 | 19.0 | 20.0 | 24.0 | 6.000000 | 9.0 | 7.0 | 10.0 | 11.0 | 13.0 | 12.0 | 20.0 | 16.0 | 23.0 | 25.0 | 25.0 | 10.0 | 16.0 | 5.0 | 8.0 | 8.0 | 12.0 | 13.0 | 9.0 | 12.0 | 15.0 | 23.0 | 24.0 | 8.0 | 5.0 | 11.0 | 18.0 | 15.0 | 17.0 | 8.0 | 16.0 | 1.0 | 1.0 | 23.0 | 24.0 | 24.0 | 25.0 | 1.0 | 1.0 | 1.0 | 1.0 | 16.0 | 11.0 | 6.0 | 3.0 | 9.0 | 17.0 | 10.0 | 17.0 | 19.0 | 9.000000 | 9.0 | 9.0 | 9.0 | 8.0 | 20.0 | . 6 2013-07-23 | J | 1,165 | N | 13 | 22 | 13 | 23 | T | F | 1 | 5 | 5 | P | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | N | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | J | 0 | 0 | 1 | -1 | 21 | 4 | 0 | B | 1 | 0 | 0 | 0 | 0 | 2 | 1 | 10 | B | N | N | N | G | N | 0 | N | N | N | -1 | 21 | -1 | 25 | -1 | 23 | -1 | -1 | -1 | 15 | 25 | -1 | -1 | -1 | -1 | 23 | N | TX | False | False | 23.0 | 0.9691 | 0.0004 | 1.2665 | 4.0 | 4.0 | 4.0 | 4.0 | 3.0 | 3.0 | 3.0 | 3.0 | 6.000000 | 12.0 | 15.0 | 24.0 | 23.0 | 25.0 | 1.0 | 4841.000848 | -3.006702e-08 | 5.0 | 6.0 | 8.0 | 17.0 | 2.0 | 24.0 | 2.0 | 10.000000 | 15.0 | 10.0 | 2.0 | 1.0 | 4.0 | 4.0 | 6.0 | 9.0 | 1.0 | 12.0 | 18.0 | 3.896914e-12 | 19.0 | 23.0 | 2.0 | 3.0 | 7.0 | 4.0 | 3.0 | 2.0 | 11.0 | 10.0 | 20.000000 | 23.0 | 22.0 | 22.0 | 20.000000 | 23.0 | 21.000000 | 23.0 | 21.0 | 23.0 | 21.0 | 22.0 | 21.0 | 23.0 | 23.0 | 23.0 | 18.000000 | 23.0 | 2.0 | 4.0 | 8.0 | 15.0 | 16.0 | 2.0 | 1.0 | 15.0 | 25.0 | 18.000000 | 16.0 | 15.000000 | 13.0 | 10.0 | 14.0 | 17.000000 | 16.0 | 2.0 | 2.0 | 3.0 | 2.0 | 11.0 | 20.0 | 4.0 | 3.0 | 2.000000 | 2.0 | 3.0 | 2.0 | 4.0 | 3.0 | 2.0 | 2.0 | 10.0 | 14.0 | 3.0 | 5.0 | 3.0 | 3.0 | 3.0 | 3.0 | 8.0 | 13.0 | 10.0 | 7.0 | 16.0 | 22.0 | 8.0 | 7.0 | 10.0 | 9.0 | 6.0 | 6.0 | 18.0 | 19.0 | 5.0 | 3.0 | 5.0 | 3.0 | 24.0 | 24.0 | 20.0 | 20.0 | 2.0 | 2.0 | 3.0 | 2.0 | 19.0 | 16.0 | 1.0 | 1.0 | 6.0 | 13.0 | 15.0 | 16.0 | 18.0 | 11.000000 | 12.0 | 11.0 | 13.0 | 3.0 | 21.0 | . 7 2013-07-27 | E | 1,480 | N | 13 | 22 | 13 | 23 | T | K | 1 | 5 | 5 | R | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | N | 0 | 0 | 1 | -1 | 21 | 1 | 0 | B | 4 | 0 | 0 | 0 | 0 | 2 | 1 | 10 | B | N | O | N | G | N | 0 | N | N | N | -1 | 13 | -1 | 25 | -1 | 22 | -1 | -1 | -1 | 17 | -1 | -1 | -1 | -1 | -1 | 13 | N | IL | False | False | 23.0 | 0.9487 | 0.0006 | 1.3045 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 10.000000 | 19.0 | 2.0 | 5.0 | 8.0 | 20.0 | 4.0 | 9426.999381 | -3.006702e-08 | 3.0 | 3.0 | 25.0 | 25.0 | 1.0 | 24.0 | 2.0 | 24.999999 | 25.0 | 5.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 3.0 | 2.0 | 3.896914e-12 | 9.0 | 9.0 | 10.0 | 19.0 | 6.0 | 4.0 | 3.0 | 2.0 | 9.0 | 8.0 | 12.000000 | 19.0 | 13.0 | 19.0 | 12.000000 | 19.0 | 16.000000 | 19.0 | 11.0 | 19.0 | 15.0 | 19.0 | 11.0 | 19.0 | 11.0 | 18.0 | 12.000000 | 22.0 | 2.0 | 3.0 | 15.0 | 13.0 | 15.0 | 5.0 | 13.0 | 23.0 | 10.0 | 2.000000 | 2.0 | 2.000001 | 2.0 | 2.0 | 2.0 | 2.000001 | 2.0 | 7.0 | 10.0 | 1.0 | 1.0 | 10.0 | 19.0 | 8.0 | 10.0 | 3.000000 | 2.0 | 2.0 | 1.0 | 14.0 | 18.0 | 4.0 | 5.0 | 13.0 | 20.0 | 7.0 | 17.0 | 10.0 | 16.0 | 1.0 | 1.0 | 13.0 | 22.0 | 16.0 | 14.0 | 16.0 | 22.0 | 9.0 | 10.0 | 19.0 | 20.0 | 3.0 | 5.0 | 25.0 | 25.0 | 2.0 | 1.0 | 9.0 | 6.0 | 8.0 | 3.0 | 8.0 | 4.0 | 10.0 | 8.0 | 10.0 | 8.0 | 14.0 | 8.0 | 7.0 | 4.0 | 7.0 | 14.0 | 9.0 | 5.0 | 2.0 | 25.000001 | 25.0 | 1.0 | 1.0 | 9.0 | 14.0 | . 8 2014-07-31 | E | 1,487 | N | 13 | 22 | 13 | 23 | Y | K | 1 | 5 | 5 | Q | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 3 | 1 | 5 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | R | 0 | 0 | 1 | -1 | 21 | 4 | 2 | C | 5 | 0 | 1 | 0 | 0 | 2 | 1 | 10 | B | N | O | N | G | N | 0 | N | Y | N | -1 | 13 | -1 | 25 | -1 | 17 | -1 | -1 | 25 | 25 | -1 | -1 | -1 | -1 | -1 | 16 | N | IL | False | False | 23.0 | 0.9392 | 0.0006 | 1.3045 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 16.000000 | 24.0 | 3.0 | 7.0 | 6.0 | 19.0 | 11.0 | 67123.999677 | -3.006702e-08 | 1.0 | 1.0 | -1.0 | -1.0 | 1.0 | 24.0 | 2.0 | 15.000000 | 19.0 | 9.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 1.0 | 6.0 | 8.0 | 3.896914e-12 | 12.0 | 15.0 | 6.0 | 13.0 | 12.0 | 10.0 | 4.0 | 3.0 | 12.0 | 11.0 | 8.000000 | 16.0 | 11.0 | 17.0 | 9.000000 | 16.0 | 11.000000 | 16.0 | 8.0 | 16.0 | 13.0 | 17.0 | 9.0 | 17.0 | 9.0 | 12.0 | 7.000000 | 16.0 | 2.0 | 6.0 | 6.0 | 19.0 | 17.0 | 4.0 | 10.0 | 23.0 | 18.0 | 1.000001 | 1.0 | 0.999999 | 1.0 | 1.0 | 1.0 | 0.999999 | 1.0 | 4.0 | 3.0 | 3.0 | 2.0 | 7.0 | 10.0 | 9.0 | 14.0 | 4.000000 | 4.0 | 8.0 | 11.0 | 13.0 | 16.0 | 7.0 | 11.0 | 8.0 | 9.0 | 5.0 | 13.0 | 12.0 | 19.0 | 3.0 | 3.0 | 10.0 | 15.0 | 18.0 | 17.0 | 18.0 | 23.0 | 10.0 | 11.0 | 16.0 | 17.0 | 8.0 | 8.0 | 16.0 | 18.0 | 6.0 | 6.0 | 16.0 | 14.0 | 14.0 | 8.0 | 9.0 | 5.0 | 20.0 | 23.0 | 20.0 | 23.0 | 19.0 | 16.0 | 14.0 | 13.0 | 5.0 | 9.0 | 4.0 | 22.0 | 24.0 | 18.000000 | 22.0 | 13.0 | 16.0 | 2.0 | 25.0 | . 9 2015-01-06 | F | 548 | N | 13 | 22 | 1 | 6 | Y | E | 0 | 4 | 3 | P | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 6 | 1 | N | 1 | 2 | 0 | 1 | 2 | ZH | XV | YP | XC | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | R | 1 | 0 | 1 | -1 | 21 | 1 | 0 | B | 4 | 0 | 2 | 0 | 0 | 2 | 1 | 13 | B | N | N | Y | H | Y | 1 | N | N | N | -1 | 19 | -1 | 25 | -1 | 16 | -1 | -1 | -1 | 17 | -1 | -1 | -1 | -1 | -1 | 14 | N | NJ | True | True | 7.0 | 1.0005 | 0.0040 | 1.2433 | 5.0 | 6.0 | 5.0 | 6.0 | 7.0 | 12.0 | 5.0 | 6.0 | 11.000000 | 21.0 | 3.0 | 7.0 | 4.0 | 14.0 | 20.0 | 8238.999442 | 2.000000e+00 | 4.0 | 5.0 | 13.0 | 20.0 | 2.0 | 8.0 | 2.0 | 7.000000 | 11.0 | 8.0 | 4.0 | 9.0 | 5.0 | 6.0 | 8.0 | 11.0 | 1.5 | 8.0 | 11.0 | 3.896914e-12 | 18.0 | 22.0 | 7.0 | 15.0 | 14.0 | 13.0 | 14.0 | 16.0 | 15.0 | 17.0 | 5.000000 | 16.0 | 10.0 | 15.0 | 5.000000 | 16.0 | 7.000000 | 16.0 | 4.0 | 12.0 | 9.0 | 14.0 | 5.0 | 14.0 | 11.0 | 17.0 | 7.000000 | 16.0 | 2.0 | 11.0 | 12.0 | 22.0 | 22.0 | 23.0 | 24.0 | 17.0 | 18.0 | 18.000000 | 17.0 | 14.000000 | 13.0 | 7.0 | 10.0 | 19.000000 | 18.0 | 3.0 | 3.0 | 3.0 | 2.0 | 6.0 | 6.0 | 10.0 | 16.0 | 14.000000 | 22.0 | 6.0 | 7.0 | 5.0 | 3.0 | 7.0 | 11.0 | 9.0 | 14.0 | 3.0 | 6.0 | 8.0 | 11.0 | 10.0 | 18.0 | 25.0 | 25.0 | 24.0 | 24.0 | 23.0 | 25.0 | 12.0 | 20.0 | 19.0 | 20.0 | 7.0 | 7.0 | 17.0 | 18.0 | 5.0 | 3.0 | 17.0 | 15.0 | 14.0 | 9.0 | 14.0 | 10.0 | 19.0 | 21.0 | 18.0 | 20.0 | 16.0 | 11.0 | 8.0 | 5.0 | 6.0 | 11.0 | 13.0 | 9.0 | 6.0 | 13.000000 | 16.0 | 8.0 | 7.0 | 16.0 | 8.0 | . Doing inferences based on this blog post from Walk With Fastai . row, cls, probs = learn.predict(df_test.iloc[0]) . row.show() . Original_Quote_Date Field6 Field10 Field12 CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 SalesField3 SalesField4 SalesField5 SalesField7 SalesField9 SalesField10 SalesField11 SalesField13 SalesField14 SalesField15 PersonalField1 PersonalField2 PersonalField5 PersonalField6 PersonalField7 PersonalField8 PersonalField9 PersonalField11 PersonalField12 PersonalField13 PersonalField16 PersonalField17 PersonalField18 PersonalField19 PersonalField22 PersonalField23 PersonalField24 PersonalField25 PersonalField26 PersonalField27 PersonalField28 PersonalField29 PersonalField30 PersonalField31 PersonalField32 PersonalField33 PersonalField34 PersonalField35 PersonalField36 PersonalField37 PersonalField38 PersonalField39 PersonalField40 PersonalField41 PersonalField42 PersonalField43 PersonalField44 PersonalField45 PersonalField46 PersonalField47 PersonalField48 PersonalField49 PersonalField50 PersonalField51 PersonalField52 PersonalField53 PersonalField54 PersonalField55 PersonalField56 PersonalField57 PersonalField58 PersonalField59 PersonalField60 PersonalField61 PersonalField62 PersonalField63 PersonalField64 PersonalField65 PersonalField66 PersonalField67 PersonalField68 PersonalField69 PersonalField70 PersonalField71 PersonalField72 PersonalField73 PersonalField74 PersonalField75 PersonalField76 PersonalField77 PersonalField78 PersonalField79 PersonalField80 PersonalField81 PersonalField82 PersonalField83 PropertyField2A PropertyField3 PropertyField4 PropertyField5 PropertyField6 PropertyField7 PropertyField8 PropertyField9 PropertyField10 PropertyField11A PropertyField11B PropertyField12 PropertyField13 PropertyField14 PropertyField15 PropertyField17 PropertyField18 PropertyField19 PropertyField20 PropertyField22 PropertyField23 PropertyField27 PropertyField28 PropertyField30 PropertyField31 PropertyField32 PropertyField33 PropertyField34 PropertyField35 PropertyField36 PropertyField37 PropertyField38 GeographicField5A GeographicField5B GeographicField10A GeographicField10B GeographicField14A GeographicField14B GeographicField18A GeographicField21A GeographicField22A GeographicField22B GeographicField23A GeographicField56A GeographicField60A GeographicField61A GeographicField62A GeographicField62B GeographicField63 GeographicField64 PersonalField84_na PropertyField29_na Field7 Field8 Field9 Field11 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField6 SalesField8 SalesField12 PersonalField4A PersonalField4B PersonalField10A PersonalField10B PersonalField14 PersonalField15 PersonalField84 PropertyField1A PropertyField1B PropertyField2B PropertyField16A PropertyField16B PropertyField21A PropertyField21B PropertyField24A PropertyField24B PropertyField25 PropertyField26A PropertyField26B PropertyField29 PropertyField39A PropertyField39B GeographicField1A GeographicField1B GeographicField2A GeographicField2B GeographicField3A GeographicField3B GeographicField4A GeographicField4B GeographicField6A GeographicField6B GeographicField7A GeographicField7B GeographicField8A GeographicField8B GeographicField9A GeographicField9B GeographicField11A GeographicField11B GeographicField12A GeographicField12B GeographicField13A GeographicField13B GeographicField15A GeographicField15B GeographicField16A GeographicField16B GeographicField17A GeographicField17B GeographicField18B GeographicField19A GeographicField19B GeographicField20A GeographicField20B GeographicField21B GeographicField23B GeographicField24A GeographicField24B GeographicField25A GeographicField25B GeographicField26A GeographicField26B GeographicField27A GeographicField27B GeographicField28A GeographicField28B GeographicField29A GeographicField29B GeographicField30A GeographicField30B GeographicField31A GeographicField31B GeographicField32A GeographicField32B GeographicField33A GeographicField33B GeographicField34A GeographicField34B GeographicField35A GeographicField35B GeographicField36A GeographicField36B GeographicField37A GeographicField37B GeographicField38A GeographicField38B GeographicField39A GeographicField39B GeographicField40A GeographicField40B GeographicField41A GeographicField41B GeographicField42A GeographicField42B GeographicField43A GeographicField43B GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60B GeographicField61B QuoteConversion_Flag . 0 2014-08-12 | E | 1,487 | N | 13 | 22 | 13 | 23 | Y | K | 0 | 5 | 5 | P | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | N | 1 | 3 | 0 | 5 | 2 | YH | XR | XQ | ZQ | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | D | 1 | 0 | 1 | -1 | 24 | 1 | 2 | A | 1 | 0 | 2 | 0 | 0 | 2 | 1 | 19 | B | N | N | Y | G | N | 2 | N | Y | N | -1 | 13 | -1 | 25 | -1 | 19 | -1 | -1 | -1 | 24 | -1 | -1 | -1 | -1 | -1 | 25 | Y | IL | True | True | 16.0 | 0.9364 | 0.0006 | 1.3045 | 4.0 | 4.0 | 4.0 | 4.0 | 3.0 | 3.0 | 3.0 | 4.0 | 13.0 | 22.0 | 6.0 | 16.0 | 9.0 | 21.0 | 11.0 | 67051.999655 | 2.523887e-08 | 11.0 | 14.0 | -1.0 | -1.0 | 5.0 | 14.0 | 2.0 | 18.0 | 23.0 | 4.0 | 3.0 | 5.0 | 4.0 | 4.0 | 6.0 | 8.0 | 1.5 | 7.0 | 8.0 | -7.737822e-13 | 18.0 | 21.0 | 25.0 | 25.0 | 9.0 | 6.0 | 1.0 | 1.0 | 24.0 | 25.0 | 9.0 | 18.0 | 12.0 | 18.0 | 10.0 | 18.0 | 16.0 | 19.0 | 9.0 | 18.0 | 13.0 | 18.0 | 9.0 | 18.0 | 10.0 | 13.0 | 8.0 | 17.0 | 2.0 | 10.0 | 20.0 | 19.0 | 18.0 | 4.0 | 9.0 | 25.0 | 15.0 | 16.0 | 15.0 | 17.0 | 17.0 | 10.0 | 13.0 | 21.0 | 20.0 | 25.0 | 25.0 | 16.0 | 22.0 | 25.0 | 25.0 | 8.0 | 11.0 | 10.0 | 17.0 | 11.0 | 17.0 | 24.000001 | 25.0 | 15.0 | 23.0 | 24.999999 | 25.0 | -1.0 | -1.0 | 21.0 | 24.0 | 18.0 | 24.0 | 11.0 | 18.0 | 18.0 | 16.0 | 20.0 | 24.0 | 11.0 | 17.0 | 22.0 | 23.0 | 9.0 | 12.0 | 24.999999 | 25.0 | 6.0 | 9.0 | 4.0 | 2.0 | 16.0 | 12.0 | 20.0 | 20.0 | 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 10.0 | 7.0 | 25.0 | 25.0 | 19.0 | 19.0 | 22.0 | 12.0 | 15.0 | 1.0 | 1.0 | 1.0 | 20.0 | False | . dl_test = learn.dls.test_dl(df_test.iloc[:]) . dl_test.show_batch() . Original_Quote_Date Field6 Field10 Field12 CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 SalesField3 SalesField4 SalesField5 SalesField7 SalesField9 SalesField10 SalesField11 SalesField13 SalesField14 SalesField15 PersonalField1 PersonalField2 PersonalField5 PersonalField6 PersonalField7 PersonalField8 PersonalField9 PersonalField11 PersonalField12 PersonalField13 PersonalField16 PersonalField17 PersonalField18 PersonalField19 PersonalField22 PersonalField23 PersonalField24 PersonalField25 PersonalField26 PersonalField27 PersonalField28 PersonalField29 PersonalField30 PersonalField31 PersonalField32 PersonalField33 PersonalField34 PersonalField35 PersonalField36 PersonalField37 PersonalField38 PersonalField39 PersonalField40 PersonalField41 PersonalField42 PersonalField43 PersonalField44 PersonalField45 PersonalField46 PersonalField47 PersonalField48 PersonalField49 PersonalField50 PersonalField51 PersonalField52 PersonalField53 PersonalField54 PersonalField55 PersonalField56 PersonalField57 PersonalField58 PersonalField59 PersonalField60 PersonalField61 PersonalField62 PersonalField63 PersonalField64 PersonalField65 PersonalField66 PersonalField67 PersonalField68 PersonalField69 PersonalField70 PersonalField71 PersonalField72 PersonalField73 PersonalField74 PersonalField75 PersonalField76 PersonalField77 PersonalField78 PersonalField79 PersonalField80 PersonalField81 PersonalField82 PersonalField83 PropertyField2A PropertyField3 PropertyField4 PropertyField5 PropertyField6 PropertyField7 PropertyField8 PropertyField9 PropertyField10 PropertyField11A PropertyField11B PropertyField12 PropertyField13 PropertyField14 PropertyField15 PropertyField17 PropertyField18 PropertyField19 PropertyField20 PropertyField22 PropertyField23 PropertyField27 PropertyField28 PropertyField30 PropertyField31 PropertyField32 PropertyField33 PropertyField34 PropertyField35 PropertyField36 PropertyField37 PropertyField38 GeographicField5A GeographicField5B GeographicField10A GeographicField10B GeographicField14A GeographicField14B GeographicField18A GeographicField21A GeographicField22A GeographicField22B GeographicField23A GeographicField56A GeographicField60A GeographicField61A GeographicField62A GeographicField62B GeographicField63 GeographicField64 PersonalField84_na PropertyField29_na Field7 Field8 Field9 Field11 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField6 SalesField8 SalesField12 PersonalField4A PersonalField4B PersonalField10A PersonalField10B PersonalField14 PersonalField15 PersonalField84 PropertyField1A PropertyField1B PropertyField2B PropertyField16A PropertyField16B PropertyField21A PropertyField21B PropertyField24A PropertyField24B PropertyField25 PropertyField26A PropertyField26B PropertyField29 PropertyField39A PropertyField39B GeographicField1A GeographicField1B GeographicField2A GeographicField2B GeographicField3A GeographicField3B GeographicField4A GeographicField4B GeographicField6A GeographicField6B GeographicField7A GeographicField7B GeographicField8A GeographicField8B GeographicField9A GeographicField9B GeographicField11A GeographicField11B GeographicField12A GeographicField12B GeographicField13A GeographicField13B GeographicField15A GeographicField15B GeographicField16A GeographicField16B GeographicField17A GeographicField17B GeographicField18B GeographicField19A GeographicField19B GeographicField20A GeographicField20B GeographicField21B GeographicField23B GeographicField24A GeographicField24B GeographicField25A GeographicField25B GeographicField26A GeographicField26B GeographicField27A GeographicField27B GeographicField28A GeographicField28B GeographicField29A GeographicField29B GeographicField30A GeographicField30B GeographicField31A GeographicField31B GeographicField32A GeographicField32B GeographicField33A GeographicField33B GeographicField34A GeographicField34B GeographicField35A GeographicField35B GeographicField36A GeographicField36B GeographicField37A GeographicField37B GeographicField38A GeographicField38B GeographicField39A GeographicField39B GeographicField40A GeographicField40B GeographicField41A GeographicField41B GeographicField42A GeographicField42B GeographicField43A GeographicField43B GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60B GeographicField61B . 0 2014-08-12 | E | 1,487 | N | 13 | 22 | 13 | 23 | Y | K | 0 | 5 | 5 | P | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | N | 1 | 3 | 0 | 5 | 2 | YH | XR | XQ | ZQ | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | D | 1 | 0 | 1 | -1 | 24 | 1 | 2 | A | 1 | 0 | 2 | 0 | 0 | 2 | 1 | 19 | B | N | N | Y | G | N | 2 | N | Y | N | -1 | 13 | -1 | 25 | -1 | 19 | -1 | -1 | -1 | 24 | -1 | -1 | -1 | -1 | -1 | 25 | Y | IL | True | True | 16.0 | 0.9364 | 0.0006 | 1.3045 | 4.0 | 4.0 | 4.0 | 4.0 | 3.0 | 3.0 | 3.000000 | 4.0 | 13.0 | 22.0 | 6.0 | 16.0 | 9.0 | 21.0 | 11.000000 | 67051.999655 | 2.523887e-08 | 11.0 | 14.0 | -1.000000 | -1.0 | 5.0 | 14.0 | 2.0 | 18.0 | 23.0 | 4.0 | 3.0 | 5.0 | 4.0 | 4.0 | 6.0 | 8.0 | 1.5 | 7.0 | 8.0 | -7.737822e-13 | 18.0 | 21.0 | 25.0 | 25.0 | 9.0 | 6.0 | 1.0 | 1.0 | 24.0 | 25.0 | 9.0 | 18.0 | 12.0 | 18.0 | 10.0 | 18.0 | 16.000000 | 19.0 | 9.0 | 18.0 | 13.0 | 18.0 | 9.000000 | 18.0 | 10.0 | 13.0 | 8.0 | 17.0 | 2.0 | 10.0 | 20.0 | 19.0 | 18.0 | 4.0 | 9.0 | 25.0 | 15.0 | 16.0 | 15.0 | 17.0 | 17.0 | 10.0 | 13.0 | 21.000000 | 20.0 | 25.0 | 25.0 | 16.0 | 22.0 | 25.0 | 25.0 | 8.0 | 11.0 | 10.0 | 17.0 | 11.0 | 17.0 | 24.000001 | 25.0 | 15.0 | 23.0 | 24.999999 | 25.0 | -1.0 | -1.0 | 21.0 | 24.0 | 18.0 | 24.0 | 11.0 | 18.0 | 18.0 | 16.0 | 20.0 | 24.0 | 11.0 | 17.0 | 22.0 | 23.0 | 9.0 | 12.0 | 24.999999 | 25.0 | 6.0 | 9.0 | 4.0 | 2.0 | 16.0 | 12.0 | 20.0 | 20.0 | 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 10.0 | 7.0 | 25.0 | 25.0 | 19.0 | 19.0 | 22.0 | 12.0 | 15.0 | 1.0 | 1.0 | 1.0 | 20.0 | . 1 2013-09-07 | F | 564 | N | 13 | 22 | 13 | 23 | T | E | 1 | 5 | 5 | R | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | N | 1 | 2 | 0 | 1 | 2 | YF | XS | YP | XC | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | J | 1 | 0 | 1 | -1 | 21 | 1 | 1 | A | 4 | 1 | 1 | 1 | 0 | 2 | 1 | 10 | B | N | O | Y | H | N | 1 | N | N | N | -1 | 20 | -1 | 25 | -1 | 13 | -1 | -1 | -1 | 22 | -1 | -1 | -1 | -1 | -1 | 21 | N | NJ | True | True | 11.0 | 0.9919 | 0.0038 | 1.1886 | 8.0 | 14.0 | 8.0 | 14.0 | 7.0 | 12.0 | 8.000000 | 13.0 | 4.0 | 5.0 | 3.0 | 6.0 | 3.0 | 6.0 | 4.000000 | 27288.000271 | 2.523887e-08 | 15.0 | 20.0 | 9.000000 | 18.0 | 2.0 | 4.0 | 2.0 | 22.0 | 24.0 | 10.0 | 7.0 | 15.0 | 8.0 | 14.0 | 5.0 | 6.0 | 1.0 | 10.0 | 15.0 | -7.737822e-13 | 5.0 | 3.0 | 17.0 | 24.0 | 17.0 | 17.0 | 15.0 | 17.0 | 10.0 | 9.0 | 4.0 | 13.0 | 9.0 | 13.0 | 5.0 | 13.0 | 5.000000 | 12.0 | 4.0 | 15.0 | 8.0 | 13.0 | 5.000000 | 14.0 | 9.0 | 11.0 | 5.0 | 12.0 | 2.0 | 16.0 | 22.0 | 23.0 | 24.0 | 21.0 | 22.0 | 22.0 | 14.0 | 10.0 | 4.0 | 10.0 | 5.0 | 5.0 | 5.0 | 13.000000 | 6.0 | 9.0 | 14.0 | 8.0 | 10.0 | 12.0 | 22.0 | 7.0 | 10.0 | 9.0 | 15.0 | 11.0 | 18.0 | 14.000000 | 18.0 | 5.0 | 7.0 | 19.000000 | 24.0 | 6.0 | 14.0 | 16.0 | 23.0 | 6.0 | 12.0 | 11.0 | 19.0 | 25.0 | 25.0 | 15.0 | 20.0 | 12.0 | 20.0 | 23.0 | 24.0 | 12.0 | 21.0 | 23.000000 | 25.0 | 7.0 | 11.0 | 16.0 | 14.0 | 13.0 | 6.0 | 17.0 | 15.0 | 7.0 | 5.0 | 7.0 | 5.0 | 13.0 | 7.0 | 14.0 | 14.0 | 7.0 | 14.0 | 4.0 | 1.0 | 1.0 | 5.0 | 3.0 | 10.0 | 10.0 | 5.0 | 5.0 | . 2 2013-03-29 | F | 564 | N | 13 | 22 | 13 | 23 | T | E | 1 | 5 | 5 | V | 0 | 1 | 2 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | N | 1 | 2 | 0 | 1 | 2 | XE | ZH | YK | ZN | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | R | 1 | 0 | 1 | -1 | 24 | 1 | 2 | C | 1 | 0 | 2 | 1 | 0 | 2 | 1 | 10 | B | N | M | Y | G | Y | 1 | N | N | N | -1 | 16 | -1 | 25 | -1 | 11 | -1 | -1 | -1 | 18 | -1 | -1 | -1 | -1 | -1 | 11 | N | NJ | True | True | 15.0 | 0.8945 | 0.0038 | 1.0670 | 11.0 | 18.0 | 11.0 | 18.0 | 10.0 | 16.0 | 10.000000 | 18.0 | 3.0 | 3.0 | 5.0 | 14.0 | 3.0 | 9.0 | 23.000000 | 65263.998972 | 2.000000e+00 | 12.0 | 15.0 | 22.000001 | 24.0 | 4.0 | 10.0 | 2.0 | 6.0 | 9.0 | 23.0 | 8.0 | 17.0 | 11.0 | 18.0 | 12.0 | 18.0 | 1.0 | 20.0 | 24.0 | -7.737822e-13 | 11.0 | 12.0 | 3.0 | 6.0 | 16.0 | 16.0 | 15.0 | 18.0 | 14.0 | 16.0 | 4.0 | 10.0 | 10.0 | 16.0 | 5.0 | 11.0 | 6.000000 | 13.0 | 4.0 | 10.0 | 10.0 | 16.0 | 5.000000 | 10.0 | 11.0 | 18.0 | 5.0 | 13.0 | 2.0 | 13.0 | 15.0 | 21.0 | 20.0 | 19.0 | 21.0 | 16.0 | 16.0 | 15.0 | 11.0 | 11.0 | 7.0 | 6.0 | 7.0 | 13.000000 | 7.0 | 7.0 | 11.0 | 15.0 | 21.0 | 5.0 | 5.0 | 3.0 | 2.0 | 14.0 | 21.0 | 8.0 | 12.0 | 12.000000 | 15.0 | 14.0 | 22.0 | 7.000000 | 7.0 | 5.0 | 11.0 | 9.0 | 14.0 | 13.0 | 22.0 | 8.0 | 13.0 | 22.0 | 19.0 | 11.0 | 14.0 | 11.0 | 16.0 | 16.0 | 18.0 | 9.0 | 10.0 | 14.000000 | 16.0 | 6.0 | 8.0 | 20.0 | 19.0 | 17.0 | 14.0 | 16.0 | 13.0 | 20.0 | 22.0 | 20.0 | 22.0 | 20.0 | 18.0 | 10.0 | 7.0 | 4.0 | 7.0 | 11.0 | 13.0 | 12.0 | 18.0 | 22.0 | 10.0 | 11.0 | 20.0 | 22.0 | . 3 2015-03-21 | K | 1,113 | Y | 13 | 22 | 13 | 23 | Y | F | 1 | 5 | 5 | R | 1 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | N | 1 | 2 | 0 | 1 | 2 | XR | YY | XT | XC | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | Q | 1 | 0 | 1 | -1 | 21 | 4 | 2 | C | 4 | 1 | 2 | 1 | 0 | 2 | 1 | 4 | B | N | M | N | G | N | 0 | N | N | N | -1 | 14 | -1 | 25 | 25 | 25 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 21 | N | TX | True | True | 21.0 | 0.8870 | 0.0004 | 1.2665 | 14.0 | 22.0 | 15.0 | 22.0 | 13.0 | 20.0 | 21.999999 | 25.0 | 5.0 | 9.0 | 9.0 | 20.0 | 5.0 | 16.0 | 11.000000 | 32725.000016 | 1.000000e+00 | 5.0 | 6.0 | 7.000000 | 15.0 | 3.0 | 5.0 | 2.0 | 5.0 | 8.0 | 6.0 | 5.0 | 10.0 | 14.0 | 22.0 | 13.0 | 18.0 | 2.0 | 8.0 | 11.0 | -7.737822e-13 | 7.0 | 5.0 | 9.0 | 18.0 | 11.0 | 9.0 | 13.0 | 15.0 | 22.0 | 24.0 | 24.0 | 25.0 | 23.0 | 23.0 | 24.0 | 25.0 | 25.000001 | 25.0 | 24.0 | 24.0 | 23.0 | 23.0 | 23.999999 | 24.0 | 23.0 | 22.0 | 24.0 | 25.0 | 2.0 | 7.0 | 13.0 | 11.0 | 14.0 | 6.0 | 16.0 | 15.0 | 22.0 | 11.0 | 5.0 | 10.0 | 4.0 | 5.0 | 4.0 | 13.000000 | 5.0 | 7.0 | 10.0 | 3.0 | 2.0 | 5.0 | 5.0 | 9.0 | 14.0 | 15.0 | 22.0 | 7.0 | 10.0 | 9.000000 | 8.0 | 13.0 | 22.0 | 6.000000 | 5.0 | 3.0 | 7.0 | 8.0 | 11.0 | 11.0 | 20.0 | 17.0 | 23.0 | 14.0 | 12.0 | 15.0 | 20.0 | 10.0 | 14.0 | 11.0 | 11.0 | 9.0 | 10.0 | 11.000000 | 13.0 | 15.0 | 21.0 | 14.0 | 12.0 | 17.0 | 13.0 | 10.0 | 6.0 | 20.0 | 22.0 | 20.0 | 22.0 | 19.0 | 16.0 | 12.0 | 11.0 | 4.0 | 6.0 | 13.0 | 10.0 | 8.0 | 5.0 | 3.0 | 8.0 | 8.0 | 13.0 | 8.0 | . 4 2014-12-10 | B | 935 | N | 13 | 22 | 13 | 23 | Y | D | 0 | 5 | 5 | T | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 1 | 0 | 4 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | R | 0 | 0 | 1 | -1 | 21 | 2 | 1 | C | 1 | 0 | 2 | 0 | 0 | 2 | 1 | 10 | B | N | O | Y | H | N | 1 | Y | N | N | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 21 | N | CA | False | True | 25.0 | 0.9153 | 0.0007 | 1.0200 | 4.0 | 5.0 | 4.0 | 5.0 | 4.0 | 4.0 | 4.000000 | 5.0 | 12.0 | 21.0 | 1.0 | 1.0 | 3.0 | 6.0 | 11.000000 | 56024.998915 | 1.000000e+00 | 1.0 | 1.0 | 4.000000 | 2.0 | 2.0 | 24.0 | 2.0 | 2.0 | 2.0 | 8.0 | 5.0 | 11.0 | 4.0 | 5.0 | 4.0 | 4.0 | 1.0 | 9.0 | 12.0 | -7.737822e-13 | 10.0 | 11.0 | 2.0 | 4.0 | 12.0 | 11.0 | 1.0 | 1.0 | 11.0 | 9.0 | 2.0 | 8.0 | 4.0 | 6.0 | 2.0 | 6.0 | 2.000000 | 9.0 | 2.0 | 3.0 | 3.0 | 5.0 | 2.000000 | 5.0 | 6.0 | 6.0 | 2.0 | 6.0 | 15.0 | 23.0 | 17.0 | 4.0 | 8.0 | 6.0 | 17.0 | 13.0 | 8.0 | 17.0 | 15.0 | 15.0 | 13.0 | 15.0 | 18.0 | 14.000000 | 7.0 | 7.0 | 10.0 | 12.0 | 19.0 | 8.0 | 14.0 | 7.0 | 9.0 | 2.0 | 1.0 | 15.0 | 21.0 | 12.000000 | 15.0 | 16.0 | 23.0 | 9.000000 | 13.0 | 16.0 | 24.0 | 6.0 | 8.0 | 4.0 | 6.0 | 2.0 | 5.0 | 11.0 | 7.0 | 8.0 | 9.0 | 6.0 | 5.0 | 9.0 | 8.0 | 25.0 | 25.0 | 9.000000 | 3.0 | 9.0 | 18.0 | 7.0 | 4.0 | 16.0 | 12.0 | 13.0 | 9.0 | 8.0 | 6.0 | 8.0 | 6.0 | 11.0 | 5.0 | 19.0 | 21.0 | 13.0 | 21.0 | 23.0 | 11.0 | 8.0 | 5.0 | 3.0 | 7.0 | 7.0 | 3.0 | 22.0 | . 5 2013-07-19 | B | 965 | N | 13 | 22 | 13 | 23 | Y | D | 0 | 5 | 5 | V | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | O | 0 | 0 | 1 | -1 | 21 | 2 | 2 | C | 1 | 1 | 4 | 0 | 0 | 2 | 1 | 10 | B | N | O | N | H | Y | 2 | N | N | N | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 8 | N | CA | False | True | 24.0 | 0.9403 | 0.0006 | 1.0200 | 11.0 | 18.0 | 11.0 | 18.0 | 10.0 | 16.0 | 10.000000 | 18.0 | 6.0 | 12.0 | 6.0 | 17.0 | 4.0 | 14.0 | 24.000000 | 42867.999917 | 2.523887e-08 | 15.0 | 19.0 | 5.000000 | 7.0 | 3.0 | 24.0 | 2.0 | 15.0 | 20.0 | 20.0 | 10.0 | 19.0 | 11.0 | 18.0 | 13.0 | 19.0 | 1.0 | 21.0 | 24.0 | -7.737822e-13 | 12.0 | 14.0 | 13.0 | 23.0 | 14.0 | 13.0 | 6.0 | 6.0 | 14.0 | 15.0 | 2.0 | 3.0 | 4.0 | 5.0 | 2.0 | 5.0 | 2.000000 | 2.0 | 2.0 | 6.0 | 3.0 | 6.0 | 2.000000 | 6.0 | 5.0 | 5.0 | 2.0 | 5.0 | 15.0 | 23.0 | 24.0 | 2.0 | 2.0 | 2.0 | 4.0 | 7.0 | 2.0 | 13.0 | 8.0 | 14.0 | 11.0 | 11.0 | 16.0 | 14.000000 | 9.0 | 10.0 | 18.0 | 9.0 | 12.0 | 8.0 | 14.0 | 18.0 | 24.0 | 12.0 | 20.0 | 13.0 | 20.0 | 9.000000 | 9.0 | 11.0 | 19.0 | 15.000000 | 23.0 | 7.0 | 16.0 | 12.0 | 20.0 | 5.0 | 8.0 | 3.0 | 6.0 | 2.0 | 3.0 | 6.0 | 5.0 | 8.0 | 7.0 | 14.0 | 14.0 | 2.0 | 4.0 | 10.000000 | 10.0 | 21.0 | 23.0 | 12.0 | 10.0 | 17.0 | 13.0 | 15.0 | 12.0 | 6.0 | 4.0 | 6.0 | 4.0 | 17.0 | 12.0 | 14.0 | 14.0 | 11.0 | 20.0 | 21.0 | 10.0 | 6.0 | 10.0 | 12.0 | 9.0 | 9.0 | 2.0 | 20.0 | . 6 2014-07-28 | E | 1,487 | N | 13 | 22 | 13 | 23 | Y | K | 1 | 5 | 5 | K | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | N | Y | 0 | N | 0 | 0 | 1 | -1 | 21 | 1 | 2 | C | 2 | 1 | 1 | 0 | 0 | 2 | 1 | 10 | B | N | O | N | E | N | 0 | N | N | N | -1 | 13 | -1 | 25 | -1 | 23 | -1 | -1 | -1 | 18 | -1 | -1 | -1 | -1 | -1 | 8 | N | IL | False | False | 23.0 | 0.9392 | 0.0006 | 1.3045 | 5.0 | 6.0 | 5.0 | 6.0 | 4.0 | 5.0 | 4.000000 | 5.0 | 11.0 | 21.0 | 3.0 | 8.0 | 5.0 | 15.0 | 11.000000 | 52430.999721 | 2.523887e-08 | 18.0 | 22.0 | 15.000000 | 21.0 | 2.0 | 24.0 | 2.0 | 4.0 | 6.0 | 8.0 | 4.0 | 7.0 | 5.0 | 6.0 | 4.0 | 4.0 | 1.0 | 9.0 | 12.0 | -7.737822e-13 | 10.0 | 10.0 | 2.0 | 3.0 | 14.0 | 14.0 | 7.0 | 6.0 | 17.0 | 19.0 | 13.0 | 21.0 | 15.0 | 19.0 | 13.0 | 20.0 | 23.000000 | 25.0 | 11.0 | 19.0 | 16.0 | 19.0 | 11.000000 | 19.0 | 14.0 | 19.0 | 15.0 | 22.0 | 2.0 | 18.0 | 19.0 | 16.0 | 16.0 | 2.0 | 4.0 | 24.0 | 19.0 | 2.0 | 1.0 | 2.0 | 1.0 | 2.0 | 1.0 | 2.000001 | 1.0 | 5.0 | 7.0 | 4.0 | 3.0 | 6.0 | 7.0 | 8.0 | 12.0 | 7.0 | 11.0 | 6.0 | 8.0 | 11.000000 | 12.0 | 8.0 | 14.0 | 6.000000 | 6.0 | 9.0 | 20.0 | 9.0 | 14.0 | 2.0 | 2.0 | 23.0 | 25.0 | 17.0 | 15.0 | 20.0 | 24.0 | 11.0 | 17.0 | 8.0 | 3.0 | 6.0 | 6.0 | 13.000000 | 15.0 | 4.0 | 2.0 | 21.0 | 21.0 | 20.0 | 20.0 | 15.0 | 12.0 | 23.0 | 24.0 | 23.0 | 24.0 | 20.0 | 18.0 | 16.0 | 16.0 | 2.0 | 3.0 | 3.0 | 25.0 | 25.0 | 9.0 | 9.0 | 11.0 | 12.0 | 7.0 | 4.0 | . 7 2015-01-20 | B | 935 | N | 13 | 22 | 13 | 23 | Y | D | 1 | 5 | 5 | P | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | N | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | O | 0 | 0 | 1 | -1 | 21 | 4 | 2 | C | 4 | 1 | 2 | 0 | 0 | 2 | 1 | 10 | B | N | O | Y | H | Y | 2 | N | N | N | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 8 | N | CA | False | True | 25.0 | 0.9153 | 0.0007 | 1.0200 | 6.0 | 9.0 | 6.0 | 9.0 | 5.0 | 7.0 | 5.000000 | 8.0 | 10.0 | 19.0 | 2.0 | 5.0 | 3.0 | 9.0 | 11.000000 | 2571.999669 | 2.523887e-08 | 7.0 | 10.0 | 21.000000 | 23.0 | 3.0 | 24.0 | 2.0 | 15.0 | 19.0 | 20.0 | 7.0 | 15.0 | 6.0 | 9.0 | 5.0 | 6.0 | 1.0 | 11.0 | 15.0 | -7.737822e-13 | 9.0 | 10.0 | 6.0 | 13.0 | 17.0 | 18.0 | 10.0 | 11.0 | 10.0 | 8.0 | 2.0 | 7.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.000000 | 4.0 | 2.0 | 7.0 | 1.0 | 1.0 | 2.000000 | 2.0 | 3.0 | 3.0 | 2.0 | 3.0 | 5.0 | 20.0 | 18.0 | 3.0 | 6.0 | 4.0 | 11.0 | 11.0 | 5.0 | 23.0 | 24.0 | 15.0 | 13.0 | 14.0 | 17.0 | 15.000000 | 9.0 | 9.0 | 14.0 | 3.0 | 2.0 | 9.0 | 16.0 | 3.0 | 2.0 | 15.0 | 22.0 | 14.0 | 21.0 | 8.000000 | 7.0 | 9.0 | 16.0 | 14.000000 | 22.0 | 15.0 | 23.0 | 15.0 | 22.0 | 6.0 | 12.0 | 1.0 | 1.0 | 6.0 | 6.0 | 12.0 | 15.0 | 2.0 | 2.0 | 14.0 | 14.0 | 2.0 | 2.0 | 9.000000 | 3.0 | 1.0 | 1.0 | 18.0 | 18.0 | 20.0 | 19.0 | 14.0 | 10.0 | 23.0 | 24.0 | 22.0 | 24.0 | 15.0 | 10.0 | 22.0 | 24.0 | 7.0 | 14.0 | 16.0 | 15.0 | 16.0 | 2.0 | 1.0 | 15.0 | 19.0 | 17.0 | 15.0 | . 8 2014-05-28 | J | 1,113 | N | 13 | 22 | 13 | 23 | T | F | 0 | 5 | 5 | T | 0 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | Y | Y | 0 | D | 0 | 0 | 1 | -1 | 21 | 4 | 2 | A | 5 | 1 | 3 | 1 | 0 | 2 | 1 | 4 | B | N | K | Y | E | N | 2 | N | N | N | -1 | 23 | -1 | 25 | -1 | 16 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 8 | N | TX | False | False | 23.0 | 0.8928 | 0.0004 | 1.2665 | 12.0 | 20.0 | 12.0 | 20.0 | 11.0 | 18.0 | 12.000000 | 19.0 | 2.0 | 3.0 | 15.0 | 24.0 | 10.0 | 22.0 | 11.000000 | 10064.999387 | 1.000000e+00 | -1.0 | -1.0 | 24.000000 | 25.0 | 5.0 | 24.0 | 2.0 | 10.0 | 13.0 | 24.0 | 4.0 | 8.0 | 12.0 | 20.0 | 21.0 | 24.0 | 3.0 | 8.0 | 10.0 | -7.737822e-13 | 18.0 | 22.0 | 4.0 | 9.0 | 11.0 | 9.0 | 12.0 | 13.0 | 14.0 | 14.0 | 13.0 | 21.0 | 22.0 | 22.0 | 14.0 | 21.0 | 20.000000 | 23.0 | 14.0 | 22.0 | 21.0 | 22.0 | 15.000000 | 22.0 | 23.0 | 23.0 | 9.0 | 18.0 | 2.0 | 4.0 | 13.0 | 9.0 | 11.0 | 1.0 | 1.0 | 8.0 | 24.0 | 19.0 | 20.0 | 13.0 | 10.0 | 7.0 | 10.0 | 16.000000 | 14.0 | 9.0 | 15.0 | 9.0 | 13.0 | 11.0 | 20.0 | 14.0 | 22.0 | 15.0 | 22.0 | 9.0 | 14.0 | 12.000000 | 14.0 | 11.0 | 19.0 | 14.000000 | 21.0 | 4.0 | 10.0 | 9.0 | 15.0 | 11.0 | 21.0 | 8.0 | 13.0 | 13.0 | 9.0 | 11.0 | 13.0 | 25.0 | 25.0 | 8.0 | 5.0 | 12.0 | 21.0 | 11.000000 | 13.0 | 8.0 | 15.0 | 11.0 | 8.0 | 17.0 | 12.0 | 19.0 | 19.0 | 13.0 | 12.0 | 13.0 | 12.0 | 17.0 | 12.0 | 8.0 | 5.0 | 6.0 | 11.0 | 15.0 | 12.0 | 10.0 | 4.0 | 2.0 | 7.0 | 7.0 | 12.0 | 5.0 | . 9 2013-07-11 | J | 1,165 | N | 13 | 22 | 13 | 23 | Y | F | 1 | 3 | 4 | Q | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | N | 1 | 3 | 0 | 5 | 2 | XD | XS | YP | XC | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | O | 1 | 0 | 1 | -1 | 21 | 4 | 2 | C | 4 | 0 | 1 | 0 | 0 | 2 | 1 | 10 | B | N | O | Y | H | N | 2 | N | Y | N | -1 | 24 | -1 | 25 | -1 | 18 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 13 | N | TX | True | False | 23.0 | 0.9691 | 0.0004 | 1.2665 | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | 3.000000 | 3.0 | 14.0 | 23.0 | 9.0 | 21.0 | 15.0 | 24.0 | 0.999999 | 8320.999063 | 2.523887e-08 | 6.0 | 7.0 | -1.000000 | -1.0 | 1.0 | 20.0 | 2.0 | 16.0 | 20.0 | 4.0 | 2.0 | 3.0 | 3.0 | 3.0 | 5.0 | 6.0 | 1.0 | 10.0 | 14.0 | -7.737822e-13 | 17.0 | 21.0 | 10.0 | 20.0 | 10.0 | 7.0 | 1.0 | 1.0 | 16.0 | 18.0 | 13.0 | 22.0 | 22.0 | 22.0 | 14.0 | 22.0 | 23.000000 | 24.0 | 14.0 | 21.0 | 21.0 | 22.0 | 15.000000 | 21.0 | 23.0 | 22.0 | 10.0 | 20.0 | 2.0 | 5.0 | 14.0 | 9.0 | 11.0 | 3.0 | 8.0 | 7.0 | 22.0 | 19.0 | 20.0 | 13.0 | 10.0 | 7.0 | 10.0 | 16.000000 | 14.0 | 12.0 | 20.0 | 8.0 | 11.0 | 10.0 | 19.0 | 7.0 | 10.0 | 4.0 | 4.0 | 5.0 | 5.0 | 10.000000 | 12.0 | 4.0 | 5.0 | 24.999999 | 25.0 | 25.0 | 25.0 | 9.0 | 14.0 | 4.0 | 5.0 | 11.0 | 17.0 | 13.0 | 9.0 | 17.0 | 22.0 | 19.0 | 23.0 | 10.0 | 9.0 | 11.0 | 18.0 | 12.000000 | 14.0 | 8.0 | 16.0 | 2.0 | 1.0 | 22.0 | 22.0 | 23.0 | 24.0 | 1.0 | 1.0 | 1.0 | 1.0 | 14.0 | 8.0 | 2.0 | 1.0 | 9.0 | 17.0 | 13.0 | 15.0 | 17.0 | 19.0 | 23.0 | 7.0 | 6.0 | 1.0 | 8.0 | . preds, _ = learn.get_preds(dl=dl_test) . type(preds) . torch.Tensor . preds.shape . torch.Size([173836, 2]) . preds[:,:1] . tensor([[0.9999], [0.9836], [0.9839], ..., [0.0022], [0.9999], [0.8141]]) . (preds[:,:1] &gt;= 0.5).sum(), (preds[:,:1] &lt; 0.5).sum() . (tensor(147304), tensor(26532)) . learn.save(&#39;qad_fastai_nn3&#39;) . Path(&#39;models/qad_fastai_nn3.pth&#39;) . Submission To Kaggle . path.ls() . (#9) [Path(&#39;homesite-quote-conversion.zip&#39;),Path(&#39;models&#39;),Path(&#39;sample_submission.csv.zip&#39;),Path(&#39;test.csv.zip&#39;),Path(&#39;train.csv.zip&#39;),Path(&#39;train.csv&#39;),Path(&#39;test.csv&#39;),Path(&#39;sample_submission.csv&#39;),Path(&#39;submission.csv&#39;)] . file_extract(path/&quot;sample_submission.csv.zip&quot;) . path.ls() . (#9) [Path(&#39;homesite-quote-conversion.zip&#39;),Path(&#39;models&#39;),Path(&#39;sample_submission.csv.zip&#39;),Path(&#39;test.csv.zip&#39;),Path(&#39;train.csv.zip&#39;),Path(&#39;train.csv&#39;),Path(&#39;test.csv&#39;),Path(&#39;sample_submission.csv&#39;),Path(&#39;submission.csv&#39;)] . df_submission = pd.read_csv(path/&quot;sample_submission.csv&quot;) #I could add `low_memory=false` but it makes things slower df_submission.head() . QuoteNumber QuoteConversion_Flag . 0 3 | 0 | . 1 5 | 0 | . 2 7 | 0 | . 3 9 | 0 | . 4 10 | 0 | . df_submission.tail() . QuoteNumber QuoteConversion_Flag . 173831 434570 | 0 | . 173832 434573 | 0 | . 173833 434574 | 0 | . 173834 434575 | 0 | . 173835 434589 | 0 | . len(df_test.index), len(preds) . (173836, 173836) . type(preds) . torch.Tensor . preds.dtype . torch.float32 . preds[0,0] . tensor(0.9999) . preds_for_submission = preds[:,:1].tolist() preds_for_submission[0:2] fpfs = [float(pfs[0]) for pfs in preds_for_submission] fpfs[0:2] . [0.9999121427536011, 0.9835898280143738] . integers = [[1], [2], [3]] strings = [int(integer[0]) for integer in integers] strings . [1, 2, 3] . submission = pd.DataFrame({&#39;QuoteNumber&#39;: df_test.index, &#39;QuoteConversion_Flag&#39;: preds[:,:1].tolist()}, columns=[&#39;QuoteNumber&#39;, &#39;QuoteConversion_Flag&#39;]) . Needed to figure out how to extract the floating point value alone from the list to properly compose the csv output dataframe . type(submission.QuoteConversion_Flag) . pandas.core.series.Series . type(submission.QuoteConversion_Flag[0]) . list . type(submission.QuoteConversion_Flag[0][0]) . float . submission.QuoteConversion_Flag[0][0] . 0.9999121427536011 . Played around with the example on list comprehension here to get it to work with what I had to work with . submission.QuoteConversion_Flag = [float(qcf[0]) for qcf in submission.QuoteConversion_Flag] . submission.head() . QuoteNumber QuoteConversion_Flag . 0 3 | 0.999912 | . 1 5 | 0.983590 | . 2 7 | 0.983852 | . 3 9 | 0.999770 | . 4 10 | 0.628180 | . submission.QuoteConversion_Flag = round(submission.QuoteConversion_Flag).astype(&#39;Int64&#39;) . submission.head() . QuoteNumber QuoteConversion_Flag . 0 3 | 1 | . 1 5 | 1 | . 2 7 | 1 | . 3 9 | 1 | . 4 10 | 1 | . len(submission[submission.QuoteConversion_Flag==1]) . 147304 . len(submission[submission.QuoteConversion_Flag==0]) . 26532 . submission.to_csv(path/&#39;submission2.csv&#39;, index=False) . api.competition_submit(path/&#39;submission.csv&#39;,message=&quot;First pass&quot;, competition=&#39;homesite-quote-conversion&#39;) . 100%|██████████| 1.45M/1.45M [00:03&lt;00:00, 428kB/s] . Successfully submitted to Homesite Quote Conversion .",
            "url": "https://redditech.github.io/team-fast-tabulous/kaggle/fastai/2021/07/23/_First_Pass_Using_Fastai_For_Homesite_Competition.html",
            "relUrl": "/kaggle/fastai/2021/07/23/_First_Pass_Using_Fastai_For_Homesite_Competition.html",
            "date": " • Jul 23, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Another Pass Using Fastai Tabular For Homesite Competition",
            "content": "Introduction . This is a modification of the &quot;first pass&quot; submission to the Homesite Competition on Kaggle Competition using Google Colab, but modifying some of the default parameters and maybe adding some learning from the initial exploratory data analysis at this time, to see if I can improve the baseline after applying what we learnt so far to see how it improves (or not) our submission then. . Changes made: . Changed from RandomSplitter() to TrainTestSplitter() for making test and validation sets more fairly weighted based on the bias of the input data towards negative results | Increase batch size to 1024 to make training shorter, but to still hopefully get a better predictor for it. Set a separate validator batch size to 128. | Increase the validation percentage to 0.25 | Fix the learning rate to 1e-2 | Increase epochs to 7, see if it overfits and what effect that has | Modified the cat_names and cont_names arrays with the initial insights from the EDA notebook post | Add a date part for dates | Add weight decay of 0.2 | . Setup fastai and Google drive . !pip install -Uqq fastai . from fastai.tabular.all import * . The snippet below is only useful in Colab for accessing my Google Drive and is straight out the fastbook source code in Github . global gdrive gdrive = Path(&#39;/content/gdrive/My Drive&#39;) from google.colab import drive if not gdrive.exists(): drive.mount(str(gdrive.parent)) . Only add the Kaggle bits below if I&#39;m running locally, in Collab they&#39;re already here . . !ls /content/gdrive/MyDrive/Kaggle/kaggle.json . /content/gdrive/MyDrive/Kaggle/kaggle.json . Useful links here: . Documentation on Path library | Documentation on fastai extensions to Path library | . Path.cwd() . Path(&#39;/content&#39;) . Setup kaggle environment parameters . !mkdir -p ~/.kaggle !cp /content/gdrive/MyDrive/Kaggle/kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . from kaggle import api . path = Path.cwd() path.ls() . (#4) [Path(&#39;/content/.config&#39;),Path(&#39;/content/models&#39;),Path(&#39;/content/gdrive&#39;),Path(&#39;/content/sample_data&#39;)] . path = path/&quot;gdrive/MyDrive/Kaggle/homesite_data&quot; path.mkdir(exist_ok=True) Path.BASE_PATH = path api.competition_download_cli(&#39;homesite-quote-conversion&#39;, path=path) file_extract(path/&quot;homesite-quote-conversion.zip&quot;) file_extract(path/&quot;train.csv.zip&quot;) file_extract(path/&quot;test.csv.zip&quot;) . homesite-quote-conversion.zip: Skipping, found more recently modified local copy (use --force to force download) . path . Path(&#39;.&#39;) . path.ls() . (#18) [Path(&#39;homesite-quote-conversion.zip&#39;),Path(&#39;models&#39;),Path(&#39;sample_submission.csv.zip&#39;),Path(&#39;test.csv.zip&#39;),Path(&#39;train.csv.zip&#39;),Path(&#39;train.csv&#39;),Path(&#39;test.csv&#39;),Path(&#39;sample_submission.csv&#39;),Path(&#39;submission.csv&#39;),Path(&#39;submission2.csv&#39;)...] . Exploring the Homesite data . First set the random seed so that the results are reproducible . set_seed(42) bs = 1024 val_bs = 128 test_size = 0.25 epochs = 7 lr = 1e-2 wd=0.2 . df_train = pd.read_csv(path/&quot;train.csv&quot;, low_memory=False) df_train.head() . QuoteNumber Original_Quote_Date QuoteConversion_Flag Field6 Field7 Field8 Field9 Field10 Field11 Field12 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField3 SalesField4 SalesField5 SalesField6 SalesField7 SalesField8 SalesField9 SalesField10 SalesField11 SalesField12 ... GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56A GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60A GeographicField60B GeographicField61A GeographicField61B GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 1 | 2013-08-16 | 0 | B | 23 | 0.9403 | 0.0006 | 965 | 1.0200 | N | 17 | 23 | 17 | 23 | 15 | 22 | 16 | 22 | 13 | 22 | 13 | 23 | T | D | 2 | 1 | 7 | 18 | 3 | 8 | 0 | 5 | 5 | 24 | V | 48649 | 0 | 0 | 0 | 0 | ... | 8 | 4 | 20 | 22 | 10 | 8 | 6 | 5 | 15 | 13 | 19 | 18 | 16 | 14 | 21 | 23 | 21 | 23 | 16 | 11 | 22 | 24 | 7 | 14 | -1 | 17 | 15 | 17 | 14 | 18 | 9 | 9 | -1 | 8 | -1 | 18 | -1 | 10 | N | CA | . 1 2 | 2014-04-22 | 0 | F | 7 | 1.0006 | 0.0040 | 548 | 1.2433 | N | 6 | 8 | 6 | 8 | 5 | 7 | 5 | 8 | 13 | 22 | 13 | 23 | T | E | 5 | 9 | 5 | 14 | 6 | 18 | 1 | 5 | 5 | 11 | P | 26778 | 0 | 0 | 1 | 1 | ... | 23 | 24 | 11 | 15 | 21 | 24 | 6 | 11 | 21 | 21 | 18 | 15 | 20 | 20 | 13 | 12 | 12 | 12 | 15 | 9 | 13 | 11 | 11 | 20 | -1 | 9 | 18 | 21 | 8 | 7 | 10 | 10 | -1 | 11 | -1 | 17 | -1 | 20 | N | NJ | . 2 4 | 2014-08-25 | 0 | F | 7 | 1.0006 | 0.0040 | 548 | 1.2433 | N | 7 | 12 | 7 | 12 | 6 | 10 | 7 | 11 | 25 | 25 | 13 | 23 | T | J | 4 | 6 | 3 | 10 | 4 | 11 | 1 | 5 | 5 | 11 | K | 8751 | 0 | 0 | 2 | 2 | ... | 21 | 22 | 24 | 25 | 20 | 22 | 7 | 13 | 23 | 23 | 20 | 19 | 20 | 20 | 18 | 20 | 19 | 21 | 20 | 19 | 11 | 8 | 3 | 3 | -1 | 5 | 21 | 24 | 12 | 15 | 15 | 18 | -1 | 21 | -1 | 11 | -1 | 8 | N | NJ | . 3 6 | 2013-04-15 | 0 | J | 10 | 0.9769 | 0.0004 | 1,165 | 1.2665 | N | 3 | 2 | 3 | 2 | 2 | 2 | 3 | 2 | 13 | 22 | 13 | 23 | Y | F | 15 | 23 | 8 | 19 | 14 | 24 | 0 | 5 | 5 | 23 | V | 43854 | 0 | 0 | 0 | 0 | ... | 3 | 1 | 14 | 22 | 6 | 2 | 7 | 14 | 11 | 8 | 19 | 18 | 18 | 16 | 13 | 12 | 13 | 12 | 17 | 13 | 5 | 2 | 3 | 4 | -1 | 7 | 14 | 14 | 14 | 18 | 6 | 5 | -1 | 10 | -1 | 9 | -1 | 21 | N | TX | . 4 8 | 2014-01-25 | 0 | E | 23 | 0.9472 | 0.0006 | 1,487 | 1.3045 | N | 8 | 13 | 8 | 13 | 7 | 11 | 7 | 13 | 13 | 22 | 13 | 23 | T | F | 4 | 6 | 3 | 6 | 3 | 6 | 1 | 5 | 5 | 7 | R | 12505 | 1 | 0 | 0 | 0 | ... | 24 | 25 | 9 | 11 | 25 | 25 | 5 | 3 | 22 | 22 | 21 | 21 | 17 | 15 | 25 | 25 | 25 | 25 | 17 | 13 | 13 | 11 | 3 | 4 | -1 | 7 | 11 | 9 | 10 | 10 | 18 | 22 | -1 | 10 | -1 | 11 | -1 | 12 | N | IL | . 5 rows × 299 columns . df_train.shape . (260753, 299) . df_test = pd.read_csv(path/&quot;test.csv&quot;, low_memory=False) df_test.head() . QuoteNumber Original_Quote_Date Field6 Field7 Field8 Field9 Field10 Field11 Field12 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField3 SalesField4 SalesField5 SalesField6 SalesField7 SalesField8 SalesField9 SalesField10 SalesField11 SalesField12 SalesField13 ... GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56A GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60A GeographicField60B GeographicField61A GeographicField61B GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 3 | 2014-08-12 | E | 16 | 0.9364 | 0.0006 | 1,487 | 1.3045 | N | 4 | 4 | 4 | 4 | 3 | 3 | 3 | 4 | 13 | 22 | 13 | 23 | Y | K | 13 | 22 | 6 | 16 | 9 | 21 | 0 | 5 | 5 | 11 | P | 67052 | 0 | 0 | 0 | 0 | 0 | ... | 22 | 23 | 9 | 12 | 25 | 25 | 6 | 9 | 4 | 2 | 16 | 12 | 20 | 20 | 2 | 2 | 2 | 1 | 1 | 1 | 10 | 7 | 25 | 25 | -1 | 19 | 19 | 22 | 12 | 15 | 1 | 1 | -1 | 1 | -1 | 20 | -1 | 25 | Y | IL | . 1 5 | 2013-09-07 | F | 11 | 0.9919 | 0.0038 | 564 | 1.1886 | N | 8 | 14 | 8 | 14 | 7 | 12 | 8 | 13 | 13 | 22 | 13 | 23 | T | E | 4 | 5 | 3 | 6 | 3 | 6 | 1 | 5 | 5 | 4 | R | 27288 | 1 | 0 | 0 | 0 | 0 | ... | 23 | 24 | 12 | 21 | 23 | 25 | 7 | 11 | 16 | 14 | 13 | 6 | 17 | 15 | 7 | 5 | 7 | 5 | 13 | 7 | 14 | 14 | 7 | 14 | -1 | 4 | 1 | 1 | 5 | 3 | 10 | 10 | -1 | 5 | -1 | 5 | -1 | 21 | N | NJ | . 2 7 | 2013-03-29 | F | 15 | 0.8945 | 0.0038 | 564 | 1.0670 | N | 11 | 18 | 11 | 18 | 10 | 16 | 10 | 18 | 13 | 22 | 13 | 23 | T | E | 3 | 3 | 5 | 14 | 3 | 9 | 1 | 5 | 5 | 23 | V | 65264 | 0 | 1 | 2 | 2 | 0 | ... | 16 | 18 | 9 | 10 | 14 | 16 | 6 | 8 | 20 | 19 | 17 | 14 | 16 | 13 | 20 | 22 | 20 | 22 | 20 | 18 | 10 | 7 | 4 | 7 | -1 | 11 | 13 | 12 | 18 | 22 | 10 | 11 | -1 | 20 | -1 | 22 | -1 | 11 | N | NJ | . 3 9 | 2015-03-21 | K | 21 | 0.8870 | 0.0004 | 1,113 | 1.2665 | Y | 14 | 22 | 15 | 22 | 13 | 20 | 22 | 25 | 13 | 22 | 13 | 23 | Y | F | 5 | 9 | 9 | 20 | 5 | 16 | 1 | 5 | 5 | 11 | R | 32725 | 1 | 1 | 1 | 1 | 0 | ... | 11 | 11 | 9 | 10 | 11 | 13 | 15 | 21 | 14 | 12 | 17 | 13 | 10 | 6 | 20 | 22 | 20 | 22 | 19 | 16 | 12 | 11 | 4 | 6 | -1 | 13 | 10 | 8 | 5 | 3 | 8 | 8 | -1 | 13 | -1 | 8 | -1 | 21 | N | TX | . 4 10 | 2014-12-10 | B | 25 | 0.9153 | 0.0007 | 935 | 1.0200 | N | 4 | 5 | 4 | 5 | 4 | 4 | 4 | 5 | 13 | 22 | 13 | 23 | Y | D | 12 | 21 | 1 | 1 | 3 | 6 | 0 | 5 | 5 | 11 | T | 56025 | 0 | 1 | 1 | 1 | 0 | ... | 9 | 8 | 25 | 25 | 9 | 3 | 9 | 18 | 7 | 4 | 16 | 12 | 13 | 9 | 8 | 6 | 8 | 6 | 11 | 5 | 19 | 21 | 13 | 21 | -1 | 23 | 11 | 8 | 5 | 3 | 7 | 7 | -1 | 3 | -1 | 22 | -1 | 21 | N | CA | . 5 rows × 298 columns . df_test.shape . (173836, 298) . y_column = df_train.columns.difference(df_test.columns) . y_column . Index([&#39;QuoteConversion_Flag&#39;], dtype=&#39;object&#39;) . From this it looks like QuoteConversion_Flag is the value we want to predict. Let&#39;s take a look at this . type(df_train.QuoteConversion_Flag) . pandas.core.series.Series . df_train.QuoteConversion_Flag.unique() . array([0, 1]) . type(df_train.QuoteConversion_Flag.unique()[0]) . numpy.int64 . Make this a boolean for the purpose of generating predictions as a binary classification . df_train.QuoteConversion_Flag = df_train.QuoteConversion_Flag.astype(dtype=&#39;boolean&#39;) . Let&#39;s see how the training data outcomes are balanced . df_train.QuoteConversion_Flag.describe() . count 260753 unique 2 top False freq 211859 Name: QuoteConversion_Flag, dtype: object . train_data_balance = pd.DataFrame(df_train[&quot;QuoteConversion_Flag&quot;]).groupby(&quot;QuoteConversion_Flag&quot;) . train_data_balance[&quot;QuoteConversion_Flag&quot;].describe() . count unique top freq . QuoteConversion_Flag . False 211859 | 1 | False | 211859 | . True 48894 | 1 | True | 48894 | . We have about 5 times as many &quot;No Sale&quot; data rows as we do data that shows a successful sale happened. This data bias may have an impact on the effectiveness of our model to predict positive sales results . First things first . Learning from my colleague Tim&#39;s work already we know: . Quotenumber is unique so we can make it the index | Original_Quote_Date column should be set as a date type | . Additionally, we should make sure to apply any changes to data types to both train and test data so predictions don&#39;t fail later on . df_train = df_train.set_index(&#39;QuoteNumber&#39;) df_test = df_test.set_index(&#39;QuoteNumber&#39;) . We may have some NaN values for Original_Quote_Date in either the training or test dataset, but let&#39;s confirm there are none. . df_train[&#39;Original_Quote_Date&#39;].isna().sum(), df_test[&#39;Original_Quote_Date&#39;].isna().sum() . (0, 0) . df_train[&#39;Original_Quote_Date&#39;] = pd.to_datetime(df_train[&#39;Original_Quote_Date&#39;]) df_test[&#39;Original_Quote_Date&#39;] = pd.to_datetime(df_test[&#39;Original_Quote_Date&#39;]) . Add the date_part to see if this helps improve modeling . df_train = add_datepart(df_train, &#39;Original_Quote_Date&#39;) df_test = add_datepart(df_test, &#39;Original_Quote_Date&#39;) . Goal: Better model training, refining fastai parameters, using EDA insights gathered to date . y_names = [y_column[0]] y_names . [&#39;QuoteConversion_Flag&#39;] . cont_names, cat_names = cont_cat_split(df_train, dep_var=y_names) len(cont_names), len(cat_names) . (155, 154) . Modifying these lists here based on EDA notebook learnings to date . &#39;Field8&#39; in cont_names . True . &#39;Field9&#39; in cat_names, &#39;Field9&#39; in cont_names . (False, True) . field9_categories = df_train[&#39;Field9&#39;].unique() df_train[&#39;Field9&#39;] = df_train[&#39;Field9&#39;].astype(&#39;category&#39;) df_train[&#39;Field9&#39;].cat.set_categories(field9_categories, inplace=True) . cont_names.remove(&#39;Field9&#39;) cat_names.append(&#39;Field9&#39;) . &#39;Field11&#39; in cat_names, &#39;Field11&#39; in cont_names . (False, True) . field11_categories = df_train[&#39;Field11&#39;].unique() df_train[&#39;Field11&#39;] = df_train[&#39;Field11&#39;].astype(&#39;category&#39;) df_train[&#39;Field11&#39;].cat.set_categories(field11_categories, inplace=True) . cont_names.remove(&#39;Field11&#39;) cat_names.append(&#39;Field11&#39;) . &#39;PropertyField25&#39; in cat_names, &#39;PropertyField25&#39; in cont_names . (False, True) . propertyfield25_categories = df_train[&#39;PropertyField25&#39;].unique() df_train[&#39;PropertyField25&#39;] = df_train[&#39;PropertyField25&#39;].astype(&#39;category&#39;) df_train[&#39;PropertyField25&#39;].cat.set_categories(propertyfield25_categories, inplace=True) . cont_names.remove(&#39;PropertyField25&#39;) cat_names.append(&#39;PropertyField25&#39;) . df_train.drop(&#39;PropertyField29&#39;, axis=1, inplace=True) df_train.drop(&#39;PersonalField84&#39;, axis=1, inplace=True) . cont_names.remove(&#39;PersonalField84&#39;) cont_names.remove(&#39;PropertyField29&#39;) . &quot;QuoteConversion_Flag&quot; in cont_names, &quot;QuoteConversion_Flag&quot; in cat_names #Make sure we&#39;ve gotten our y-column excluded . (False, False) . procs = [Categorify, FillMissing, Normalize] splits = TrainTestSplitter(test_size=test_size, stratify=df_train[y_names])(df_train) . to = TabularPandas(df=df_train, procs=procs, cat_names=cat_names, cont_names=cont_names, y_names=y_names,splits=splits) dls = to.dataloaders(bs=bs, val_bs=val_bs) dls.valid.show_batch() . Field6 Field10 Field12 CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 SalesField3 SalesField4 SalesField5 SalesField7 SalesField9 SalesField10 SalesField11 SalesField13 SalesField14 SalesField15 PersonalField1 PersonalField2 PersonalField5 PersonalField6 PersonalField7 PersonalField8 PersonalField9 PersonalField11 PersonalField12 PersonalField13 PersonalField16 PersonalField17 PersonalField18 PersonalField19 PersonalField22 PersonalField23 PersonalField24 PersonalField25 PersonalField26 PersonalField27 PersonalField28 PersonalField29 PersonalField30 PersonalField31 PersonalField32 PersonalField33 PersonalField34 PersonalField35 PersonalField36 PersonalField37 PersonalField38 PersonalField39 PersonalField40 PersonalField41 PersonalField42 PersonalField43 PersonalField44 PersonalField45 PersonalField46 PersonalField47 PersonalField48 PersonalField49 PersonalField50 PersonalField51 PersonalField52 PersonalField53 PersonalField54 PersonalField55 PersonalField56 PersonalField57 PersonalField58 PersonalField59 PersonalField60 PersonalField61 PersonalField62 PersonalField63 PersonalField64 PersonalField65 PersonalField66 PersonalField67 PersonalField68 PersonalField69 PersonalField70 PersonalField71 PersonalField72 PersonalField73 PersonalField74 PersonalField75 PersonalField76 PersonalField77 PersonalField78 PersonalField79 PersonalField80 PersonalField81 PersonalField82 PersonalField83 PropertyField2A PropertyField3 PropertyField4 PropertyField5 PropertyField6 PropertyField7 PropertyField8 PropertyField9 PropertyField10 PropertyField11A PropertyField11B PropertyField12 PropertyField13 PropertyField14 PropertyField15 PropertyField17 PropertyField18 PropertyField19 PropertyField20 PropertyField22 PropertyField23 PropertyField27 PropertyField28 PropertyField30 PropertyField31 PropertyField32 PropertyField33 PropertyField34 PropertyField35 PropertyField36 PropertyField37 PropertyField38 GeographicField5A GeographicField5B GeographicField10A GeographicField10B GeographicField14A GeographicField14B GeographicField18A GeographicField21A GeographicField22A GeographicField22B GeographicField23A GeographicField56A GeographicField60A GeographicField61A GeographicField62A GeographicField62B GeographicField63 GeographicField64 Original_Quote_Year Original_Quote_Month Original_Quote_Dayofweek Original_Quote_Is_month_end Original_Quote_Is_month_start Original_Quote_Is_quarter_end Original_Quote_Is_quarter_start Original_Quote_Is_year_end Original_Quote_Is_year_start Field9 Field11 PropertyField25 Field7 Field8 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField6 SalesField8 SalesField12 PersonalField4A PersonalField4B PersonalField10A PersonalField10B PersonalField14 PersonalField15 PropertyField1A PropertyField1B PropertyField2B PropertyField16A PropertyField16B PropertyField21A PropertyField21B PropertyField24A PropertyField24B PropertyField26A PropertyField26B PropertyField39A PropertyField39B GeographicField1A GeographicField1B GeographicField2A GeographicField2B GeographicField3A GeographicField3B GeographicField4A GeographicField4B GeographicField6A GeographicField6B GeographicField7A GeographicField7B GeographicField8A GeographicField8B GeographicField9A GeographicField9B GeographicField11A GeographicField11B GeographicField12A GeographicField12B GeographicField13A GeographicField13B GeographicField15A GeographicField15B GeographicField16A GeographicField16B GeographicField17A GeographicField17B GeographicField18B GeographicField19A GeographicField19B GeographicField20A GeographicField20B GeographicField21B GeographicField23B GeographicField24A GeographicField24B GeographicField25A GeographicField25B GeographicField26A GeographicField26B GeographicField27A GeographicField27B GeographicField28A GeographicField28B GeographicField29A GeographicField29B GeographicField30A GeographicField30B GeographicField31A GeographicField31B GeographicField32A GeographicField32B GeographicField33A GeographicField33B GeographicField34A GeographicField34B GeographicField35A GeographicField35B GeographicField36A GeographicField36B GeographicField37A GeographicField37B GeographicField38A GeographicField38B GeographicField39A GeographicField39B GeographicField40A GeographicField40B GeographicField41A GeographicField41B GeographicField42A GeographicField42B GeographicField43A GeographicField43B GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60B GeographicField61B Original_Quote_Week Original_Quote_Day Original_Quote_Dayofyear Original_Quote_Elapsed QuoteConversion_Flag . 0 B | 935 | N | 13 | 22 | 13 | 23 | T | D | 1 | 5 | 5 | K | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | Y | Y | 0 | O | 0 | 0 | 1 | -1 | 21 | 4 | 3 | C | 4 | 1 | 3 | 0 | 0 | 2 | 2 | 4 | B | N | N | Y | G | Y | 1 | N | N | N | -1 | 13 | -1 | 25 | -1 | 9 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 12 | N | CA | 2014 | 2 | 0 | False | False | False | False | False | False | 0.0007 | 1.02 | 3 | 23.0 | 0.9403 | 25.000000 | 25.0 | 25.0 | 25.0 | 22.000000 | 25.0 | 24.0 | 25.0 | 1.0 | 1.0 | 8.0 | 19.0 | 2.0 | 4.0 | 11.0 | 42316.000014 | 1.000000e+00 | 3.0 | 3.0 | 9.0 | 18.0 | 4.0 | 24.0 | 4.0 | 6.0 | 17.0 | 10.0 | 19.0 | 25.000001 | 25.0 | 18.0 | 23.0 | 7.0 | 8.0 | 3.0 | 2.0 | 7.0 | 15.0 | 11.0 | 9.0 | 11.0 | 13.0 | 25.0 | 25.0 | 2.0 | 9.0 | 6.0 | 7.0 | 2.0 | 7.0 | 2.0 | 5.0 | 2.0 | 9.0 | 4.0 | 7.0 | 2.0 | 8.0 | 8.0 | 7.0 | 3.0 | 7.0 | 10.000000 | 21.0 | 9.0 | 2.0 | 3.0 | 3.000000 | 6.0 | 4.0 | 2.0 | 13.0 | 8.0 | 16.0 | 15.0 | 16.0 | 20.0 | 15.0 | 11.0 | 6.0 | 9.0 | 6.0 | 6.0 | 5.0 | 4.0 | 7.000000 | 8.0 | 16.0 | 22.0 | 14.0 | 21.0 | 6.000000 | 5.0 | 6.0 | 9.0 | 5.0 | 4.0 | 7.0 | 17.0 | 12.0 | 19.0 | 8.000000 | 16.0 | 3.0 | 8.0 | 2.0 | 1.0 | 5.0 | 3.0 | 10.0 | 11.0 | 15.0 | 17.0 | 2.0 | 4.0 | 10.0 | 12.0 | 21.000000 | 23.0 | 13.0 | 10.0 | 14.0 | 7.0 | 6.0 | 3.0 | 19.0 | 21.0 | 18.0 | 20.0 | 14.0 | 8.0 | 21.0 | 23.0 | 12.0 | 20.0 | 21.0 | 6.0 | 3.0 | 6.0 | 4.0 | 9.0 | 9.0 | 16.0 | 12.0 | 6.999999 | 10.0 | 40.999994 | 1.391990e+09 | False | . 1 B | 965 | N | 13 | 22 | 13 | 23 | T | D | 1 | 5 | 5 | Q | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | N | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | Y | Y | 0 | O | 0 | 0 | 1 | -1 | 21 | 4 | 2 | C | 4 | 1 | 3 | 0 | 0 | 2 | 4 | 10 | B | N | N | Y | E | Y | 1 | N | N | N | -1 | 13 | -1 | 25 | -1 | 8 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 24 | N | CA | 2013 | 11 | 3 | False | False | False | False | False | False | 0.0006 | 1.02 | 1 | 23.0 | 0.9403 | 23.999999 | 25.0 | 24.0 | 25.0 | 21.000000 | 24.0 | 23.0 | 25.0 | 1.0 | 1.0 | 12.0 | 22.0 | 4.0 | 13.0 | 7.0 | 59943.999147 | 1.000000e+00 | 19.0 | 23.0 | 16.0 | 21.0 | 4.0 | 24.0 | 17.0 | 22.0 | 4.0 | 9.0 | 18.0 | 24.000000 | 25.0 | 17.0 | 22.0 | 25.0 | 25.0 | 3.0 | 2.0 | 11.0 | 21.0 | 12.0 | 11.0 | 4.0 | 3.0 | 11.0 | 10.0 | 2.0 | 10.0 | 2.0 | 3.0 | 2.0 | 4.0 | 2.0 | 7.0 | 2.0 | 10.0 | 2.0 | 4.0 | 2.0 | 7.0 | 2.0 | 2.0 | 2.0 | 2.0 | 21.000001 | 25.0 | 24.0 | 2.0 | 4.0 | 3.000000 | 7.0 | 4.0 | 5.0 | 13.0 | 8.0 | 16.0 | 15.0 | 16.0 | 20.0 | 15.0 | 11.0 | 13.0 | 20.0 | 20.0 | 24.0 | 11.0 | 20.0 | 16.000000 | 23.0 | 7.0 | 11.0 | 20.0 | 24.0 | 13.000000 | 17.0 | 17.0 | 24.0 | 14.0 | 22.0 | 10.0 | 21.0 | 10.0 | 15.0 | 8.000000 | 15.0 | 3.0 | 8.0 | 1.0 | 1.0 | 7.0 | 8.0 | 7.0 | 6.0 | 15.0 | 16.0 | 2.0 | 2.0 | 9.0 | 4.0 | 7.000000 | 14.0 | 8.0 | 5.0 | 12.0 | 5.0 | 12.0 | 8.0 | 4.0 | 3.0 | 4.0 | 3.0 | 9.0 | 4.0 | 12.0 | 10.0 | 18.0 | 23.0 | 21.0 | 16.0 | 19.0 | 16.0 | 21.0 | 5.0 | 3.0 | 6.0 | 17.0 | 46.000001 | 14.0 | 317.999998 | 1.384387e+09 | False | . 2 B | 965 | N | 13 | 22 | 13 | 23 | X | D | 1 | 5 | 5 | Q | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 3 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | O | 1 | 0 | 2 | -1 | 21 | 4 | 2 | C | 1 | 1 | 2 | 0 | 0 | 2 | 4 | 10 | B | N | N | Y | H | N | 1 | N | N | N | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 12 | N | CA | 2013 | 11 | 1 | False | False | False | False | False | False | 0.0006 | 1.02 | 1 | 25.0 | 0.9403 | 14.000000 | 21.0 | 14.0 | 21.0 | 12.000000 | 19.0 | 13.0 | 20.0 | 14.0 | 22.0 | 4.0 | 11.0 | 2.0 | 3.0 | 20.0 | 3553.999261 | -3.226606e-08 | 6.0 | 8.0 | 5.0 | 4.0 | 5.0 | 24.0 | 6.0 | 9.0 | 22.0 | 4.0 | 9.0 | 14.000000 | 21.0 | 9.0 | 14.0 | 16.0 | 22.0 | 4.0 | 2.0 | 7.0 | 14.0 | 8.0 | 5.0 | 8.0 | 8.0 | 20.0 | 22.0 | 2.0 | 4.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 3.0 | 2.0 | 7.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 18.000000 | 24.0 | 21.0 | 2.0 | 2.0 | 2.000000 | 3.0 | 11.0 | 5.0 | 15.0 | 11.0 | 19.0 | 20.0 | 20.0 | 22.0 | 17.0 | 16.0 | 6.0 | 10.0 | 11.0 | 17.0 | 23.0 | 25.0 | 3.000000 | 2.0 | 13.0 | 21.0 | 19.0 | 24.0 | 6.000000 | 5.0 | 24.0 | 25.0 | 6.0 | 6.0 | 7.0 | 17.0 | 18.0 | 24.0 | 8.000000 | 16.0 | 2.0 | 3.0 | 3.0 | 4.0 | 4.0 | 3.0 | 8.0 | 8.0 | 10.0 | 9.0 | 3.0 | 5.0 | 10.0 | 8.0 | 25.000001 | 25.0 | 8.0 | 5.0 | 14.0 | 8.0 | 12.0 | 7.0 | 12.0 | 11.0 | 11.0 | 10.0 | 7.0 | 3.0 | 20.0 | 22.0 | 16.0 | 23.0 | 19.0 | 9.0 | 6.0 | 22.0 | 24.0 | 6.0 | 5.0 | 7.0 | 17.0 | 44.999999 | 5.0 | 308.999998 | 1.383610e+09 | True | . 3 C | 1,487 | Y | 13 | 22 | 25 | 25 | V | A | 1 | 5 | 5 | P | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | XR | XS | YP | XC | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | A | 1 | 0 | 1 | -1 | 25 | 1 | 3 | C | 5 | 1 | 3 | 1 | 0 | 2 | 13 | 7 | B | Y | K | N | E | N | 0 | N | N | N | -1 | 13 | -1 | 25 | -1 | 20 | -1 | 25 | -1 | 23 | -1 | -1 | -1 | -1 | -1 | 8 | N | IL | 2015 | 5 | 6 | False | False | False | False | False | False | 0.0006 | 1.3045 | 2 | 17.0 | 0.8746 | 25.000000 | 25.0 | 25.0 | 25.0 | 25.000001 | 25.0 | 25.0 | 25.0 | 12.0 | 21.0 | 7.0 | 18.0 | 1.0 | 1.0 | 11.0 | 7220.998942 | -3.226606e-08 | 16.0 | 20.0 | 8.0 | 18.0 | 3.0 | 4.0 | 6.0 | 9.0 | 23.0 | 12.0 | 21.0 | 25.000001 | 25.0 | 23.0 | 25.0 | 16.0 | 21.0 | 2.0 | 1.0 | 3.0 | 7.0 | 16.0 | 16.0 | 24.0 | 25.0 | 19.0 | 21.0 | 10.0 | 18.0 | 12.0 | 17.0 | 10.0 | 18.0 | 18.0 | 21.0 | 9.0 | 18.0 | 13.0 | 17.0 | 9.0 | 18.0 | 10.0 | 12.0 | 9.0 | 18.0 | 2.000000 | 9.0 | 19.0 | 20.0 | 19.0 | 5.000000 | 16.0 | 25.0 | 16.0 | 5.0 | 2.0 | 5.0 | 2.0 | 3.0 | 3.0 | 6.0 | 2.0 | 4.0 | 4.0 | 7.0 | 8.0 | 7.0 | 10.0 | 7.000000 | 8.0 | 10.0 | 17.0 | 13.0 | 20.0 | 10.000000 | 12.0 | 14.0 | 22.0 | 9.0 | 13.0 | 5.0 | 12.0 | 7.0 | 8.0 | 6.000000 | 11.0 | 5.0 | 10.0 | 16.0 | 14.0 | 9.0 | 10.0 | 10.0 | 13.0 | 19.0 | 20.0 | 10.0 | 13.0 | 13.0 | 15.0 | 6.000000 | 7.0 | 20.0 | 20.0 | 20.0 | 20.0 | 17.0 | 15.0 | 21.0 | 23.0 | 21.0 | 23.0 | 20.0 | 19.0 | 19.0 | 21.0 | 5.0 | 9.0 | 9.0 | 22.0 | 24.0 | 14.0 | 17.0 | 13.0 | 16.0 | 24.0 | 2.0 | 19.000000 | 10.0 | 130.000000 | 1.431216e+09 | False | . 4 F | 548 | N | 13 | 22 | 13 | 23 | T | E | 1 | 5 | 5 | Q | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | XH | XG | XF | ZT | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | R | 1 | 0 | 1 | -1 | 21 | 2 | 2 | C | 4 | 0 | 1 | 0 | 0 | 2 | 1 | 10 | B | N | O | Y | H | Y | 1 | N | N | N | -1 | 22 | -1 | 25 | -1 | 18 | -1 | -1 | -1 | 16 | -1 | -1 | -1 | -1 | -1 | 17 | N | NJ | 2014 | 3 | 3 | False | False | False | False | False | False | 0.004 | 1.1886 | 1 | 11.0 | 0.9566 | 3.000000 | 2.0 | 3.0 | 3.0 | 2.000000 | 2.0 | 3.0 | 2.0 | 7.0 | 14.0 | 4.0 | 11.0 | 7.0 | 20.0 | 11.0 | 64502.998849 | -3.226606e-08 | 8.0 | 10.0 | 11.0 | 19.0 | 5.0 | 10.0 | 8.0 | 12.0 | 15.0 | 4.0 | 6.0 | 3.000000 | 2.0 | 3.0 | 2.0 | 7.0 | 9.0 | 12.0 | 13.0 | 3.0 | 7.0 | 8.0 | 6.0 | 8.0 | 8.0 | 10.0 | 9.0 | 5.0 | 15.0 | 11.0 | 16.0 | 5.0 | 15.0 | 6.0 | 12.0 | 4.0 | 10.0 | 10.0 | 16.0 | 4.0 | 10.0 | 12.0 | 19.0 | 8.0 | 17.0 | 2.000000 | 8.0 | 11.0 | 24.0 | 25.0 | 25.000001 | 25.0 | 16.0 | 18.0 | 25.0 | 25.0 | 24.0 | 25.0 | 16.0 | 18.0 | 25.0 | 25.0 | 1.0 | 1.0 | 4.0 | 3.0 | 5.0 | 5.0 | 8.000000 | 12.0 | 4.0 | 4.0 | 5.0 | 5.0 | 7.000000 | 6.0 | 11.0 | 19.0 | 5.0 | 4.0 | 6.0 | 15.0 | 3.0 | 2.0 | 7.000000 | 13.0 | 9.0 | 14.0 | 24.0 | 24.0 | 12.0 | 15.0 | 10.0 | 13.0 | 12.0 | 12.0 | 8.0 | 8.0 | 10.0 | 11.0 | 7.000000 | 11.0 | 10.0 | 7.0 | 5.0 | 2.0 | 3.0 | 1.0 | 12.0 | 11.0 | 10.0 | 9.0 | 6.0 | 2.0 | 20.0 | 22.0 | 12.0 | 20.0 | 18.0 | 9.0 | 5.0 | 8.0 | 7.0 | 13.0 | 16.0 | 5.0 | 11.0 | 11.000000 | 13.0 | 71.999999 | 1.394669e+09 | False | . 5 B | 965 | N | 13 | 22 | 13 | 23 | T | D | 0 | 5 | 5 | T | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 3 | 0 | 5 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | O | 1 | 0 | 1 | -1 | 21 | 2 | 1 | A | 1 | 0 | 1 | 0 | 0 | 2 | 1 | 10 | B | N | O | Y | G | Y | 2 | N | Y | N | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 19 | N | CA | 2013 | 9 | 3 | False | False | False | False | False | False | 0.0006 | 1.02 | 1 | 24.0 | 0.9403 | 1.000000 | 1.0 | 1.0 | 1.0 | 1.000000 | 1.0 | 1.0 | 1.0 | 10.0 | 20.0 | 2.0 | 1.0 | 5.0 | 17.0 | 4.0 | 37300.000122 | 1.000000e+00 | 1.0 | 1.0 | -1.0 | -1.0 | 4.0 | 24.0 | 19.0 | 23.0 | 15.0 | 1.0 | 1.0 | 1.000000 | 1.0 | 1.0 | 1.0 | 4.0 | 4.0 | 12.0 | 15.0 | 9.0 | 18.0 | 10.0 | 8.0 | 2.0 | 1.0 | 17.0 | 19.0 | 2.0 | 3.0 | 7.0 | 8.0 | 2.0 | 8.0 | 2.0 | 5.0 | 2.0 | 4.0 | 5.0 | 8.0 | 2.0 | 8.0 | 9.0 | 12.0 | 3.0 | 8.0 | 2.000000 | 16.0 | 9.0 | 4.0 | 8.0 | 6.000000 | 16.0 | 4.0 | 9.0 | 25.0 | 25.0 | 25.0 | 25.0 | 25.0 | 25.0 | 24.0 | 25.0 | 9.0 | 16.0 | 10.0 | 14.0 | 13.0 | 23.0 | 7.000000 | 10.0 | 6.0 | 9.0 | 7.0 | 8.0 | 16.000000 | 21.0 | 2.0 | 2.0 | 14.0 | 21.0 | 7.0 | 16.0 | 13.0 | 21.0 | 2.000000 | 2.0 | 9.0 | 14.0 | 18.0 | 17.0 | 10.0 | 11.0 | 6.0 | 6.0 | 12.0 | 12.0 | 25.0 | 25.0 | 10.0 | 9.0 | 5.000000 | 3.0 | 8.0 | 5.0 | 12.0 | 5.0 | 12.0 | 8.0 | 7.0 | 5.0 | 7.0 | 5.0 | 13.0 | 7.0 | 16.0 | 16.0 | 11.0 | 19.0 | 16.0 | 15.0 | 17.0 | 1.0 | 1.0 | 5.0 | 4.0 | 3.0 | 24.0 | 37.000000 | 12.0 | 255.000001 | 1.378944e+09 | False | . 6 B | 965 | N | 13 | 22 | 13 | 23 | T | D | 1 | 5 | 5 | T | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 3 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 3 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 3 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | O | 0 | 0 | 1 | -1 | 21 | 2 | 2 | C | 1 | 1 | 2 | 0 | 0 | 2 | 1 | 10 | B | N | N | Y | G | Y | 1 | N | N | N | -1 | 13 | -1 | 25 | -1 | 8 | -1 | -1 | -1 | 15 | -1 | -1 | 25 | -1 | -1 | 9 | N | CA | 2013 | 3 | 2 | False | False | False | False | False | False | 0.0006 | 1.02 | 1 | 23.0 | 0.9403 | 9.000000 | 15.0 | 9.0 | 15.0 | 8.000000 | 13.0 | 8.0 | 14.0 | 3.0 | 5.0 | 6.0 | 17.0 | 5.0 | 17.0 | 23.0 | 48178.000355 | -3.226606e-08 | 11.0 | 14.0 | 8.0 | 17.0 | 1.0 | 24.0 | 15.0 | 20.0 | 8.0 | 15.0 | 23.0 | 9.000000 | 15.0 | 9.0 | 14.0 | 16.0 | 22.0 | 10.0 | 11.0 | 3.0 | 6.0 | 23.0 | 24.0 | 12.0 | 14.0 | 11.0 | 9.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 3.0 | 2.0 | 4.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 14.000000 | 23.0 | 22.0 | 2.0 | 4.0 | 3.000000 | 8.0 | 11.0 | 3.0 | 15.0 | 11.0 | 19.0 | 20.0 | 20.0 | 22.0 | 17.0 | 16.0 | 18.0 | 23.0 | 25.0 | 25.0 | 7.0 | 9.0 | 20.999999 | 24.0 | 19.0 | 24.0 | 25.0 | 25.0 | 23.000001 | 25.0 | 25.0 | 25.0 | 13.0 | 21.0 | 11.0 | 22.0 | 13.0 | 20.0 | 21.999999 | 25.0 | 2.0 | 3.0 | 3.0 | 5.0 | 8.0 | 9.0 | 14.0 | 22.0 | 14.0 | 15.0 | 2.0 | 3.0 | 8.0 | 3.0 | 24.000000 | 25.0 | 23.0 | 23.0 | 22.0 | 23.0 | 23.0 | 24.0 | 17.0 | 18.0 | 18.0 | 19.0 | 22.0 | 21.0 | 14.0 | 13.0 | 3.0 | 5.0 | 4.0 | 10.0 | 7.0 | 12.0 | 14.0 | 17.0 | 21.0 | 25.0 | 4.0 | 11.000000 | 13.0 | 71.999999 | 1.363133e+09 | False | . 7 J | 1,113 | N | 13 | 22 | 13 | 23 | Y | K | 1 | 5 | 5 | Q | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 6 | 1 | N | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | Y | Y | 0 | J | 0 | 0 | 1 | -1 | 21 | 4 | 2 | C | 2 | 0 | 2 | 0 | 0 | 2 | 1 | 10 | B | N | N | N | G | N | 0 | N | N | N | -1 | 23 | -1 | 25 | -1 | 10 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 16 | N | TX | 2014 | 7 | 3 | False | False | False | False | False | False | 0.0004 | 1.2665 | 1 | 23.0 | 0.8928 | 6.000000 | 8.0 | 6.0 | 8.0 | 5.000000 | 7.0 | 5.0 | 8.0 | 10.0 | 19.0 | 19.0 | 24.0 | 23.0 | 25.0 | 11.0 | 60277.000961 | 1.000000e+00 | 7.0 | 9.0 | 25.0 | 25.0 | 4.0 | 24.0 | 6.0 | 8.0 | 3.0 | 3.0 | 3.0 | 6.000000 | 8.0 | 8.0 | 12.0 | 14.0 | 20.0 | 15.0 | 19.0 | 4.0 | 7.0 | 6.0 | 3.0 | 10.0 | 11.0 | 14.0 | 15.0 | 3.0 | 10.0 | 23.0 | 23.0 | 6.0 | 16.0 | 6.0 | 14.0 | 3.0 | 10.0 | 23.0 | 22.0 | 5.0 | 16.0 | 24.0 | 23.0 | 7.0 | 15.0 | 2.000000 | 9.0 | 5.0 | 6.0 | 9.0 | 7.000000 | 18.0 | 5.0 | 10.0 | 21.0 | 23.0 | 19.0 | 21.0 | 10.0 | 13.0 | 24.0 | 25.0 | 6.0 | 10.0 | 3.0 | 2.0 | 3.0 | 2.0 | 2.000000 | 2.0 | 11.0 | 18.0 | 6.0 | 7.0 | 6.000000 | 4.0 | 4.0 | 3.0 | 3.0 | 2.0 | 6.0 | 13.0 | 2.0 | 1.0 | 2.000000 | 1.0 | 16.0 | 23.0 | 19.0 | 18.0 | 6.0 | 5.0 | 15.0 | 22.0 | 9.0 | 6.0 | 1.0 | 1.0 | 20.0 | 22.0 | 6.000000 | 5.0 | 9.0 | 6.0 | 21.0 | 21.0 | 19.0 | 19.0 | 10.0 | 9.0 | 11.0 | 10.0 | 24.0 | 25.0 | 5.0 | 3.0 | 6.0 | 12.0 | 20.0 | 17.0 | 20.0 | 18.0 | 22.0 | 7.0 | 7.0 | 9.0 | 4.0 | 29.000000 | 17.0 | 198.000000 | 1.405555e+09 | False | . 8 B | 935 | N | 13 | 22 | 1 | 6 | T | J | 0 | 4 | 3 | P | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 6 | 1 | N | 1 | 2 | 0 | 3 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | O | 1 | 0 | 1 | -1 | 21 | 2 | 2 | A | 4 | 1 | 3 | 0 | 0 | 2 | 1 | 10 | B | N | N | Y | G | Y | 1 | N | N | N | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 16 | N | CA | 2014 | 1 | 1 | False | False | False | False | False | False | 0.0007 | 1.02 | 1 | 25.0 | 0.9403 | 7.000000 | 11.0 | 7.0 | 11.0 | 6.000000 | 9.0 | 6.0 | 10.0 | 4.0 | 7.0 | 5.0 | 16.0 | 6.0 | 18.0 | 20.0 | 16438.000261 | -3.226606e-08 | 16.0 | 20.0 | 4.0 | 3.0 | 1.0 | 24.0 | 16.0 | 21.0 | 20.0 | 9.0 | 18.0 | 7.000000 | 11.0 | 7.0 | 10.0 | 13.0 | 19.0 | 10.0 | 12.0 | 12.0 | 22.0 | 12.0 | 11.0 | 4.0 | 3.0 | 24.0 | 25.0 | 2.0 | 3.0 | 4.0 | 5.0 | 2.0 | 5.0 | 2.0 | 2.0 | 2.0 | 6.0 | 3.0 | 6.0 | 2.0 | 6.0 | 5.0 | 5.0 | 2.0 | 5.0 | 15.000000 | 23.0 | 24.0 | 2.0 | 2.0 | 2.000000 | 4.0 | 7.0 | 2.0 | 13.0 | 8.0 | 14.0 | 11.0 | 11.0 | 16.0 | 14.0 | 9.0 | 11.0 | 19.0 | 10.0 | 15.0 | 10.0 | 18.0 | 20.999999 | 24.0 | 8.0 | 13.0 | 12.0 | 19.0 | 14.000000 | 18.0 | 15.0 | 23.0 | 17.0 | 23.0 | 5.0 | 12.0 | 12.0 | 18.0 | 4.000000 | 6.0 | 3.0 | 7.0 | 2.0 | 3.0 | 6.0 | 6.0 | 8.0 | 8.0 | 14.0 | 16.0 | 2.0 | 4.0 | 10.0 | 10.0 | 21.000000 | 23.0 | 13.0 | 10.0 | 16.0 | 11.0 | 16.0 | 14.0 | 9.0 | 7.0 | 9.0 | 7.0 | 14.0 | 9.0 | 17.0 | 18.0 | 13.0 | 21.0 | 21.0 | 13.0 | 12.0 | 20.0 | 24.0 | 1.0 | 1.0 | 6.0 | 6.0 | 2.999999 | 14.0 | 14.000005 | 1.389658e+09 | False | . 9 E | 1,480 | N | 13 | 22 | 13 | 23 | T | F | 1 | 5 | 5 | P | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | ZG | ZF | ZN | ZQ | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | R | 1 | 0 | 1 | -1 | 23 | 3 | 2 | C | 4 | 0 | 2 | 1 | 0 | 2 | 1 | 14 | A | N | K | N | E | N | 0 | N | N | N | -1 | 13 | -1 | 25 | -1 | 18 | -1 | -1 | 25 | 25 | -1 | -1 | -1 | -1 | -1 | 18 | N | IL | 2013 | 8 | 1 | False | False | False | False | False | False | 0.0006 | 1.3045 | 2 | 14.0 | 0.9487 | 8.000000 | 14.0 | 2.0 | 2.0 | 2.000000 | 2.0 | 8.0 | 13.0 | 4.0 | 6.0 | 6.0 | 17.0 | 6.0 | 18.0 | 4.0 | 49660.999846 | 1.000000e+00 | 7.0 | 9.0 | 5.0 | 6.0 | 4.0 | 11.0 | 6.0 | 8.0 | 3.0 | 3.0 | 5.0 | 8.000000 | 14.0 | 7.0 | 9.0 | 4.0 | 4.0 | 8.0 | 7.0 | 5.0 | 10.0 | 14.0 | 13.0 | 10.0 | 11.0 | 21.0 | 23.0 | 9.0 | 17.0 | 11.0 | 17.0 | 9.0 | 17.0 | 14.0 | 18.0 | 9.0 | 17.0 | 13.0 | 17.0 | 9.0 | 17.0 | 9.0 | 12.0 | 7.0 | 16.0 | 2.000000 | 9.0 | 16.0 | 20.0 | 19.0 | 4.000000 | 9.0 | 24.0 | 17.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 2.0 | 1.0 | 6.0 | 8.0 | 4.0 | 3.0 | 9.0 | 15.0 | 9.000000 | 13.0 | 9.0 | 16.0 | 11.0 | 17.0 | 15.000000 | 20.0 | 9.0 | 16.0 | 11.0 | 17.0 | 8.0 | 18.0 | 9.0 | 13.0 | 4.000000 | 7.0 | 12.0 | 22.0 | 18.0 | 16.0 | 19.0 | 23.0 | 14.0 | 22.0 | 22.0 | 23.0 | 8.0 | 8.0 | 16.0 | 18.0 | 6.000000 | 8.0 | 17.0 | 16.0 | 17.0 | 13.0 | 12.0 | 8.0 | 20.0 | 22.0 | 19.0 | 21.0 | 17.0 | 13.0 | 10.0 | 7.0 | 6.0 | 13.0 | 6.0 | 17.0 | 19.0 | 15.0 | 19.0 | 9.0 | 9.0 | 14.0 | 23.0 | 33.000000 | 13.0 | 225.000001 | 1.376352e+09 | False | . len(dls.train)*512, len(dls.valid)*128 . (97280, 65280) . roc_auc_binary = RocAucBinary() learn = tabular_learner(dls, metrics=roc_auc_binary) . type(roc_auc_binary) . fastai.metrics.AccumMetric . learn.lr_find() . SuggestedLRs(valley=tensor(0.0010)) . Reference to why we use fit_one_cycle . Note I ran fit_one_cycle with a value of 10 when prepping this notebook for publishing, but the test results came out suspiciously high on 1 outputs, given that the test submission I ran before was heavily weighted with 0 outputs and got a 0.83 score when I ran with 5 but I didn&#39;t set the random seed value then. What happened I think was changing the splitter, I got a different tensor output and I was looking at the alternate value column instead column with the prediction value. . learn.fit_one_cycle(epochs, lr, wd=wd) . epoch train_loss valid_loss roc_auc_score time . 0 | 0.248215 | 0.201386 | 0.949011 | 00:13 | . 1 | 0.196663 | 0.190094 | 0.955587 | 00:13 | . 2 | 0.185024 | 0.185835 | 0.957160 | 00:13 | . 3 | 0.181798 | 0.182034 | 0.958874 | 00:13 | . 4 | 0.175761 | 0.179600 | 0.960017 | 00:13 | . 5 | 0.168771 | 0.177265 | 0.961372 | 00:13 | . 6 | 0.158945 | 0.179190 | 0.960952 | 00:13 | . Referenced another Kaggle notebook for this, we don&#39;t need it but it&#39;s good to see what fastai metrics is actually packaging up for you . preds, targs = learn.get_preds() . preds[0:1][0][0], preds[0:1][0][1] . (tensor(1.0000), tensor(1.0704e-05)) . Here was my mistake, I was looking at the wrong classifier value . len(preds) . 65189 . (preds[:][:][:,1] &gt;= 0.5).sum(), (preds[:][:][:,1] &lt; 0.5).sum() . (tensor(9910), tensor(55279)) . from sklearn.metrics import roc_auc_score valid_score = roc_auc_score(to_np(targs), to_np(preds[:][:][:,1])) valid_score . 0.9609515205141398 . Doing inferences based on this blog post from Walk With Fastai initially, but then experimenting to get this . dl_test = dls.test_dl(df_test) . preds, _ = learn.get_preds(dl=dl_test) . (preds[:][:][:,1] &gt;= 0.5).sum(), (preds[:][:][:,1] &lt; 0.5).sum() . (tensor(26627), tensor(147209)) . Submission To Kaggle . path.ls() . (#18) [Path(&#39;homesite-quote-conversion.zip&#39;),Path(&#39;models&#39;),Path(&#39;sample_submission.csv.zip&#39;),Path(&#39;test.csv.zip&#39;),Path(&#39;train.csv.zip&#39;),Path(&#39;train.csv&#39;),Path(&#39;test.csv&#39;),Path(&#39;sample_submission.csv&#39;),Path(&#39;submission.csv&#39;),Path(&#39;submission2.csv&#39;)...] . file_extract(path/&quot;sample_submission.csv.zip&quot;) . path.ls() . (#18) [Path(&#39;homesite-quote-conversion.zip&#39;),Path(&#39;models&#39;),Path(&#39;sample_submission.csv.zip&#39;),Path(&#39;test.csv.zip&#39;),Path(&#39;train.csv.zip&#39;),Path(&#39;train.csv&#39;),Path(&#39;test.csv&#39;),Path(&#39;sample_submission.csv&#39;),Path(&#39;submission.csv&#39;),Path(&#39;submission2.csv&#39;)...] . df_submission = pd.read_csv(path/&quot;sample_submission.csv&quot;) #I could add `low_memory=false` but it makes things slower df_submission.head() . QuoteNumber QuoteConversion_Flag . 0 3 | 0 | . 1 5 | 0 | . 2 7 | 0 | . 3 9 | 0 | . 4 10 | 0 | . df_submission.tail() . QuoteNumber QuoteConversion_Flag . 173831 434570 | 0 | . 173832 434573 | 0 | . 173833 434574 | 0 | . 173834 434575 | 0 | . 173835 434589 | 0 | . len(df_test.index), len(preds[:][:][:,1]) . (173836, 173836) . type(preds) . torch.Tensor . preds.dtype . torch.float32 . preds[0,1] . tensor(0.0010) . preds[:1][:1] . tensor([[0.9990, 0.0010]]) . We want the 2nd value, this is what gives us our confidence value if it is going to be a sale . preds[:1][:1][:,1] . tensor([0.0010]) . preds_for_submission = preds[:][:][:,1].tolist() preds_for_submission[0:3] . [0.0010175276547670364, 0.03169810026884079, 0.025726569816470146] . fpfs = [float(pfs) for pfs in preds_for_submission] fpfs[0:2] . [0.0010175276547670364, 0.03169810026884079] . integers = [[1], [2], [3]] strings = [int(integer[0]) for integer in integers] strings . [1, 2, 3] . submission = pd.DataFrame({&#39;QuoteNumber&#39;: df_test.index, &#39;QuoteConversion_Flag&#39;: preds[:][:][:,1].tolist()}, columns=[&#39;QuoteNumber&#39;, &#39;QuoteConversion_Flag&#39;]) . Needed to figure out how to extract the floating point value alone from the list to properly compose the csv output dataframe . type(submission.QuoteConversion_Flag) . pandas.core.series.Series . type(submission.QuoteConversion_Flag[0]) . numpy.float64 . submission.QuoteConversion_Flag[0] . 0.0010175276547670364 . Played around with the example on list comprehension here to get it to work with what I had to work with . submission.QuoteConversion_Flag = [float(qcf) for qcf in submission.QuoteConversion_Flag] . submission.head() . QuoteNumber QuoteConversion_Flag . 0 3 | 0.001018 | . 1 5 | 0.031698 | . 2 7 | 0.025727 | . 3 9 | 0.000540 | . 4 10 | 0.429832 | . submission.QuoteConversion_Flag = round(submission.QuoteConversion_Flag).astype(&#39;Int64&#39;) . submission.head() . QuoteNumber QuoteConversion_Flag . 0 3 | 0 | . 1 5 | 0 | . 2 7 | 0 | . 3 9 | 0 | . 4 10 | 0 | . len(submission[submission.QuoteConversion_Flag==1]) . 26627 . len(submission[submission.QuoteConversion_Flag==0]) . 147209 . submission.to_csv(path/&#39;submission11.csv&#39;, index=False) . api.competition_submit(path/&#39;submission11.csv&#39;,message=&quot;Tenth pass&quot;, competition=&#39;homesite-quote-conversion&#39;) . 100%|██████████| 1.45M/1.45M [00:01&lt;00:00, 765kB/s] . Successfully submitted to Homesite Quote Conversion . learn.save(&#39;homesite_fastai_nn11&#39;) . Path(&#39;models/homesite_fastai_nn11.pth&#39;) .",
            "url": "https://redditech.github.io/team-fast-tabulous/kaggle/fastai/2021/07/23/_Another_Pass_Using_Fastai_For_Homesite_Competition.html",
            "relUrl": "/kaggle/fastai/2021/07/23/_Another_Pass_Using_Fastai_For_Homesite_Competition.html",
            "date": " • Jul 23, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "FastShap Analysis for Tabular For Homesite Competition",
            "content": "This notebook compares 2 basic Random Forest models, each with 2 different criterion to split decision trees. Distributions of predicitons are plotted at the end. Rather than provide any concrete conclusions, I leave this here as food for thought. . Setup Working Environment . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . Path.cwd() . Path(&#39;/content&#39;) . os.chdir(&#39;/content/gdrive/MyDrive/Colab Notebooks/GroupProject&#39;) path = Path.cwd() path . Path(&#39;/content/gdrive/MyDrive/Colab Notebooks/GroupProject&#39;) . We should now have all the functions we require to run the notebook. The next thing we need is our dataset... . Kaggle Dataset . I have previosuly downloadeded the dataset from kaggle, and extracted the files. My teammate Nissan has already described how to download Kaggle data. So lets check the files listed in the directory: . path.ls() . (#1) [Path(&#39;/content/gdrive/MyDrive/Colab Notebooks/GroupProject/_data&#39;)] . train = pd.read_csv(path/&#39;_data/train.csv&#39;,low_memory=False) test = pd.read_csv(path/&#39;_data/test.csv&#39;,low_memory=False) . Minimal Data Exploration . Only presenting basic steps here. My teammates have provided other notebooks on more detailed EDA. . train.shape,train.columns . ((260753, 299), Index([&#39;QuoteNumber&#39;, &#39;Original_Quote_Date&#39;, &#39;QuoteConversion_Flag&#39;, &#39;Field6&#39;, &#39;Field7&#39;, &#39;Field8&#39;, &#39;Field9&#39;, &#39;Field10&#39;, &#39;Field11&#39;, &#39;Field12&#39;, ... &#39;GeographicField59A&#39;, &#39;GeographicField59B&#39;, &#39;GeographicField60A&#39;, &#39;GeographicField60B&#39;, &#39;GeographicField61A&#39;, &#39;GeographicField61B&#39;, &#39;GeographicField62A&#39;, &#39;GeographicField62B&#39;, &#39;GeographicField63&#39;, &#39;GeographicField64&#39;], dtype=&#39;object&#39;, length=299)) . train.shape tells us that there are 260,753 rows, and 299 columns of data in the training set. To look at the column names specifically we can use train.columns, which also confirms there are 299 columns. . test.shape,test.columns . ((173836, 298), Index([&#39;QuoteNumber&#39;, &#39;Original_Quote_Date&#39;, &#39;Field6&#39;, &#39;Field7&#39;, &#39;Field8&#39;, &#39;Field9&#39;, &#39;Field10&#39;, &#39;Field11&#39;, &#39;Field12&#39;, &#39;CoverageField1A&#39;, ... &#39;GeographicField59A&#39;, &#39;GeographicField59B&#39;, &#39;GeographicField60A&#39;, &#39;GeographicField60B&#39;, &#39;GeographicField61A&#39;, &#39;GeographicField61B&#39;, &#39;GeographicField62A&#39;, &#39;GeographicField62B&#39;, &#39;GeographicField63&#39;, &#39;GeographicField64&#39;], dtype=&#39;object&#39;, length=298)) . test.shape tells us that there are fewer records in the test set compared to the training set (260,753 vs. 173,753), and one less column (299 vs 298). The missing column is our dependent variable, &quot;QuoteConversion_Flag&quot;. . The output from .columns also shows that we have a variable containing dates in the 2nd column - &quot;Original_Quote_Date&quot;. Numerical coding of the date, although easy to read, hides potentially valuable information about dates. Such as: which day of the week? Was it a holiday? Is this date closer to the start or end of the calendar/financial year? This information could have an influence on what we are trying to predict. Thankfully, fast.ai has provided a useful function that takes the numerical date format and generates extra columns that hold this sort of information. Lets give it a go and have a look at the output. . train = add_datepart(train, &#39;Original_Quote_Date&#39;) train.columns . Index([&#39;QuoteNumber&#39;, &#39;QuoteConversion_Flag&#39;, &#39;Field6&#39;, &#39;Field7&#39;, &#39;Field8&#39;, &#39;Field9&#39;, &#39;Field10&#39;, &#39;Field11&#39;, &#39;Field12&#39;, &#39;CoverageField1A&#39;, ... &#39;Original_Quote_Day&#39;, &#39;Original_Quote_Dayofweek&#39;, &#39;Original_Quote_Dayofyear&#39;, &#39;Original_Quote_Is_month_end&#39;, &#39;Original_Quote_Is_month_start&#39;, &#39;Original_Quote_Is_quarter_end&#39;, &#39;Original_Quote_Is_quarter_start&#39;, &#39;Original_Quote_Is_year_end&#39;, &#39;Original_Quote_Is_year_start&#39;, &#39;Original_Quote_Elapsed&#39;], dtype=&#39;object&#39;, length=311) . test = add_datepart(test, &#39;Original_Quote_Date&#39;) #Run once test.columns . Index([&#39;QuoteNumber&#39;, &#39;Field6&#39;, &#39;Field7&#39;, &#39;Field8&#39;, &#39;Field9&#39;, &#39;Field10&#39;, &#39;Field11&#39;, &#39;Field12&#39;, &#39;CoverageField1A&#39;, &#39;CoverageField1B&#39;, ... &#39;Original_Quote_Day&#39;, &#39;Original_Quote_Dayofweek&#39;, &#39;Original_Quote_Dayofyear&#39;, &#39;Original_Quote_Is_month_end&#39;, &#39;Original_Quote_Is_month_start&#39;, &#39;Original_Quote_Is_quarter_end&#39;, &#39;Original_Quote_Is_quarter_start&#39;, &#39;Original_Quote_Is_year_end&#39;, &#39;Original_Quote_Is_year_start&#39;, &#39;Original_Quote_Elapsed&#39;], dtype=&#39;object&#39;, length=310) . By calling .columns once again, we can see we now have 311 (training) and 310 (test) columns with info about the day of week, day of year, holiday, etc. . train[&#39;QuoteConversion_Flag&#39;].unique() train[&#39;QuoteConversion_Flag&#39;].describe() dep_var = &#39;QuoteConversion_Flag&#39; . Sample Training and Validation Sets from Train DF . Before we run our models, we need to split our trainging dataframe into a training set and a validation set. The validation set will not be passed to the model for training, and will provide us metrics for how well our model generalises to &#39;unseen&#39; data. For a first attempt, lets randomly assign 80% of our records to the training set and 20% of our records to the validation set. . dep_var = &#39;QuoteConversion_Flag&#39; cont,cat = cont_cat_split(train, 1, dep_var=dep_var) #Specify Continuous &amp; Categorical Columns in the DataSe #cont,cat,dep_var procs = [Categorify,FillMissing] . random.seed(42) splits = RandomSplitter(valid_pct=0.2)(range_of(train)) . to = TabularPandas(train,procs,cat,cont,y_names=dep_var,splits=splits) . len(to.train),len(to.valid) . (208603, 52150) . xs,y = to.train.xs,to.train.y valid_xs,valid_y = to.valid.xs,to.valid.y . Random Forest Classifier . Looking at the dependent variable - &quot;QuoteConversion_Flag&quot;, we can seen that this is a binary outcome, i.e 0 or 1. The obvious choice when using Decision Trees with binary data is a Random Forest Classifier. In the classifier, each tree votes and the most popular class is chosen as the final result. . Split Trees by Gini Impurity . rfclass_gini = RandomForestClassifier(n_jobs=-1, random_state=42, criterion = &quot;gini&quot;,oob_score=False).fit(xs, y) . train_classifier_gini_predictions = rfclass_gini.predict(xs) valid_classifier_gini_predictions = rfclass_gini.predict(valid_xs) . The output from the Classifier are binary predictions. See below: . plt.hist(valid_classifier_gini_predictions, bins=10) . (array([44760., 0., 0., 0., 0., 0., 0., 0., 0., 7390.]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]), &lt;a list of 10 Patch objects&gt;) . To get the probabilities of each outcome; whether a customer purchases insurance (1) or not (0), we need to call a different function . rfclass_gini.predict_proba(valid_xs) plt.hist(rfclass_gini.predict_proba(valid_xs), bins=10) . (array([[ 974., 1627., 1470., 1362., 1957., 2984., 3141., 3236., 5677., 29722.], [28868., 6131., 3959., 2818., 2766., 2175., 1489., 1214., 1596., 1134.]]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]), &lt;a list of 2 Lists of Patches objects&gt;) . We can see the output is a probability for each class. So in this case we have 2 probabilty estimates for each record. However, the Kaggle competition compares entries on the area under the curve (AUC). Therefore, lets see how the classifier performs for this metric. . train_classifier_auc = roc_auc_score(y,train_classifier_gini_predictions) valid_classifier_auc = roc_auc_score(valid_y,valid_classifier_gini_predictions) train_classifier_auc,valid_classifier_auc . (1.0, 0.8121955713748932) . For a first model, without any feature selection and data cleaning an ROC AUC of 0.81 in the validation isnt bad. A model predicting outcomes randomly would produce an ROC AUC of 0.5. The code below shoes this: . random_probs = [0 for _ in range(len(valid_y))] random_lr_auc = roc_auc_score(valid_y, random_probs) random_lr_auc . 0.5 . We can see that the random forest classfier is doing a lot better than a model making random predcitions, so that is encouraging. . Another way of visualising the results from classification tasks is a confusion matrix. This plots the counts of the predicted vs true classes. This Classifier seems to do a relatively good job at predicting records that relate to customers who did not purchase home insurance (True = 0), but struggled a lot more at predicting records that relate to customers that did go onto purchase home insurance (True = 1). Another observation is that is a skew in the number of records towaards those of customers that did NOT go onto purchase home. . plot_confusion_matrix(rfclass_gini,valid_xs, valid_y,values_format=&#39;d&#39;) plt.show() . Split Trees by Entropy . The Random Forest Classifier has a second option for the criteria used to split the decision trees. Lets try the same process with the &#39;entropy&#39; criterion and compare the results. . rfclass_entropy = RandomForestClassifier(n_jobs=-1, random_state=42, criterion = &quot;entropy&quot;,oob_score=False).fit(xs, y) train_classifier_entropy_predictions = rfclass_entropy.predict(xs) valid_classifier_entropy_predictions = rfclass_entropy.predict(valid_xs) . roc_auc_score(y,train_classifier_entropy_predictions),roc_auc_score(valid_y,valid_classifier_entropy_predictions) . (0.9999743412105817, 0.8091649897936626) . plot_confusion_matrix(rfclass_entropy,valid_xs, valid_y,values_format=&#39;d&#39;) plt.show() . Comparing the confusion matrices gives us an indication of how the different criterion influenced the predictions from the classifiers. . The &#39;entropy&#39; criterion looks as though it did very slightly better at classfying records in which customers did not purchase home insurance (True Label = 0_, and slightly worse than the &#39;gini impurity&#39; for records in which customers did go on to purchase home insurance (True Label = 1). . Given the task is to predict the probabilty that a customer went onto purchase home insurance, using the &#39;gini impurity&#39; criterion seems more appropriate. Although given such as small difference between criterion, the decision between these two criterion is lilkey to have very little influence on the results of the model overall. Therefore, time investigating the effect of removing redundant or highly correlated variablesmay be more fruitful. . Create Random Forest Regressor . Although I said a Random Forest Classifier is the most obvious choice for use on binary data, we could try something else. The Kaggle competition task is to predict the probability that a customer purchased home insurance. Therefore, why not use a method that directly outputs probabilities. . In short: . A classifier will take the most common prediction from all of the decision trees. So if we run 100 trees, and 51 predict the customer purchased home insurance (1), and 49 predicted that the customer did NOT purchase home insurance (0), the overall prediction would be that the customer purchased home insurance (1). Since, we just want a probability, not a classification for the task, why force the model to make a classification. This could hae a particularly big imapct in the intermediate cases, as above. . Why not try a regressor? a Random Forest Regressor will take the average of the predictions from all of the decision trees. Therefore, in the example above, the overall prediction would be 0.51. This prediciton could be passed directly to the Kaggle to compute the ROC AUC score. Using this more conservative approach could provide improvements for these intermediate cases. So lets try it! . rfregress_mse = RandomForestRegressor(n_jobs=-1,random_state=42,criterion=&quot;mse&quot;,oob_score=False).fit(xs, y) train_rfregress_mse_predictions = rfregress_mse.predict(xs) valid_rfregress_mse_predictions = rfregress_mse.predict(valid_xs) . train_lr_auc = roc_auc_score(y, train_rfregress_mse_predictions) valid_lr_auc = roc_auc_score(valid_y, valid_rfregress_mse_predictions) train_lr_auc,valid_lr_auc . (1.0, 0.9587383322397882) . With the Regressor, using defult parameters and splitting trees based on the Mean Squared Error (mse), we get over a 10% increase in the ROC AUC for the validation set! (Caveat, This difference may be less after more detailed EDA and QC) . Lets re-check the Classifier ROC AUC to be sure: . train_classifier_auc,valid_classifier_auc . (1.0, 0.8121955713748932) . I will leave investigating the reasons for the differences for another notebook, this could involve writing our own classifiers and regressors so me can compare with the same or different loss functions. But to show the differences in the outputs of the two models, histograms of the predictions from the Random Forest Classifier &amp; and the Random Forest Regressor are shown below: . Classifier . plt.hist(rfclass_gini.predict_proba(valid_xs)[:,1], bins=100,range=[0, 1]) . (array([12171., 4645., 2879., 2036., 1653., 1305., 1154., 1094., 1000., 931., 854., 756., 733., 667., 597., 587., 532., 452., 496., 457., 400., 399., 360., 398., 371., 352., 323., 347., 340., 346., 323., 326., 319., 317., 612., 0., 302., 311., 317., 314., 634., 0., 282., 271., 263., 276., 532., 0., 257., 251., 218., 227., 229., 212., 203., 181., 396., 0., 187., 175., 147., 184., 159., 147., 159., 167., 129., 131., 286., 127., 0., 136., 140., 125., 115., 128., 128., 130., 154., 158., 129., 325., 157., 0., 147., 175., 168., 158., 156., 181., 160., 178., 149., 302., 106., 0., 93., 73., 35., 38.]), array([0. , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1. ]), &lt;a list of 100 Patch objects&gt;) . Regressor . plt.hist(valid_rfregress_mse_predictions, bins=100,range=[0, 1]) . (array([21304., 2296., 1756., 1427., 1255., 1147., 984., 918., 748., 744., 646., 594., 565., 473., 456., 434., 441., 379., 363., 349., 366., 324., 312., 329., 283., 327., 286., 274., 240., 237., 242., 210., 276., 239., 519., 0., 228., 223., 228., 179., 387., 0., 210., 214., 191., 167., 314., 0., 159., 186., 139., 148., 131., 127., 122., 136., 195., 0., 103., 94., 93., 75., 76., 73., 64., 59., 61., 57., 87., 47., 0., 45., 44., 32., 33., 32., 31., 35., 32., 36., 33., 45., 26., 0., 22., 25., 26., 34., 31., 28., 39., 44., 42., 117., 83., 0., 91., 147., 232., 4449.]), array([0. , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1. ]), &lt;a list of 100 Patch objects&gt;) . Test Set Predictions . Data Prep . cont,cat = cont_cat_split(test, max_card=1000) #Specify Continuous &amp; Categorical Columns in the DataSe procs = [Categorify,FillMissing] . to.test = TabularPandas(test,procs,cat,cont,splits=None) to.test.show() . Field6 Field7 Field10 Field12 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField3 SalesField4 SalesField5 SalesField6 SalesField7 SalesField9 SalesField10 SalesField11 SalesField12 SalesField13 SalesField14 SalesField15 PersonalField1 PersonalField2 PersonalField4A PersonalField4B PersonalField5 PersonalField6 PersonalField7 PersonalField8 PersonalField9 PersonalField10A PersonalField10B PersonalField11 PersonalField12 PersonalField13 PersonalField14 PersonalField15 PersonalField16 PersonalField17 PersonalField18 PersonalField19 PersonalField22 PersonalField23 PersonalField24 PersonalField25 PersonalField26 PersonalField27 PersonalField28 PersonalField29 PersonalField30 PersonalField31 PersonalField32 PersonalField33 PersonalField34 PersonalField35 PersonalField36 PersonalField37 PersonalField38 PersonalField39 PersonalField40 PersonalField41 PersonalField42 PersonalField43 PersonalField44 PersonalField45 PersonalField46 PersonalField47 PersonalField48 PersonalField49 PersonalField50 PersonalField51 PersonalField52 PersonalField53 PersonalField54 PersonalField55 PersonalField56 PersonalField57 PersonalField58 PersonalField59 PersonalField60 PersonalField61 PersonalField62 PersonalField63 PersonalField64 PersonalField65 PersonalField66 PersonalField67 PersonalField68 PersonalField69 PersonalField70 PersonalField71 PersonalField72 PersonalField73 PersonalField74 PersonalField75 PersonalField76 PersonalField77 PersonalField78 PersonalField79 PersonalField80 PersonalField81 PersonalField82 PersonalField83 PropertyField1A PropertyField1B PropertyField2A PropertyField2B PropertyField3 PropertyField4 PropertyField5 PropertyField6 PropertyField7 PropertyField8 PropertyField9 PropertyField10 PropertyField11A PropertyField11B PropertyField12 PropertyField13 PropertyField14 PropertyField15 PropertyField16A PropertyField16B PropertyField17 PropertyField18 PropertyField19 PropertyField20 PropertyField21A PropertyField21B PropertyField22 PropertyField23 PropertyField24A PropertyField24B PropertyField26A PropertyField26B PropertyField27 PropertyField28 PropertyField30 PropertyField31 PropertyField32 PropertyField33 PropertyField34 PropertyField35 PropertyField36 PropertyField37 PropertyField38 PropertyField39A PropertyField39B GeographicField1A GeographicField1B GeographicField2A GeographicField2B GeographicField3A GeographicField3B GeographicField4A GeographicField4B GeographicField5A GeographicField5B GeographicField6A GeographicField6B GeographicField7A GeographicField7B GeographicField8A GeographicField8B GeographicField9A GeographicField9B GeographicField10A GeographicField10B GeographicField11A GeographicField11B GeographicField12A GeographicField12B GeographicField13A GeographicField13B GeographicField14A GeographicField14B GeographicField15A GeographicField15B GeographicField16A GeographicField16B GeographicField17A GeographicField17B GeographicField18A GeographicField18B GeographicField19A GeographicField19B GeographicField20A GeographicField20B GeographicField21A GeographicField21B GeographicField22A GeographicField22B GeographicField23A GeographicField23B GeographicField24A GeographicField24B GeographicField25A GeographicField25B GeographicField26A GeographicField26B GeographicField27A GeographicField27B GeographicField28A GeographicField28B GeographicField29A GeographicField29B GeographicField30A GeographicField30B GeographicField31A GeographicField31B GeographicField32A GeographicField32B GeographicField33A GeographicField33B GeographicField34A GeographicField34B GeographicField35A GeographicField35B GeographicField36A GeographicField36B GeographicField37A GeographicField37B GeographicField38A GeographicField38B GeographicField39A GeographicField39B GeographicField40A GeographicField40B GeographicField41A GeographicField41B GeographicField42A GeographicField42B GeographicField43A GeographicField43B GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56A GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60A GeographicField60B GeographicField61A GeographicField61B GeographicField62A GeographicField62B GeographicField63 GeographicField64 Original_Quote_Year Original_Quote_Month Original_Quote_Week Original_Quote_Day Original_Quote_Dayofweek Original_Quote_Dayofyear Original_Quote_Is_month_end Original_Quote_Is_month_start Original_Quote_Is_quarter_end Original_Quote_Is_quarter_start Original_Quote_Is_year_end Original_Quote_Is_year_start PersonalField84_na PropertyField29_na QuoteNumber Field8 Field9 Field11 SalesField8 PersonalField84 PropertyField25 PropertyField29 Original_Quote_Elapsed . 0 E | 16 | 1,487 | N | 4 | 4 | 4 | 4 | 3 | 3 | 3 | 4 | 13 | 22 | 13 | 23 | Y | K | 13 | 22 | 6 | 16 | 9 | 21 | 0 | 5 | 5 | 11 | P | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 11 | 14 | 6 | 1 | N | 1 | 3 | -1 | -1 | 0 | 5 | 2 | 5 | 14 | YH | XR | XQ | ZQ | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 18 | 23 | -1 | 4 | N | N | Y | 0 | D | 1 | 0 | 1 | -1 | 24 | 1 | 2 | A | 1 | 3 | 5 | 0 | 2 | 0 | 0 | 4 | 4 | 2 | 1 | 6 | 8 | 7 | 8 | 19 | B | N | N | Y | G | N | 2 | N | Y | N | 18 | 21 | 25 | 25 | 9 | 6 | 1 | 1 | 24 | 25 | -1 | 13 | 9 | 18 | 12 | 18 | 10 | 18 | 16 | 19 | -1 | 25 | 9 | 18 | 13 | 18 | 9 | 18 | -1 | 19 | 10 | 13 | 8 | 17 | 2 | 10 | -1 | 20 | 19 | 18 | 4 | 9 | -1 | 25 | -1 | 24 | -1 | 15 | 16 | 15 | 17 | 17 | 10 | 13 | 21 | 20 | 25 | 25 | 16 | 22 | 25 | 25 | 8 | 11 | 10 | 17 | 11 | 17 | 24 | 25 | 15 | 23 | 25 | 25 | -1 | -1 | 21 | 24 | 18 | 24 | 11 | 18 | 18 | 16 | 20 | 24 | 11 | 17 | 22 | 23 | 9 | 12 | 25 | 25 | 6 | 9 | 4 | 2 | 16 | 12 | 20 | 20 | 2 | 2 | 2 | 1 | 1 | 1 | 10 | 7 | 25 | 25 | -1 | 19 | 19 | 22 | 12 | 15 | 1 | 1 | -1 | 1 | -1 | 20 | -1 | 25 | Y | IL | 2014 | 8 | 33 | 12 | 1 | 224 | False | False | False | False | False | False | True | True | 3 | 0.9364 | 0.0006 | 1.3045 | 67052 | 2.0 | 1.5 | 0.0 | 1.407802e+09 | . 1 F | 11 | 564 | N | 8 | 14 | 8 | 14 | 7 | 12 | 8 | 13 | 13 | 22 | 13 | 23 | T | E | 4 | 5 | 3 | 6 | 3 | 6 | 1 | 5 | 5 | 4 | R | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 15 | 20 | 6 | 1 | N | 1 | 2 | 9 | 18 | 0 | 1 | 2 | 2 | 4 | YF | XS | YP | XC | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 22 | 24 | -1 | 10 | N | N | Y | 0 | J | 1 | 0 | 1 | -1 | 21 | 1 | 1 | A | 4 | 7 | 15 | 1 | 1 | 1 | 0 | 8 | 14 | 2 | 1 | 5 | 6 | 10 | 15 | 10 | B | N | O | Y | H | N | 1 | N | N | N | 5 | 3 | 17 | 24 | 17 | 17 | 15 | 17 | 10 | 9 | -1 | 20 | 4 | 13 | 9 | 13 | 5 | 13 | 5 | 12 | -1 | 25 | 4 | 15 | 8 | 13 | 5 | 14 | -1 | 13 | 9 | 11 | 5 | 12 | 2 | 16 | -1 | 22 | 23 | 24 | 21 | 22 | -1 | 22 | -1 | 22 | -1 | 14 | 10 | 4 | 10 | 5 | 5 | 5 | 13 | 6 | 9 | 14 | 8 | 10 | 12 | 22 | 7 | 10 | 9 | 15 | 11 | 18 | 14 | 18 | 5 | 7 | 19 | 24 | 6 | 14 | 16 | 23 | 6 | 12 | 11 | 19 | 25 | 25 | 15 | 20 | 12 | 20 | 23 | 24 | 12 | 21 | 23 | 25 | 7 | 11 | 16 | 14 | 13 | 6 | 17 | 15 | 7 | 5 | 7 | 5 | 13 | 7 | 14 | 14 | 7 | 14 | -1 | 4 | 1 | 1 | 5 | 3 | 10 | 10 | -1 | 5 | -1 | 5 | -1 | 21 | N | NJ | 2013 | 9 | 36 | 7 | 5 | 250 | False | False | False | False | False | False | True | True | 5 | 0.9919 | 0.0038 | 1.1886 | 27288 | 2.0 | 1.0 | 0.0 | 1.378512e+09 | . 2 F | 15 | 564 | N | 11 | 18 | 11 | 18 | 10 | 16 | 10 | 18 | 13 | 22 | 13 | 23 | T | E | 3 | 3 | 5 | 14 | 3 | 9 | 1 | 5 | 5 | 23 | V | 0 | 1 | 2 | 2 | 0 | 0 | 0 | 1 | 1 | 12 | 15 | 6 | 1 | N | 1 | 2 | 22 | 24 | 0 | 1 | 2 | 4 | 10 | XE | ZH | YK | ZN | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 6 | 9 | -1 | 23 | N | N | Y | 0 | R | 1 | 0 | 1 | -1 | 24 | 1 | 2 | C | 1 | 8 | 17 | 0 | 2 | 1 | 0 | 11 | 18 | 2 | 1 | 12 | 18 | 20 | 24 | 10 | B | N | M | Y | G | Y | 1 | N | N | N | 11 | 12 | 3 | 6 | 16 | 16 | 15 | 18 | 14 | 16 | -1 | 16 | 4 | 10 | 10 | 16 | 5 | 11 | 6 | 13 | -1 | 25 | 4 | 10 | 10 | 16 | 5 | 10 | -1 | 11 | 11 | 18 | 5 | 13 | 2 | 13 | -1 | 15 | 21 | 20 | 19 | 21 | -1 | 16 | -1 | 18 | -1 | 16 | 15 | 11 | 11 | 7 | 6 | 7 | 13 | 7 | 7 | 11 | 15 | 21 | 5 | 5 | 3 | 2 | 14 | 21 | 8 | 12 | 12 | 15 | 14 | 22 | 7 | 7 | 5 | 11 | 9 | 14 | 13 | 22 | 8 | 13 | 22 | 19 | 11 | 14 | 11 | 16 | 16 | 18 | 9 | 10 | 14 | 16 | 6 | 8 | 20 | 19 | 17 | 14 | 16 | 13 | 20 | 22 | 20 | 22 | 20 | 18 | 10 | 7 | 4 | 7 | -1 | 11 | 13 | 12 | 18 | 22 | 10 | 11 | -1 | 20 | -1 | 22 | -1 | 11 | N | NJ | 2013 | 3 | 13 | 29 | 4 | 88 | False | False | False | False | False | False | True | True | 7 | 0.8945 | 0.0038 | 1.0670 | 65264 | 2.0 | 1.0 | 0.0 | 1.364515e+09 | . 3 K | 21 | 1,113 | Y | 14 | 22 | 15 | 22 | 13 | 20 | 22 | 25 | 13 | 22 | 13 | 23 | Y | F | 5 | 9 | 9 | 20 | 5 | 16 | 1 | 5 | 5 | 11 | R | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 5 | 6 | 6 | 1 | N | 1 | 2 | 7 | 15 | 0 | 1 | 2 | 3 | 5 | XR | YY | XT | XC | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 5 | 8 | -1 | 6 | N | N | Y | 0 | Q | 1 | 0 | 1 | -1 | 21 | 4 | 2 | C | 4 | 5 | 10 | 1 | 2 | 1 | 0 | 14 | 22 | 2 | 1 | 13 | 18 | 8 | 11 | 4 | B | N | M | N | G | N | 0 | N | N | N | 7 | 5 | 9 | 18 | 11 | 9 | 13 | 15 | 22 | 24 | -1 | 14 | 24 | 25 | 23 | 23 | 24 | 25 | 25 | 25 | -1 | 25 | 24 | 24 | 23 | 23 | 24 | 24 | 25 | 25 | 23 | 22 | 24 | 25 | 2 | 7 | -1 | 13 | 11 | 14 | 6 | 16 | -1 | 15 | -1 | 15 | -1 | 22 | 11 | 5 | 10 | 4 | 5 | 4 | 13 | 5 | 7 | 10 | 3 | 2 | 5 | 5 | 9 | 14 | 15 | 22 | 7 | 10 | 9 | 8 | 13 | 22 | 6 | 5 | 3 | 7 | 8 | 11 | 11 | 20 | 17 | 23 | 14 | 12 | 15 | 20 | 10 | 14 | 11 | 11 | 9 | 10 | 11 | 13 | 15 | 21 | 14 | 12 | 17 | 13 | 10 | 6 | 20 | 22 | 20 | 22 | 19 | 16 | 12 | 11 | 4 | 6 | -1 | 13 | 10 | 8 | 5 | 3 | 8 | 8 | -1 | 13 | -1 | 8 | -1 | 21 | N | TX | 2015 | 3 | 12 | 21 | 5 | 80 | False | False | False | False | False | False | True | True | 9 | 0.8870 | 0.0004 | 1.2665 | 32725 | 2.0 | 2.0 | 0.0 | 1.426896e+09 | . 4 B | 25 | 935 | N | 4 | 5 | 4 | 5 | 4 | 4 | 4 | 5 | 13 | 22 | 13 | 23 | Y | D | 12 | 21 | 1 | 1 | 3 | 6 | 0 | 5 | 5 | 11 | T | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 7 | 0 | N | 1 | 1 | 4 | 2 | 0 | 4 | 2 | 2 | 24 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | -1 | 8 | N | N | Y | 0 | R | 0 | 0 | 1 | -1 | 21 | 2 | 1 | C | 1 | 5 | 11 | 0 | 2 | 0 | 0 | 4 | 5 | 2 | 1 | 4 | 4 | 9 | 12 | 10 | B | N | O | Y | H | N | 1 | Y | N | N | 10 | 11 | 2 | 4 | 12 | 11 | 1 | 1 | 11 | 9 | -1 | 13 | 2 | 8 | 4 | 6 | 2 | 6 | 2 | 9 | -1 | 25 | 2 | 3 | 3 | 5 | 2 | 5 | -1 | 7 | 6 | 6 | 2 | 6 | 15 | 23 | -1 | 17 | 4 | 8 | 6 | 17 | -1 | 13 | -1 | 15 | -1 | 8 | 17 | 15 | 15 | 13 | 15 | 18 | 14 | 7 | 7 | 10 | 12 | 19 | 8 | 14 | 7 | 9 | 2 | 1 | 15 | 21 | 12 | 15 | 16 | 23 | 9 | 13 | 16 | 24 | 6 | 8 | 4 | 6 | 2 | 5 | 11 | 7 | 8 | 9 | 6 | 5 | 9 | 8 | 25 | 25 | 9 | 3 | 9 | 18 | 7 | 4 | 16 | 12 | 13 | 9 | 8 | 6 | 8 | 6 | 11 | 5 | 19 | 21 | 13 | 21 | -1 | 23 | 11 | 8 | 5 | 3 | 7 | 7 | -1 | 3 | -1 | 22 | -1 | 21 | N | CA | 2014 | 12 | 50 | 10 | 2 | 344 | False | False | False | False | False | False | False | True | 10 | 0.9153 | 0.0007 | 1.0200 | 56025 | 2.0 | 1.0 | 0.0 | 1.418170e+09 | . 5 B | 24 | 965 | N | 11 | 18 | 11 | 18 | 10 | 16 | 10 | 18 | 13 | 22 | 13 | 23 | Y | D | 6 | 12 | 6 | 17 | 4 | 14 | 0 | 5 | 5 | 24 | V | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 15 | 19 | 7 | 0 | N | 1 | 2 | 5 | 7 | 0 | 1 | 2 | 3 | 24 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 15 | 20 | -1 | 20 | N | N | Y | 0 | O | 0 | 0 | 1 | -1 | 21 | 2 | 2 | C | 1 | 10 | 19 | 1 | 4 | 0 | 0 | 11 | 18 | 2 | 1 | 13 | 19 | 21 | 24 | 10 | B | N | O | N | H | Y | 2 | N | N | N | 12 | 14 | 13 | 23 | 14 | 13 | 6 | 6 | 14 | 15 | -1 | 13 | 2 | 3 | 4 | 5 | 2 | 5 | 2 | 2 | -1 | 25 | 2 | 6 | 3 | 6 | 2 | 6 | -1 | 7 | 5 | 5 | 2 | 5 | 15 | 23 | -1 | 24 | 2 | 2 | 2 | 4 | -1 | 7 | -1 | 15 | -1 | 2 | 13 | 8 | 14 | 11 | 11 | 16 | 14 | 9 | 10 | 18 | 9 | 12 | 8 | 14 | 18 | 24 | 12 | 20 | 13 | 20 | 9 | 9 | 11 | 19 | 15 | 23 | 7 | 16 | 12 | 20 | 5 | 8 | 3 | 6 | 2 | 3 | 6 | 5 | 8 | 7 | 14 | 14 | 2 | 4 | 10 | 10 | 21 | 23 | 12 | 10 | 17 | 13 | 15 | 12 | 6 | 4 | 6 | 4 | 17 | 12 | 14 | 14 | 11 | 20 | -1 | 21 | 10 | 6 | 10 | 12 | 9 | 9 | -1 | 2 | -1 | 20 | -1 | 8 | N | CA | 2013 | 7 | 29 | 19 | 4 | 200 | False | False | False | False | False | False | False | True | 11 | 0.9403 | 0.0006 | 1.0200 | 42868 | 2.0 | 1.0 | 0.0 | 1.374192e+09 | . 6 E | 23 | 1,487 | N | 5 | 6 | 5 | 6 | 4 | 5 | 4 | 5 | 13 | 22 | 13 | 23 | Y | K | 11 | 21 | 3 | 8 | 5 | 15 | 1 | 5 | 5 | 11 | K | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 18 | 22 | 7 | 0 | N | 1 | 2 | 15 | 21 | 0 | 1 | 2 | 2 | 24 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 4 | 6 | -1 | 8 | Y | N | Y | 0 | N | 0 | 0 | 1 | -1 | 21 | 1 | 2 | C | 2 | 4 | 7 | 1 | 1 | 0 | 0 | 5 | 6 | 2 | 1 | 4 | 4 | 9 | 12 | 10 | B | N | O | N | E | N | 0 | N | N | N | 10 | 10 | 2 | 3 | 14 | 14 | 7 | 6 | 17 | 19 | -1 | 13 | 13 | 21 | 15 | 19 | 13 | 20 | 23 | 25 | -1 | 25 | 11 | 19 | 16 | 19 | 11 | 19 | -1 | 23 | 14 | 19 | 15 | 22 | 2 | 18 | -1 | 19 | 16 | 16 | 2 | 4 | -1 | 24 | -1 | 18 | -1 | 19 | 2 | 1 | 2 | 1 | 2 | 1 | 2 | 1 | 5 | 7 | 4 | 3 | 6 | 7 | 8 | 12 | 7 | 11 | 6 | 8 | 11 | 12 | 8 | 14 | 6 | 6 | 9 | 20 | 9 | 14 | 2 | 2 | 23 | 25 | 17 | 15 | 20 | 24 | 11 | 17 | 8 | 3 | 6 | 6 | 13 | 15 | 4 | 2 | 21 | 21 | 20 | 20 | 15 | 12 | 23 | 24 | 23 | 24 | 20 | 18 | 16 | 16 | 2 | 3 | -1 | 3 | 25 | 25 | 9 | 9 | 11 | 12 | -1 | 7 | -1 | 4 | -1 | 8 | N | IL | 2014 | 7 | 31 | 28 | 0 | 209 | False | False | False | False | False | False | False | False | 15 | 0.9392 | 0.0006 | 1.3045 | 52431 | 2.0 | 1.0 | 0.0 | 1.406506e+09 | . 7 B | 25 | 935 | N | 6 | 9 | 6 | 9 | 5 | 7 | 5 | 8 | 13 | 22 | 13 | 23 | Y | D | 10 | 19 | 2 | 5 | 3 | 9 | 1 | 5 | 5 | 11 | P | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 10 | 6 | 1 | N | 1 | 2 | 21 | 23 | 0 | 1 | 2 | 3 | 24 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 15 | 19 | -1 | 20 | N | N | Y | 0 | O | 0 | 0 | 1 | -1 | 21 | 4 | 2 | C | 4 | 7 | 15 | 1 | 2 | 0 | 0 | 6 | 9 | 2 | 1 | 5 | 6 | 11 | 15 | 10 | B | N | O | Y | H | Y | 2 | N | N | N | 9 | 10 | 6 | 13 | 17 | 18 | 10 | 11 | 10 | 8 | -1 | 13 | 2 | 7 | 2 | 2 | 2 | 2 | 2 | 4 | -1 | 25 | 2 | 7 | 1 | 1 | 2 | 2 | -1 | 7 | 3 | 3 | 2 | 3 | 5 | 20 | -1 | 18 | 3 | 6 | 4 | 11 | -1 | 11 | -1 | 15 | -1 | 5 | 23 | 24 | 15 | 13 | 14 | 17 | 15 | 9 | 9 | 14 | 3 | 2 | 9 | 16 | 3 | 2 | 15 | 22 | 14 | 21 | 8 | 7 | 9 | 16 | 14 | 22 | 15 | 23 | 15 | 22 | 6 | 12 | 1 | 1 | 6 | 6 | 12 | 15 | 2 | 2 | 14 | 14 | 2 | 2 | 9 | 3 | 1 | 1 | 18 | 18 | 20 | 19 | 14 | 10 | 23 | 24 | 22 | 24 | 15 | 10 | 22 | 24 | 7 | 14 | -1 | 16 | 15 | 16 | 2 | 1 | 15 | 19 | -1 | 17 | -1 | 15 | -1 | 8 | N | CA | 2015 | 1 | 4 | 20 | 1 | 20 | False | False | False | False | False | False | False | True | 16 | 0.9153 | 0.0007 | 1.0200 | 2572 | 2.0 | 1.0 | 0.0 | 1.421712e+09 | . 8 J | 23 | 1,113 | N | 12 | 20 | 12 | 20 | 11 | 18 | 12 | 19 | 13 | 22 | 13 | 23 | T | F | 2 | 3 | 15 | 24 | 10 | 22 | 0 | 5 | 5 | 11 | T | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | -1 | -1 | 7 | 0 | N | 1 | 2 | 24 | 25 | 0 | 1 | 2 | 5 | 24 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 10 | 13 | -1 | 24 | Y | Y | Y | 0 | D | 0 | 0 | 1 | -1 | 21 | 4 | 2 | A | 5 | 4 | 8 | 1 | 3 | 1 | 0 | 12 | 20 | 2 | 1 | 21 | 24 | 8 | 10 | 4 | B | N | K | Y | E | N | 2 | N | N | N | 18 | 22 | 4 | 9 | 11 | 9 | 12 | 13 | 14 | 14 | -1 | 23 | 13 | 21 | 22 | 22 | 14 | 21 | 20 | 23 | -1 | 25 | 14 | 22 | 21 | 22 | 15 | 22 | -1 | 16 | 23 | 23 | 9 | 18 | 2 | 4 | -1 | 13 | 9 | 11 | 1 | 1 | -1 | 8 | -1 | 15 | -1 | 24 | 19 | 20 | 13 | 10 | 7 | 10 | 16 | 14 | 9 | 15 | 9 | 13 | 11 | 20 | 14 | 22 | 15 | 22 | 9 | 14 | 12 | 14 | 11 | 19 | 14 | 21 | 4 | 10 | 9 | 15 | 11 | 21 | 8 | 13 | 13 | 9 | 11 | 13 | 25 | 25 | 8 | 5 | 12 | 21 | 11 | 13 | 8 | 15 | 11 | 8 | 17 | 12 | 19 | 19 | 13 | 12 | 13 | 12 | 17 | 12 | 8 | 5 | 6 | 11 | -1 | 15 | 12 | 10 | 4 | 2 | 7 | 7 | -1 | 12 | -1 | 5 | -1 | 8 | N | TX | 2014 | 5 | 22 | 28 | 2 | 148 | False | False | False | False | False | False | False | False | 17 | 0.8928 | 0.0004 | 1.2665 | 10065 | 2.0 | 3.0 | 0.0 | 1.401235e+09 | . 9 J | 23 | 1,165 | N | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 13 | 22 | 13 | 23 | Y | F | 14 | 23 | 9 | 21 | 15 | 24 | 1 | 3 | 4 | 1 | Q | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 6 | 7 | 6 | 1 | N | 1 | 3 | -1 | -1 | 0 | 5 | 2 | 1 | 20 | XD | XS | YP | XC | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 16 | 20 | -1 | 4 | N | N | Y | 0 | O | 1 | 0 | 1 | -1 | 21 | 4 | 2 | C | 4 | 2 | 3 | 0 | 1 | 0 | 0 | 3 | 3 | 2 | 1 | 5 | 6 | 10 | 14 | 10 | B | N | O | Y | H | N | 2 | N | Y | N | 17 | 21 | 10 | 20 | 10 | 7 | 1 | 1 | 16 | 18 | -1 | 24 | 13 | 22 | 22 | 22 | 14 | 22 | 23 | 24 | -1 | 25 | 14 | 21 | 21 | 22 | 15 | 21 | -1 | 18 | 23 | 22 | 10 | 20 | 2 | 5 | -1 | 14 | 9 | 11 | 3 | 8 | -1 | 7 | -1 | 15 | -1 | 22 | 19 | 20 | 13 | 10 | 7 | 10 | 16 | 14 | 12 | 20 | 8 | 11 | 10 | 19 | 7 | 10 | 4 | 4 | 5 | 5 | 10 | 12 | 4 | 5 | 25 | 25 | 25 | 25 | 9 | 14 | 4 | 5 | 11 | 17 | 13 | 9 | 17 | 22 | 19 | 23 | 10 | 9 | 11 | 18 | 12 | 14 | 8 | 16 | 2 | 1 | 22 | 22 | 23 | 24 | 1 | 1 | 1 | 1 | 14 | 8 | 2 | 1 | 9 | 17 | -1 | 13 | 15 | 17 | 19 | 23 | 7 | 6 | -1 | 1 | -1 | 8 | -1 | 13 | N | TX | 2013 | 7 | 28 | 11 | 3 | 192 | False | False | False | False | False | False | True | False | 21 | 0.9691 | 0.0004 | 1.2665 | 8321 | 2.0 | 1.0 | 0.0 | 1.373501e+09 | . test_xs = to.test.xs . Predictions . test_classifier_gini_predictions = rfclass_gini.predict_proba(test_xs) #Probability of Insurance Policy Purchased - Regressor test_rfregress_mse_predictions = rfregress_mse.predict(test_xs) . classifier_submission = pd.DataFrame(zip(test.QuoteNumber,test_classifier_gini_predictions[:,1]), columns = [&#39;QuoteNumber&#39;,&#39;QuoteConversion_Flag&#39;]) classifier_submission.to_csv(path/&#39;classifier_submission.csv&#39;,index=False) regressor_submission = pd.DataFrame(zip(test.QuoteNumber,test_rfregress_mse_predictions), columns = [&#39;QuoteNumber&#39;,&#39;QuoteConversion_Flag&#39;]) regressor_submission.to_csv(path/&#39;regressor_submission.csv&#39;,index=False) .",
            "url": "https://redditech.github.io/team-fast-tabulous/kaggle/fastai/2021/07/21/Regressor-Versus-Classifier.html",
            "relUrl": "/kaggle/fastai/2021/07/21/Regressor-Versus-Classifier.html",
            "date": " • Jul 21, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Moving TabularLearner from GPU to CPU",
            "content": "Creating TabularLearner on GPU . You may recall from https://redditech.github.io/team-fast-tabulous/kaggle/fastai/2021/07/08/HomeSite-Quote-A-Fastai-Tabular-Approach.html that we created a TabularLearner using the following steps . df = pd.read_csv(&#39;train.csv&#39;, ...) # plus some EDA y_names = &#39;QuoteConversion_Flag&#39; cont_names, cat_names = cont_cat_split(df, dep_var=y_names) # create TabularPandas to = TabularPandas(df, procs, cat_names, cont_names, y_names, y_block, splits) # create DataLoaders dls = to.dataloaders(bs=4096, val_bs=512, layers=[10000,500], embed_ps=0.02, ps=[0.001,0.01]) # create TabularLearner learn = tabular_learner(dls, metrics=RocAucBinary()) # train the TabularLearner learn.fit_one_cycle(n_epoch=5, lr_max=1e-2, wd=0.002) . Moving TabularLearner from GPU to GPU . Moving the TabularLearner from one GPU to another GPU was easy . # GPU 1 save_pickle(&quot;learner.pkl&quot;, learn) # GPU 2 learn = load_pickle(&quot;learner.pkl&quot;) . Moving TabularLearner from GPU to CPU . However load_pickle(&quot;learner.pkl&quot;) on a CPU will raise an exception because the pickle file is of a TabularLearner created for a GPU. The solution is to rebuild the TabularLearner on the CPU from the DataLoaders and the TabularModel. But first you have to convert the DataLoaders and the TabularModel to CPU versions and you have to do that while on the GPU or they won&#39;t load on the CPU. Use the to() method on both objects which converts in place and returns the converted object. . # GPU save_pickle(&quot;dataloaders_cpu.pkl&quot;, learn.dls.to(&quot;cpu&quot;)) save_pickle(&quot;tabularmodel_cpu.pkl&quot;, learn.model.to(&quot;cpu&quot;)) # CPU dls = load_pickle(&quot;dataloaders_cpu.pkl&quot;) mdl = load_pickle(&quot;tabularmodel_cpu.pkl&quot;) learn = TabularLearner(dls=dls, model=mdl) . To check it loaded correctly on CPU, calculate some predictions and calculate the roc_auc_score . preds, targs = learn.get_preds() print(f&quot;Trained deep learning model has a roc_auc_score of {roc_auc_score(to_np(targs), to_np(preds[:,1]))}&quot;) . And if correct it should print the same roc_auc_score calculated on the GPU. . Trained deep learning model has a roc_auc_score of 0.9630509311593953 . Alternative . If you don&#39;t have your dataloaders converted to CPU and then pickled, but you do have your TabularPandas pickled, then because TabularPandas doesn&#39;t need to be converted to CPU you can do the following . # GPU save_pickle(&quot;tabularpandas.pkl&quot;, to) save_pickle(&quot;tabularmodel_cpu.pkl&quot;, learn.model.to(&quot;cpu&quot;)) # CPU to = load_pickle(&quot;tabularpandas.pkl&quot;) dls = to.dataloaders(bs=4096, val_bs=512, layers=[10000,500], embed_ps=0.02, ps=[0.001,0.01]) mdl = load_pickle(&quot;tabularmodel_cpu.pkl&quot;) learn = TabularLearner(dls=dls, model=mdl) . Alternatives which don&#39;t work . The error message when loading a GPU TabularLearner on a CPU is: . RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device(&#39;cpu&#39;) to map your storages to the CPU. . The error message suggests that there is something that can be done on the CPU which can solve the problem. However I tried doing what it suggested. . # GPU save_pickle(&quot;tabularlearner.pkl&quot;, learn) save_pickle(&quot;tabularpandas.pkl&quot;, to) # CPU learn = load_pickle(&quot;tabularlearner.pkl&quot;) # RuntimeError above learn = torch.load(&quot;tabularlearner.pkl&quot;, map_location=torch.device(&quot;cpu&quot;)) # RuntimeError above . Another suggestion was using learn.save(&quot;learn_save&quot;) on the GPU which saves a file called learn_save.pth. This file can be loaded on CPU with torch.load(&quot;learn_save.pth&quot;, map_location=torch.device(&quot;cpu&quot;)) but it just gives a OrderDict with keys [&quot;model&quot;, &quot;opt&quot;]. Looking online suggested I save the model state dict and use load_state-dict method on an empty model to load it back in. It looks like learn_save.pth maybe such a state dict but I didn&#39;t know how to create an empty TabularModel to call method load_state_dict on. . Finally I thought that because the to(&quot;cpu&quot;) method works in place, I could convert learn to CPU version on GPU and only need to pickle one file. However none of the following attempts on the GPU created file which could be loaded on CPU. . # Failure 1 learn.to(&quot;cpu&quot;) save_pickle(&quot;fail.pkl&quot;, learn) # Failure 2 learn.model.to(&quot;cpu&quot;) learn.dls.to(&quot;cpu&quot;) save_pickle(&quot;fail.pkl&quot;, learn) # Failure 3 learn.model = learn.model.to(&quot;cpu&quot;) learn.dls = learn.dls.to(&quot;cpu&quot;) save_pickle(&quot;fail.pkl&quot;, learn) .",
            "url": "https://redditech.github.io/team-fast-tabulous/jupyter/2021/07/19/Moving-TabularLearner-from-GPU-to-CPU.html",
            "relUrl": "/jupyter/2021/07/19/Moving-TabularLearner-from-GPU-to-CPU.html",
            "date": " • Jul 19, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Pickled Model using ipwidgets",
            "content": "import logging import random import ipywidgets as widgets import pandas as pd import numpy as np from fastai.tabular.all import * from IPython.display import display from IPython.utils import io # using io.capture_output from sklearn.metrics import roc_auc_score . Set up . Specify the folder which contains the original kaggle data (train.csv and test.csv) and the trained model (learn_0708.pkl) . pd.options.mode.chained_assignment = None # default=&#39;warn&#39; path = Path(&#39;data/homesite-quote&#39;) logger = logging.getLogger(&quot;load_pickled_model&quot;) logging.basicConfig(level=logging.INFO) . trained_dl_pkl = &quot;learn_0708.pkl&quot; learn = load_pickle(path/trained_dl_pkl) preds, targs = learn.get_preds() logger.debug(f&quot;Trained deep learning model {trained_dl_pkl} has a roc_auc_score of {roc_auc_score(to_np(targs), to_np(preds[:,1]))}&quot;) . df_train = pd.read_csv(path/&#39;train.csv&#39;, low_memory=False, parse_dates=[&#39;Original_Quote_Date&#39;], index_col=&quot;QuoteNumber&quot;) df_test = pd.read_csv(path/&#39;test.csv&#39;, low_memory=False, parse_dates=[&#39;Original_Quote_Date&#39;], index_col=&quot;QuoteNumber&quot;) sr_conv = df_train[&#39;QuoteConversion_Flag&#39;] df_train.drop(&#39;QuoteConversion_Flag&#39;, inplace=True, axis=1) df = pd.concat([df_train, df_test]) df = add_datepart(df, &#39;Original_Quote_Date&#39;) logger.debug(f&quot;{df.shape} {df_train.shape} {df_test.shape} {sr_conv.shape}&quot;) df_train = None df_test = None . Create a sensitivity analysis tool . A field is sensitive if changing the value of the field can change the outcome of the predicted quote success . While logging is INFO some logging will occur during a normal run. Setting logging level to WARNING will only log if an unknown dtype is encountered. See setup above to set level. . def sensitivity_analysis(qn): &quot;&quot;&quot;Using data from quote number qn do a sensitivity analysis on all independent variables&quot;&quot;&quot; # Independent variables ind_original = df.loc[qn] prd = learn.predict(ind_original) # Predicted quote conversion flag qcf_original = prd[1].item() # Probability that quote conversion flag is as predicted prb_original = prd[2][qcf_original].item() logger.info(f&quot;Sensitivity Analysis for Quote {qn}&quot;) # Check if we actually know the correct answer if qn in sr_conv.index: logger.info(f&quot;Actual QuoteConversion_Flag {sr_conv[qn]}&quot;) def tf_sensitive(f, v_original, lst_v, p_original): &quot;&quot;&quot;predicts quote success after changing field f from v_original to each value in lst_v. If prediction changes then quote is sensitive to the value of this field and True is returned&quot;&quot;&quot; # Create a DataFrame which has every row identical except for field in question # Field f iterates through every value provided ind_other = df.loc[qn:qn].copy().drop(f, axis=1) # fields other than f ind_f = pd.DataFrame(data={f: lst_v}, index=[qn] * len(lst_v)) # Merge these two DataFrames to create one with all rows identical except field f ind = pd.merge(ind_other, ind_f, right_index=True, left_index=True) # Copy lines from learn.predict() because we want to predict several rows at once (faster than one at a time) dl = learn.dls.test_dl(ind) dl.dataset.conts = dl.dataset.conts.astype(np.float32) # stop learn.get_preds() printing blank lines with io.capture_output() as captured: # using get_preds() rather than predict() because get_preds can do multiple rows at once inp,preds,_,dec_preds = learn.get_preds(dl=dl, with_input=True, with_decoded=True) tf = False # Check if any predictions changed for i, dp in enumerate(dec_preds): qcf = dp.item() if qcf != qcf_original: prb = preds[i][qcf].item() logger.info(f&quot;Changing {f} from {val_original} to {lst_v[i]} changes predicted quote conversion flag &quot; f&quot;from {prb_original:.2%} {qcf_original} to {prb:.2%} {qcf}&quot;) tf = True return tf set_sensitive = set() # Loop through all fields. Check different values of each field to see if result is sensitive to it. for field in df.columns: ind = ind_original.copy() val_original = ind[field] tf_important = False num_unique = df[field].nunique() # If number of unique values is under 30 then try every value (or for objects try every value) if num_unique &lt; 30 or df.dtypes[field] == &#39;O&#39;: lst_unique = df[field].unique() if tf_sensitive(field, val_original, lst_unique, prb_original): tf_important = True if tf_important: logger.info(f&quot;Possible values of {field} are {lst_unique}&quot;) set_sensitive.add(field) else: if df.dtypes[field] == &quot;int64&quot;: vmin = df[field].min() vmax = df[field].max() lst_val = [vmin + (vmax - vmin) * i // 10 for i in range(11)] logger.debug(f&quot;{field} {num_unique} {df.dtypes[field]!r} {vmin} {vmax} {lst_val}&quot;) if tf_sensitive(field, val_original, lst_val, prb_original): tf_important = True elif df.dtypes[field] == &quot;float64&quot;: vmin = df[field].min() vmax = df[field].max() lst_val = [vmin + (vmax - vmin) * i / 10 for i in range(11)] logger.debug(f&quot;{field} {num_unique} {df.dtypes[field]!r} {vmin} {vmax} {lst_val}&quot;) if tf_sensitive(field, val_original, lst_val, prb_original): tf_important = True else: logger.warning(f&quot;Unknown type {field} {num_unique} {df.dtypes[field]!r}&quot;) if tf_important: set_sensitive.add(field) # return the set of fields which had individual effects on the prediction return set_sensitive . Application: Step 1 - Ask user for quote number . Try quote 325710 for a quote with many fields which could be changed . qn_min = sr_conv.index.min() qn_max = sr_conv.index.max() qn = random.randint(qn_min, qn_max) . wdg_quote_success = widgets.Label(value=&quot;&quot;) def handle_quote_number_change(change): qn = change.new with io.capture_output() as captured: prd = learn.predict(df.loc[qn]) qcf = prd[1].item() prb = prd[2][qcf].item() act = sr_conv[qn] if qn in sr_conv else &quot;unknown&quot; wdg_quote_success.value = f&quot;Quote {change.new} actual {act} predicted {prb:.2%} {qcf}&quot; style = {&#39;description_width&#39;: &#39;initial&#39;, &#39;width&#39;: &#39;500px&#39;} wdg_quote_number = widgets.IntSlider(description=&quot;Quote number&quot;, min=qn_min, max=qn_max, value=qn, style=style, layout={&#39;width&#39;: &#39;800px&#39;}) wdg_quote_number.observe(handle_quote_number_change, names=&#39;value&#39;) display(wdg_quote_number) display(wdg_quote_success) . Application: Step 2 - Do sensitivity analysis . Normally we would hide the logging output but it helps us playing with data later. . out = widgets.Output(layout={&#39;border&#39;: &#39;1px solid green&#39;}) with out: set_field = sensitivity_analysis(wdg_quote_number.value) display(out) # list of sensitive fields L(set_field) . (#26) [&#39;PersonalField11&#39;,&#39;CoverageField9&#39;,&#39;PropertyField37&#39;,&#39;PersonalField80&#39;,&#39;SalesField10&#39;,&#39;PersonalField83&#39;,&#39;PersonalField4A&#39;,&#39;PersonalField81&#39;,&#39;PropertyField39A&#39;,&#39;PropertyField29&#39;...] . Application: Step 3 - Try altering values of sensitive fields . You can enter more than one to try to improve probability of quote success . Example CoverageField9 from E to B and SalesField10 from 0 to 6 . qn = wdg_quote_number.value lst_dropdown = [] lst_radio = [] style = {&#39;description_width&#39;: &#39;initial&#39;} def nan_if_nan(n): &quot;&quot;&quot;Can&#39;t include np.nan in dropdowns as np.nan != np.nan. Instead use a str&quot;&quot;&quot; try: if np.isnan(n): return &quot;nan&quot; except TypeError as te: pass return n for f in set_field: num_unique = df[f].nunique() lst_unique = sorted((str(nan_if_nan(u)), nan_if_nan(u)) for u in df[f].unique()) v = nan_if_nan(df.loc[qn,f]) if num_unique &lt; 5: wdg = widgets.RadioButtons(options=lst_unique, description=f, style=style, value=v) lst_radio.append(wdg) else: wdg = widgets.Dropdown(options=lst_unique, description=f, style=style, value=v) lst_dropdown.append(wdg) display(widgets.HBox(children=lst_radio)) display(widgets.VBox(children=lst_dropdown)) . Application: Step 4 - Calculate new probability of success . Example CoverageField9 from E to B and SalesField10 from 0 to 6 . Quote went from 58% unsuccessful to 78% successful . qn = wdg_quote_number.value ind = df.loc[qn].copy() for w in lst_radio + lst_dropdown: if w.value == &quot;nan&quot;: v = np.nan else: v = w.value ind[w.description] = v with io.capture_output() as captured: prd = learn.predict(ind) qcf = prd[1].item() prb = prd[2][qcf].item() act = sr_conv[qn] if qn in sr_conv else &quot;unknown&quot; print(f&quot;Quote {qn} actual {act} predicted {prb:.2%} {qcf}&quot;) . Quote 325710 actual unknown predicted 77.76% 1 .",
            "url": "https://redditech.github.io/team-fast-tabulous/jupyter/2021/07/13/Pickled-Model-using-ipywidgets.html",
            "relUrl": "/jupyter/2021/07/13/Pickled-Model-using-ipywidgets.html",
            "date": " • Jul 13, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Load pickled model and convert quote to successful",
            "content": "import logging from fastai.tabular.all import * import pandas as pd import numpy as np pd.options.mode.chained_assignment = None # default=&#39;warn&#39; from sklearn.metrics import roc_auc_score from IPython.utils import io # using io.capture_output . Set up . Specify the folder which contains the original kaggle data (train.csv and test.csv) and the trained model (learn_0708.pkl) . path = Path(&#39;data/homesite-quote&#39;) logger = logging.getLogger(&quot;load_pickled_model&quot;) logging.basicConfig(level=logging.INFO) . Load Deep Learning model . I created my pickled deep learning model using notebook https://redditech.github.io/team-fast-tabulous/kaggle/fastai/2021/07/08/HomeSite-Quote-A-Fastai-Tabular-Approach.html and in the deep learning model section after cell . dl_roc_auc_score=roc_auc_score(to_np(targs), to_np(preds[:,1])) . I ran the command . save_pickle(path/&quot;learn_0708.pkl&quot;, learn) . Now I can load that pickle into this notebook and check it gives the same dl_roc_auc_score which should be 0.963051 . trained_dl_pkl = &quot;learn_0708.pkl&quot; learn = load_pickle(path/trained_dl_pkl) preds, targs = learn.get_preds() print(f&quot;Trained deep learning model {trained_dl_pkl} has a roc_auc_score of {roc_auc_score(to_np(targs), to_np(preds[:,1]))}&quot;) . Trained deep learning model learn_0708.pkl has a roc_auc_score of 0.9630509268687871 . Load XGBoost model (optional) . I created my pickled xgboost model using notebook https://redditech.github.io/team-fast-tabulous/kaggle/fastai/2021/07/08/HomeSite-Quote-A-Fastai-Tabular-Approach.html and in the XGBoost model section after cell . plot_importance(xgb_model, height=1,max_num_features=20,) . I ran the commands . save_pickle(path/&quot;to_0708.pkl&quot;, to) save_pickle(path/&quot;xgb_model_0708.pkl&quot;, xgb_model) . Now I can load those pickles into this notebook and check it gives the same xg_roc_auc_score which should be 0.964158 . to = load_pickle(path/&quot;to_0708.pkl&quot;) trained_xgb_pkl = &quot;xgb_model_0708.pkl&quot; xgb_model = load_pickle(path/trained_xgb_pkl) xgb_preds = xgb_model.predict_proba(to.valid.xs) print(f&quot;Trained XGBoost model {trained_xgb_pkl} has a roc_auc_score of {roc_auc_score(to.valid.ys.values.ravel(), xgb_preds[:, 1])}&quot;) . Trained XGBoost model xgb_model_0708.pkl has a roc_auc_score of 0.9641575675307634 . Create a DataFrame of all quotes, train and test . I want all quotes so I can call up any quote number. We have finished training so doesn&#39;t matter whether in train or test. We save the actual conversion flags from train in sr_conv for later reference. . df_train = pd.read_csv(path/&#39;train.csv&#39;, low_memory=False, parse_dates=[&#39;Original_Quote_Date&#39;], index_col=&quot;QuoteNumber&quot;) df_test = pd.read_csv(path/&#39;test.csv&#39;, low_memory=False, parse_dates=[&#39;Original_Quote_Date&#39;], index_col=&quot;QuoteNumber&quot;) sr_conv = df_train[&#39;QuoteConversion_Flag&#39;] df_train.drop(&#39;QuoteConversion_Flag&#39;, inplace=True, axis=1) df = pd.concat([df_train, df_test]) df = add_datepart(df, &#39;Original_Quote_Date&#39;) print(df.shape, df_train.shape, df_test.shape, sr_conv.shape) df_train = None df_test = None . (434589, 309) (260753, 297) (173836, 297) (260753,) . Looking at actual conversion flags we can see a few which were successful, eg 25, 26, 32, 47 . sr_conv.head(30) . QuoteNumber 1 0 2 0 4 0 6 0 8 0 12 0 13 0 14 0 18 0 19 0 20 0 22 0 25 1 26 1 28 0 29 0 30 0 32 1 35 0 37 0 38 0 40 0 41 0 44 0 45 0 47 1 50 0 51 0 53 0 57 0 Name: QuoteConversion_Flag, dtype: int64 . Looking at sorted df we can see all quote numbers are represented, at least in the first 30 ;) . df.sort_index().head(30) . Field6 Field7 Field8 Field9 Field10 Field11 Field12 CoverageField1A CoverageField1B CoverageField2A ... Original_Quote_Day Original_Quote_Dayofweek Original_Quote_Dayofyear Original_Quote_Is_month_end Original_Quote_Is_month_start Original_Quote_Is_quarter_end Original_Quote_Is_quarter_start Original_Quote_Is_year_end Original_Quote_Is_year_start Original_Quote_Elapsed . QuoteNumber . 1 B | 23 | 0.9403 | 0.0006 | 965 | 1.0200 | N | 17 | 23 | 17 | ... | 16 | 4 | 228 | False | False | False | False | False | False | 1.376611e+09 | . 2 F | 7 | 1.0006 | 0.0040 | 548 | 1.2433 | N | 6 | 8 | 6 | ... | 22 | 1 | 112 | False | False | False | False | False | False | 1.398125e+09 | . 3 E | 16 | 0.9364 | 0.0006 | 1,487 | 1.3045 | N | 4 | 4 | 4 | ... | 12 | 1 | 224 | False | False | False | False | False | False | 1.407802e+09 | . 4 F | 7 | 1.0006 | 0.0040 | 548 | 1.2433 | N | 7 | 12 | 7 | ... | 25 | 0 | 237 | False | False | False | False | False | False | 1.408925e+09 | . 5 F | 11 | 0.9919 | 0.0038 | 564 | 1.1886 | N | 8 | 14 | 8 | ... | 7 | 5 | 250 | False | False | False | False | False | False | 1.378512e+09 | . 6 J | 10 | 0.9769 | 0.0004 | 1,165 | 1.2665 | N | 3 | 2 | 3 | ... | 15 | 0 | 105 | False | False | False | False | False | False | 1.365984e+09 | . 7 F | 15 | 0.8945 | 0.0038 | 564 | 1.0670 | N | 11 | 18 | 11 | ... | 29 | 4 | 88 | False | False | False | False | False | False | 1.364515e+09 | . 8 E | 23 | 0.9472 | 0.0006 | 1,487 | 1.3045 | N | 8 | 13 | 8 | ... | 25 | 5 | 25 | False | False | False | False | False | False | 1.390608e+09 | . 9 K | 21 | 0.8870 | 0.0004 | 1,113 | 1.2665 | Y | 14 | 22 | 15 | ... | 21 | 5 | 80 | False | False | False | False | False | False | 1.426896e+09 | . 10 B | 25 | 0.9153 | 0.0007 | 935 | 1.0200 | N | 4 | 5 | 4 | ... | 10 | 2 | 344 | False | False | False | False | False | False | 1.418170e+09 | . 11 B | 24 | 0.9403 | 0.0006 | 965 | 1.0200 | N | 11 | 18 | 11 | ... | 19 | 4 | 200 | False | False | False | False | False | False | 1.374192e+09 | . 12 E | 14 | 0.9472 | 0.0006 | 1,487 | 1.3045 | N | 13 | 20 | 13 | ... | 18 | 5 | 18 | False | False | False | False | False | False | 1.390003e+09 | . 13 J | 23 | 0.9258 | 0.0004 | 1,165 | 1.2665 | N | 16 | 23 | 17 | ... | 1 | 4 | 305 | False | True | False | False | False | False | 1.383264e+09 | . 14 B | 25 | 0.9153 | 0.0007 | 935 | 1.0200 | N | 5 | 8 | 5 | ... | 14 | 2 | 134 | False | False | False | False | False | False | 1.400026e+09 | . 15 E | 23 | 0.9392 | 0.0006 | 1,487 | 1.3045 | N | 5 | 6 | 5 | ... | 28 | 0 | 209 | False | False | False | False | False | False | 1.406506e+09 | . 16 B | 25 | 0.9153 | 0.0007 | 935 | 1.0200 | N | 6 | 9 | 6 | ... | 20 | 1 | 20 | False | False | False | False | False | False | 1.421712e+09 | . 17 J | 23 | 0.8928 | 0.0004 | 1,113 | 1.2665 | N | 12 | 20 | 12 | ... | 28 | 2 | 148 | False | False | False | False | False | False | 1.401235e+09 | . 18 J | 10 | 0.9691 | 0.0004 | 1,165 | 1.2665 | N | 5 | 8 | 6 | ... | 19 | 2 | 170 | False | False | False | False | False | False | 1.371600e+09 | . 19 F | 11 | 0.9919 | 0.0038 | 564 | 1.1886 | N | 11 | 19 | 11 | ... | 18 | 1 | 169 | False | False | False | False | False | False | 1.371514e+09 | . 20 B | 23 | 0.9403 | 0.0006 | 965 | 1.0200 | N | 4 | 4 | 4 | ... | 24 | 1 | 267 | False | False | False | False | False | False | 1.379981e+09 | . 21 J | 23 | 0.9691 | 0.0004 | 1,165 | 1.2665 | N | 3 | 3 | 3 | ... | 11 | 3 | 192 | False | False | False | False | False | False | 1.373501e+09 | . 22 B | 23 | 0.9403 | 0.0006 | 965 | 1.0200 | N | 8 | 14 | 9 | ... | 11 | 1 | 162 | False | False | False | False | False | False | 1.370909e+09 | . 23 B | 25 | 0.9403 | 0.0007 | 935 | 1.0200 | N | 9 | 15 | 9 | ... | 5 | 2 | 36 | False | False | False | False | False | False | 1.391558e+09 | . 24 B | 25 | 0.9153 | 0.0007 | 935 | 1.0200 | N | 17 | 23 | 17 | ... | 27 | 4 | 58 | False | False | False | False | False | False | 1.424995e+09 | . 25 J | 20 | 0.9497 | 0.0004 | 1,165 | 1.2665 | N | 18 | 23 | 18 | ... | 22 | 1 | 295 | False | False | False | False | False | False | 1.382400e+09 | . 26 B | 24 | 0.9403 | 0.0006 | 965 | 1.0200 | N | 5 | 6 | 5 | ... | 20 | 1 | 232 | False | False | False | False | False | False | 1.376957e+09 | . 27 B | 23 | 0.9403 | 0.0006 | 965 | 1.0200 | N | 5 | 7 | 5 | ... | 14 | 3 | 318 | False | False | False | False | False | False | 1.384387e+09 | . 28 F | 22 | 0.9893 | 0.0040 | 548 | 1.2433 | N | 10 | 17 | 10 | ... | 24 | 1 | 55 | False | False | False | False | False | False | 1.424736e+09 | . 29 J | 26 | 0.8793 | 0.0004 | 1,113 | 1.2665 | N | 8 | 14 | 8 | ... | 27 | 0 | 300 | False | False | False | False | False | False | 1.414368e+09 | . 30 B | 25 | 0.9153 | 0.0007 | 935 | 1.0200 | N | 6 | 8 | 6 | ... | 6 | 4 | 37 | False | False | False | False | False | False | 1.423181e+09 | . 30 rows × 309 columns . Check that we have every quote number between 1 and 434589 inclusive. We do. . df.index.min(), df.index.max(), df.index.max() - df.index.min() + 1, df.shape[0], df.index.nunique() . (1, 434589, 434589, 434589, 434589) . To select a quote, use DataFrame.loc with the quote number in square brackets. For example quote number 1 . df.loc[1] . Field6 B Field7 23 Field8 0.9403 Field9 0.0006 Field10 965 ... Original_Quote_Is_quarter_end False Original_Quote_Is_quarter_start False Original_Quote_Is_year_end False Original_Quote_Is_year_start False Original_Quote_Elapsed 1376611200.0 Name: 1, Length: 309, dtype: object . Similarly to select an &quot;actual&quot; quote conversion use Series of conversions sr_conv with quote number in square brackets . sr_conv[2] . 0 . We can see that our model predicts that quote 25 to be sucessful and it was . qn = 25 prd = learn.predict(df.loc[qn]) print(&quot;Predicted success&quot;, prd[1], &quot;with confidence&quot;, prd[2]) print(&quot;Actual success&quot;, sr_conv[qn]) . Predicted success tensor(1) with confidence tensor([0.0107, 0.9893]) Actual success 1 . Create a sensitivity analysis tool . A field is sensitive if changing the value of the field can change the outcome of the predicted quote success . While logging is INFO some logging will occur during a normal run. Setting logging level to WARNING will only log if an unknown dtype is encountered. See setup above to set level. . def sensitivity_analysis(qn): &quot;&quot;&quot;Using data from quote number qn do a sensitivity analysis on all independent variables&quot;&quot;&quot; # Independent variables ind_original = df.loc[qn] prd = learn.predict(ind_original) # Predicted quote conversion flag qcf_original = prd[1].item() # Probability that quote conversion flag is as predicted prb_original = prd[2][qcf_original].item() logger.info(f&quot;Sensitivity Analysis for Quote {qn}&quot;) # Check if we actually know the correct answer if qn in sr_conv.index: logger.info(f&quot;Actual QuoteConversion_Flag {sr_conv[qn]}&quot;) def tf_sensitive(f, v_original, lst_v, p_original): &quot;&quot;&quot;predicts quote success after changing field f from v_original to each value in lst_v. If prediction changes then quote is sensitive to the value of this field and True is returned&quot;&quot;&quot; # Create a DataFrame which has every row identical except for field in question # Field f iterates through every value provided ind_other = df.loc[qn:qn].copy().drop(f, axis=1) # fields other than f ind_f = pd.DataFrame(data={f: lst_v}, index=[qn] * len(lst_v)) # Merge these two DataFrames to create one with all rows identical except field f ind = pd.merge(ind_other, ind_f, right_index=True, left_index=True) # Copy lines from learn.predict() because we want to predict several rows at once (faster than one at a time) dl = learn.dls.test_dl(ind) dl.dataset.conts = dl.dataset.conts.astype(np.float32) # stop learn.get_preds() printing blank lines with io.capture_output() as captured: # using get_preds() rather than predict() because get_preds can do multiple rows at once inp,preds,_,dec_preds = learn.get_preds(dl=dl, with_input=True, with_decoded=True) tf = False # Check if any predictions changed for i, dp in enumerate(dec_preds): qcf = dp.item() if qcf != qcf_original: prb = preds[i][qcf].item() logger.info(f&quot;Changing {f} from {val_original} to {lst_v[i]} changes predicted quote conversion flag &quot; f&quot;from {prb_original:.2%} {qcf_original} to {prb:.2%} {qcf}&quot;) tf = True return tf set_sensitive = set() # Loop through all fields. Check different values of each field to see if result is sensitive to it. for field in df.columns: ind = ind_original.copy() val_original = ind[field] tf_important = False num_unique = df[field].nunique() # If number of unique values is under 30 then try every value (or for objects try every value) if num_unique &lt; 30 or df.dtypes[field] == &#39;O&#39;: lst_unique = df[field].unique() if tf_sensitive(field, val_original, lst_unique, prb_original): tf_important = True if tf_important: logger.info(f&quot;Possible values of {field} are {lst_unique}&quot;) set_sensitive.add(field) else: if df.dtypes[field] == &quot;int64&quot;: vmin = df[field].min() vmax = df[field].max() lst_val = [vmin + (vmax - vmin) * i // 10 for i in range(11)] logger.debug(f&quot;{field} {num_unique} {df.dtypes[field]!r} {vmin} {vmax} {lst_val}&quot;) if tf_sensitive(field, val_original, lst_val, prb_original): tf_important = True elif df.dtypes[field] == &quot;float64&quot;: vmin = df[field].min() vmax = df[field].max() lst_val = [vmin + (vmax - vmin) * i / 10 for i in range(11)] logger.debug(f&quot;{field} {num_unique} {df.dtypes[field]!r} {vmin} {vmax} {lst_val}&quot;) if tf_sensitive(field, val_original, lst_val, prb_original): tf_important = True else: logger.warning(f&quot;Unknown type {field} {num_unique} {df.dtypes[field]!r}&quot;) if tf_important: set_sensitive.add(field) # return the set of fields which had individual effects on the prediction return set_sensitive . Use the sensitivity tool . Here are the results of running the sensitivity analysis on quote number 2. . Sensitivity Analysis for Quote 2 . Changing SalesField5 from 5 to 3 changes predicted quote conversion flag from 97.04% 0 to 63.74% 1 Changing SalesField5 from 5 to 4 changes predicted quote conversion flag from 97.04% 0 to 51.11% 1 Changing PersonalField2 from 1 to 0 changes predicted quote conversion flag from 97.04% 0 to 65.11% 1 Changing PersonalField13 from 2 to 1 changes predicted quote conversion flag from 97.04% 0 to 68.25% 1 Changing PropertyField29 from nan to 10.0 changes predicted quote conversion flag from 97.04% 0 to 100.00% 1 Changing PropertyField37 from N to Y changes predicted quote conversion flag from 97.04% 0 to 95.74% 1 . sensitivity_analysis(2) . INFO:load_pickled_model:Sensitivity Analysis for Quote 2 INFO:load_pickled_model:Actual QuoteConversion_Flag 0 INFO:load_pickled_model:Changing SalesField5 from 5 to 3 changes predicted quote conversion flag from 97.04% 0 to 63.74% 1 INFO:load_pickled_model:Changing SalesField5 from 5 to 4 changes predicted quote conversion flag from 97.04% 0 to 51.11% 1 INFO:load_pickled_model:Possible values of SalesField5 are [5 3 2 4 1] INFO:load_pickled_model:Changing PersonalField2 from 1 to 0 changes predicted quote conversion flag from 97.04% 0 to 65.11% 1 INFO:load_pickled_model:Possible values of PersonalField2 are [0 1] INFO:load_pickled_model:Changing PersonalField13 from 2 to 1 changes predicted quote conversion flag from 97.04% 0 to 68.25% 1 INFO:load_pickled_model:Possible values of PersonalField13 are [2 1 4 3] INFO:load_pickled_model:Changing PropertyField29 from nan to 10.0 changes predicted quote conversion flag from 97.04% 0 to 100.00% 1 INFO:load_pickled_model:Possible values of PropertyField29 are [ 0. nan 1. 10.] INFO:load_pickled_model:Changing PropertyField37 from N to Y changes predicted quote conversion flag from 97.04% 0 to 95.74% 1 INFO:load_pickled_model:Possible values of PropertyField37 are [&#39;N&#39; &#39;Y&#39; &#39; &#39;] . {&#39;PersonalField13&#39;, &#39;PersonalField2&#39;, &#39;PropertyField29&#39;, &#39;PropertyField37&#39;, &#39;SalesField5&#39;} .",
            "url": "https://redditech.github.io/team-fast-tabulous/jupyter/2021/07/12/Sensitivity-Analysis-On-Pickled-Models.html",
            "relUrl": "/jupyter/2021/07/12/Sensitivity-Analysis-On-Pickled-Models.html",
            "date": " • Jul 12, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "HomeSite Quote Conversion A Fastai Tabular Approach",
            "content": "Introduction . As Jeremy Howard mentioned in one of his lessons about tabular data, there are 2 main methods in modelling tabular or structured data: . Ensembles of Decision Trees (Random Forest or Gradient Boosting Machine) | Multilayered Neural Networks | The former method is far more popular and usually our first approach when analysing new tabular dataset. This is why he was focusing more on how to train and improve performance of a random forest model but not going too deep into training a deep learning model . However, Fastai itself has a lot of great function to train and improve performance of tabular data model and this is what we want to explore in this article. . Most of the techniques and codes in this notebooks were borrowed from Walk with Fastai from Zack Mueller so please check out his official website: https://walkwithfastai.com/ . When you think about improving your model’s performance, some of the methods that you will definitely consider include: . Feature Engineering | Hyperparameter Tuning | Combining Models | . This is also exactly what we are going to discuss in this article. . Basic Deep Learning Model for Tabular Data | Permutation Analysis to identify Important Features | Model Ensembling | Hyperparameter Tuning with Bayesian Optimisation | Entity Embeddings | Setup . !pip install -Uqq fastai fastbook import fastbook fastbook.setup_book() . from fastai.tabular.all import * import pandas as pd import numpy as np pd.options.mode.chained_assignment = None # default=&#39;warn&#39; from sklearn.metrics import roc_auc_score . Download Data . Download your Kaggle KPI, store it in a folder in Google Drive and run the below codes . !mkdir -p ~/.kaggle !cp /content/gdrive/MyDrive/Kaggle/kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . Assign the folder where to want to store the data to path . path = Path(&#39;/content/gdrive/MyDrive/Kaggle/&#39; + &#39;data/homesite-quote&#39;) path.mkdir(parents=True, exist_ok=True) path . Path(&#39;/content/gdrive/MyDrive/Kaggle/data/homesite-quote&#39;) . Download your data into that folder . !kaggle competitions download -c homesite-quote-conversion -p /content/gdrive/MyDrive/Kaggle/data/homesite-quote . Unzip your data . ! unzip -q -n &#39;{path}/train.csv.zip&#39; -d &#39;{path}&#39; ! unzip -q -n &#39;{path}/test.csv.zip&#39; -d &#39;{path}&#39; . Import data and store it as a dataframe . df = pd.read_csv(path/&#39;train.csv&#39;, low_memory=False, parse_dates=[&#39;Original_Quote_Date&#39;]) test_df=pd.read_csv(path/&#39;test.csv&#39;, low_memory=False, parse_dates=[&#39;Original_Quote_Date&#39;]) . EDA . df_train = df.copy() df_test = test_df.copy() . df_train.QuoteConversion_Flag = df_train.QuoteConversion_Flag.astype(dtype=&#39;boolean&#39;) . df_train = df_train.set_index(&#39;QuoteNumber&#39;) df_test = df_test.set_index(&#39;QuoteNumber&#39;) . df_train = add_datepart(df_train, &#39;Original_Quote_Date&#39;) df_test = add_datepart(df_test, &#39;Original_Quote_Date&#39;) . y_names = &#39;QuoteConversion_Flag&#39; cont_names, cat_names = cont_cat_split(df_train, dep_var=y_names) len(cont_names), len(cat_names) . (155, 154) . Deep Learning Model . Set up hyperparameters . random_seed = 42 bs = 4096 val_bs = 512 test_size = 0.3 epochs = 5 lr = 1e-2 wd=0.002 layers = [10000,500] dropout = [0.001, 0.01] y_block=CategoryBlock() emb_dropout=0.02 set_seed(42) roc_auc_binary = RocAucBinary() . procs = [Categorify, FillMissing, Normalize] splits = TrainTestSplitter(test_size=test_size, stratify=df_train[y_names])(df_train) . to = TabularPandas(df=df_train, procs=procs, cat_names=cat_names, cont_names=cont_names, y_names=y_names,splits=splits, y_block=y_block) dls = to.dataloaders(bs=bs, val_bs=val_bs, layers=layers, embed_ps=emb_dropout, ps=dropout) dls.valid.show_batch() . Field6 Field10 Field12 CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 SalesField3 SalesField4 SalesField5 SalesField7 SalesField9 SalesField10 SalesField11 SalesField13 SalesField14 SalesField15 PersonalField1 PersonalField2 PersonalField5 PersonalField6 PersonalField7 PersonalField8 PersonalField9 PersonalField11 PersonalField12 PersonalField13 PersonalField16 PersonalField17 PersonalField18 PersonalField19 PersonalField22 PersonalField23 PersonalField24 PersonalField25 PersonalField26 PersonalField27 PersonalField28 PersonalField29 PersonalField30 PersonalField31 PersonalField32 PersonalField33 PersonalField34 PersonalField35 PersonalField36 PersonalField37 PersonalField38 PersonalField39 PersonalField40 PersonalField41 PersonalField42 PersonalField43 PersonalField44 PersonalField45 PersonalField46 PersonalField47 PersonalField48 PersonalField49 PersonalField50 PersonalField51 PersonalField52 PersonalField53 PersonalField54 PersonalField55 PersonalField56 PersonalField57 PersonalField58 PersonalField59 PersonalField60 PersonalField61 PersonalField62 PersonalField63 PersonalField64 PersonalField65 PersonalField66 PersonalField67 PersonalField68 PersonalField69 PersonalField70 PersonalField71 PersonalField72 PersonalField73 PersonalField74 PersonalField75 PersonalField76 PersonalField77 PersonalField78 PersonalField79 PersonalField80 PersonalField81 PersonalField82 PersonalField83 PropertyField2A PropertyField3 PropertyField4 PropertyField5 PropertyField6 PropertyField7 PropertyField8 PropertyField9 PropertyField10 PropertyField11A PropertyField11B PropertyField12 PropertyField13 PropertyField14 PropertyField15 PropertyField17 PropertyField18 PropertyField19 PropertyField20 PropertyField22 PropertyField23 PropertyField27 PropertyField28 PropertyField30 PropertyField31 PropertyField32 PropertyField33 PropertyField34 PropertyField35 PropertyField36 PropertyField37 PropertyField38 GeographicField5A GeographicField5B GeographicField10A GeographicField10B GeographicField14A GeographicField14B GeographicField18A GeographicField21A GeographicField22A GeographicField22B GeographicField23A GeographicField56A GeographicField60A GeographicField61A GeographicField62A GeographicField62B GeographicField63 GeographicField64 Original_Quote_Year Original_Quote_Month Original_Quote_Dayofweek Original_Quote_Is_month_end Original_Quote_Is_month_start Original_Quote_Is_quarter_end Original_Quote_Is_quarter_start Original_Quote_Is_year_end Original_Quote_Is_year_start PersonalField84_na PropertyField29_na Field7 Field8 Field9 Field11 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField6 SalesField8 SalesField12 PersonalField4A PersonalField4B PersonalField10A PersonalField10B PersonalField14 PersonalField15 PersonalField84 PropertyField1A PropertyField1B PropertyField2B PropertyField16A PropertyField16B PropertyField21A PropertyField21B PropertyField24A PropertyField24B PropertyField25 PropertyField26A PropertyField26B PropertyField29 PropertyField39A PropertyField39B GeographicField1A GeographicField1B GeographicField2A GeographicField2B GeographicField3A GeographicField3B GeographicField4A GeographicField4B GeographicField6A GeographicField6B GeographicField7A GeographicField7B GeographicField8A GeographicField8B GeographicField9A GeographicField9B GeographicField11A GeographicField11B GeographicField12A GeographicField12B GeographicField13A GeographicField13B GeographicField15A GeographicField15B GeographicField16A GeographicField16B GeographicField17A GeographicField17B GeographicField18B GeographicField19A GeographicField19B GeographicField20A GeographicField20B GeographicField21B GeographicField23B GeographicField24A GeographicField24B GeographicField25A GeographicField25B GeographicField26A GeographicField26B GeographicField27A GeographicField27B GeographicField28A GeographicField28B GeographicField29A GeographicField29B GeographicField30A GeographicField30B GeographicField31A GeographicField31B GeographicField32A GeographicField32B GeographicField33A GeographicField33B GeographicField34A GeographicField34B GeographicField35A GeographicField35B GeographicField36A GeographicField36B GeographicField37A GeographicField37B GeographicField38A GeographicField38B GeographicField39A GeographicField39B GeographicField40A GeographicField40B GeographicField41A GeographicField41B GeographicField42A GeographicField42B GeographicField43A GeographicField43B GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60B GeographicField61B Original_Quote_Week Original_Quote_Day Original_Quote_Dayofyear Original_Quote_Elapsed QuoteConversion_Flag . 0 B | 935 | N | 13 | 22 | 13 | 23 | T | D | 1 | 5 | 5 | M | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | N | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | O | 1 | 0 | 1 | -1 | 21 | 4 | 2 | C | 1 | 1 | 2 | 0 | 0 | 2 | 4 | 4 | B | N | K | Y | G | N | 2 | N | N | N | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 9 | N | CA | 2014 | 2 | 6 | False | False | False | False | False | False | False | True | 25.0 | 0.9403 | 0.0007 | 1.0200 | 9.0 | 15.0 | 9.0 | 15.0 | 8.0 | 13.0 | 8.0 | 14.0 | 3.000000 | 5.0 | 4.0 | 11.0 | 3.0 | 10.0 | 11.0 | 61662.999557 | -2.965073e-08 | 4.0 | 5.0 | 14.0 | 21.0 | 2.0 | 24.0 | 2.0 | 7.0 | 10.0 | 23.0 | 3.0 | 6.0 | 9.0 | 15.0 | 8.0 | 11.0 | 1.0 | 14.0 | 20.0 | -3.456605e-13 | 8.0 | 7.0 | 7.0 | 15.0 | 11.0 | 10.0 | 5.0 | 4.0 | 13.0 | 13.0 | 2.0 | 5.0 | 6.0 | 7.0 | 2.0 | 7.0 | 2.0 | 2.0 | 2.0 | 7.0 | 4.0 | 7.0 | 2.0 | 7.0 | 9.0 | 8.0 | 3.0 | 7.0 | 2.000000 | 18.0 | 2.0 | 2.0 | 4.0 | 3.0 | 7.0 | 8.0 | 8.0 | 16.0 | 14.0 | 18.0 | 18.0 | 20.0 | 22.0 | 15.0 | 11.0 | 10.0 | 17.0 | 14.0 | 21.0 | 5.0 | 6.0 | 10.0 | 15.0 | 11.0 | 18.0 | 7.0 | 9.0 | 16.0 | 21.0 | 3.0 | 3.0 | 6.0 | 5.0 | 2.0 | 3.0 | 7.0 | 10.0 | 7.0 | 15.0 | 3.000000 | 8.0 | 18.0 | 16.0 | 7.0 | 7.0 | 1.0 | 1.0 | 7.0 | 2.0 | 7.0 | 7.0 | 13.0 | 15.0 | 1.0 | 1.0 | 10.0 | 7.0 | 12.0 | 6.0 | 8.0 | 4.0 | 9.0 | 7.0 | 9.0 | 7.0 | 13.0 | 7.0 | 13.0 | 12.0 | 14.0 | 22.0 | 18.0 | 9.0 | 6.0 | 15.0 | 20.0 | 10.0 | 10.0 | 3.0 | 23.0 | 6.000001 | 9.000000 | 40.000002 | 1.391904e+09 | False | . 1 E | 1,480 | N | 13 | 22 | 13 | 23 | T | F | 0 | 5 | 5 | R | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | YF | ZK | XK | XC | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | Y | Y | Y | 0 | R | 1 | 0 | 1 | -1 | 21 | 2 | 2 | C | 4 | 1 | 2 | 0 | 0 | 2 | 1 | 10 | B | N | O | Y | H | N | 2 | N | N | N | -1 | 13 | -1 | 25 | -1 | 23 | -1 | -1 | -1 | 17 | -1 | -1 | -1 | -1 | -1 | 8 | N | IL | 2013 | 8 | 5 | True | False | False | False | False | False | True | True | 14.0 | 0.9487 | 0.0006 | 1.3045 | 6.0 | 9.0 | 6.0 | 9.0 | 5.0 | 8.0 | 6.0 | 9.0 | 4.000000 | 8.0 | 3.0 | 8.0 | 4.0 | 12.0 | 4.0 | 1129.999604 | -2.965073e-08 | 19.0 | 22.0 | 6.0 | 12.0 | 2.0 | 7.0 | 2.0 | 6.0 | 9.0 | 3.0 | 4.0 | 8.0 | 6.0 | 9.0 | 10.0 | 15.0 | 1.0 | 17.0 | 23.0 | -3.456605e-13 | 18.0 | 22.0 | 6.0 | 13.0 | 14.0 | 13.0 | 13.0 | 16.0 | 9.0 | 7.0 | 13.0 | 22.0 | 16.0 | 19.0 | 13.0 | 20.0 | 25.0 | 25.0 | 11.0 | 19.0 | 16.0 | 19.0 | 11.0 | 19.0 | 15.0 | 19.0 | 15.0 | 23.0 | 3.000000 | 18.0 | 18.0 | 15.0 | 16.0 | 2.0 | 5.0 | 24.0 | 19.0 | 2.0 | 1.0 | 2.0 | 2.0 | 2.0 | 2.0 | 2.0 | 1.0 | 6.0 | 8.0 | 6.0 | 6.0 | 8.0 | 15.0 | 12.0 | 20.0 | 7.0 | 11.0 | 5.0 | 5.0 | 21.0 | 24.0 | 21.0 | 24.0 | 7.0 | 8.0 | 4.0 | 9.0 | 11.0 | 18.0 | 14.0 | 23.0 | 24.999999 | 25.0 | 15.0 | 13.0 | 25.0 | 25.0 | 12.0 | 20.0 | 13.0 | 13.0 | 7.0 | 7.0 | 21.0 | 23.0 | 5.0 | 3.0 | 18.0 | 18.0 | 17.0 | 13.0 | 13.0 | 9.0 | 17.0 | 18.0 | 17.0 | 19.0 | 20.0 | 18.0 | 8.0 | 5.0 | 4.0 | 6.0 | 12.0 | 10.0 | 8.0 | 12.0 | 14.0 | 18.0 | 22.0 | 16.0 | 24.0 | 35.000000 | 31.000001 | 242.999997 | 1.377907e+09 | False | . 2 F | 548 | N | 13 | 22 | 25 | 25 | Y | E | 1 | 5 | 5 | P | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | ZH | XV | YF | XX | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | A | 1 | 0 | 1 | -1 | 21 | 1 | 2 | A | 1 | 1 | 1 | 1 | 0 | 2 | 1 | 4 | B | N | O | Y | H | Y | 2 | N | Y | N | -1 | 18 | -1 | 25 | -1 | 17 | -1 | -1 | -1 | 18 | -1 | -1 | -1 | -1 | -1 | 8 | N | NJ | 2015 | 2 | 3 | False | False | False | False | False | False | True | True | 22.0 | 0.9893 | 0.0040 | 1.2433 | 7.0 | 12.0 | 7.0 | 12.0 | 10.0 | 17.0 | 7.0 | 11.0 | 8.000000 | 17.0 | 4.0 | 10.0 | 4.0 | 12.0 | 11.0 | 30835.999977 | 1.000000e+00 | 3.0 | 3.0 | 6.0 | 13.0 | 1.0 | 10.0 | 2.0 | 24.0 | 25.0 | 12.0 | 3.0 | 6.0 | 7.0 | 12.0 | 9.0 | 13.0 | 2.0 | 6.0 | 7.0 | -3.456605e-13 | 13.0 | 15.0 | 8.0 | 17.0 | 14.0 | 13.0 | 7.0 | 6.0 | 13.0 | 13.0 | 5.0 | 16.0 | 10.0 | 16.0 | 6.0 | 16.0 | 6.0 | 14.0 | 4.0 | 13.0 | 10.0 | 16.0 | 5.0 | 16.0 | 11.0 | 17.0 | 7.0 | 16.0 | 2.000000 | 13.0 | 16.0 | 25.0 | 25.0 | 25.0 | 25.0 | 18.0 | 17.0 | 21.0 | 22.0 | 18.0 | 19.0 | 13.0 | 17.0 | 22.0 | 21.0 | 8.0 | 14.0 | 8.0 | 12.0 | 10.0 | 20.0 | 8.0 | 11.0 | 12.0 | 19.0 | 5.0 | 5.0 | 11.0 | 13.0 | 14.0 | 22.0 | 12.0 | 20.0 | 4.0 | 7.0 | 12.0 | 20.0 | 14.0 | 22.0 | 10.000000 | 17.0 | 23.0 | 20.0 | 14.0 | 18.0 | 9.0 | 11.0 | 20.0 | 21.0 | 9.0 | 12.0 | 20.0 | 22.0 | 6.0 | 7.0 | 17.0 | 16.0 | 14.0 | 8.0 | 14.0 | 10.0 | 17.0 | 19.0 | 17.0 | 18.0 | 17.0 | 13.0 | 9.0 | 6.0 | 7.0 | 14.0 | 2.0 | 15.0 | 17.0 | 17.0 | 22.0 | 16.0 | 20.0 | 8.0 | 24.0 | 9.000000 | 26.000000 | 56.999998 | 1.424909e+09 | True | . 3 B | 935 | N | 13 | 22 | 13 | 23 | Y | D | 1 | 5 | 5 | T | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 1 | 1 | 4 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | O | 1 | 0 | 1 | -1 | 21 | 4 | 2 | C | 1 | 1 | 2 | 0 | 0 | 2 | 1 | 10 | B | N | O | Y | H | Y | 2 | N | N | N | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 21 | N | CA | 2014 | 11 | 2 | False | False | False | False | False | False | False | True | 25.0 | 0.9153 | 0.0007 | 1.0200 | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | 14.000000 | 23.0 | 2.0 | 1.0 | 3.0 | 10.0 | 11.0 | 21377.999465 | -2.965073e-08 | 20.0 | 24.0 | 3.0 | 1.0 | 2.0 | 24.0 | 2.0 | 12.0 | 17.0 | 25.0 | 3.0 | 4.0 | 3.0 | 3.0 | 3.0 | 3.0 | 1.0 | 8.0 | 10.0 | -3.456605e-13 | 12.0 | 15.0 | 14.0 | 23.0 | 11.0 | 10.0 | 5.0 | 4.0 | 20.0 | 22.0 | 2.0 | 4.0 | 8.0 | 10.0 | 2.0 | 10.0 | 1.0 | 1.0 | 2.0 | 7.0 | 6.0 | 10.0 | 2.0 | 10.0 | 11.0 | 18.0 | 3.0 | 10.0 | 2.000000 | 17.0 | 14.0 | 3.0 | 5.0 | 3.0 | 7.0 | 9.0 | 8.0 | 22.0 | 23.0 | 25.0 | 25.0 | 25.0 | 25.0 | 24.0 | 23.0 | 11.0 | 18.0 | 7.0 | 9.0 | 11.0 | 21.0 | 2.0 | 1.0 | 6.0 | 10.0 | 4.0 | 4.0 | 16.0 | 21.0 | 9.0 | 16.0 | 14.0 | 21.0 | 19.0 | 24.0 | 13.0 | 20.0 | 3.0 | 3.0 | 2.000000 | 2.0 | 1.0 | 1.0 | 6.0 | 6.0 | 1.0 | 1.0 | 8.0 | 3.0 | 9.0 | 9.0 | 16.0 | 17.0 | 1.0 | 1.0 | 10.0 | 7.0 | 15.0 | 10.0 | 12.0 | 8.0 | 9.0 | 6.0 | 8.0 | 6.0 | 11.0 | 5.0 | 13.0 | 12.0 | 15.0 | 22.0 | 19.0 | 9.0 | 6.0 | 5.0 | 3.0 | 4.0 | 2.0 | 6.0 | 24.0 | 47.000000 | 19.000000 | 322.999999 | 1.416355e+09 | False | . 4 J | 1,113 | N | 13 | 22 | 1 | 6 | X | G | 0 | 5 | 5 | T | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | N | 1 | 2 | 0 | 1 | 2 | YF | ZK | XN | XC | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | Y | Y | Y | 0 | J | 1 | 0 | 1 | -1 | 21 | 4 | 2 | C | 1 | 1 | 2 | 1 | 0 | 2 | 4 | 10 | B | N | O | N | G | N | 0 | N | N | N | 25 | 25 | -1 | 25 | -1 | 17 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 8 | N | TX | 2014 | 11 | 1 | False | False | False | False | False | False | True | True | 26.0 | 0.8870 | 0.0004 | 1.2665 | 10.0 | 16.0 | 10.0 | 16.0 | 8.0 | 14.0 | 9.0 | 16.0 | 18.000000 | 24.0 | 17.0 | 24.0 | 14.0 | 24.0 | 22.0 | 29808.999885 | 1.000000e+00 | 17.0 | 21.0 | 15.0 | 21.0 | 2.0 | 7.0 | 2.0 | 10.0 | 14.0 | 8.0 | 5.0 | 10.0 | 10.0 | 16.0 | 11.0 | 16.0 | 1.0 | 18.0 | 23.0 | -3.456605e-13 | 11.0 | 12.0 | 2.0 | 2.0 | 10.0 | 8.0 | 2.0 | 2.0 | 14.0 | 15.0 | 11.0 | 19.0 | 21.0 | 20.0 | 12.0 | 19.0 | 17.0 | 20.0 | 12.0 | 20.0 | 19.0 | 19.0 | 13.0 | 20.0 | 23.0 | 22.0 | 10.0 | 19.0 | 2.000000 | 6.0 | 3.0 | 7.0 | 9.0 | 3.0 | 5.0 | 5.0 | 19.0 | 20.0 | 21.0 | 14.0 | 11.0 | 7.0 | 8.0 | 18.0 | 17.0 | 4.0 | 4.0 | 9.0 | 12.0 | 5.0 | 5.0 | 5.0 | 5.0 | 4.0 | 4.0 | 6.0 | 7.0 | 3.0 | 2.0 | 4.0 | 5.0 | 5.0 | 4.0 | 4.0 | 9.0 | 5.0 | 5.0 | 6.0 | 10.0 | 8.000000 | 13.0 | 14.0 | 10.0 | 14.0 | 18.0 | 25.0 | 25.0 | 8.0 | 4.0 | 12.0 | 20.0 | 13.0 | 15.0 | 8.0 | 15.0 | 13.0 | 10.0 | 18.0 | 16.0 | 16.0 | 14.0 | 13.0 | 12.0 | 12.0 | 12.0 | 17.0 | 12.0 | 8.0 | 5.0 | 4.0 | 8.0 | 15.0 | 18.0 | 22.0 | 1.0 | 1.0 | 7.0 | 6.0 | 1.0 | 10.0 | 44.999999 | 4.000000 | 307.999995 | 1.415059e+09 | False | . 5 B | 935 | N | 13 | 22 | 13 | 23 | Y | E | 1 | 5 | 4 | T | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 3 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | O | 0 | 0 | 1 | -1 | 21 | 2 | 1 | C | 2 | 1 | 2 | 0 | 0 | 2 | 1 | 10 | D | N | O | Y | G | Y | 2 | N | Y | N | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 21 | N | CA | 2014 | 9 | 1 | False | False | False | False | False | False | False | True | 25.0 | 0.9153 | 0.0007 | 1.0200 | 9.0 | 15.0 | 9.0 | 15.0 | 8.0 | 13.0 | 8.0 | 15.0 | 7.000000 | 15.0 | 5.0 | 14.0 | 4.0 | 13.0 | 11.0 | 20190.000282 | -2.965073e-08 | 4.0 | 5.0 | 5.0 | 4.0 | 4.0 | 24.0 | 2.0 | 17.0 | 21.0 | 22.0 | 14.0 | 22.0 | 9.0 | 15.0 | 9.0 | 13.0 | 1.0 | 16.0 | 21.0 | -3.456605e-13 | 10.0 | 10.0 | 11.0 | 21.0 | 15.0 | 14.0 | 14.0 | 16.0 | 6.0 | 3.0 | 2.0 | 6.0 | 4.0 | 6.0 | 2.0 | 6.0 | 2.0 | 8.0 | 2.0 | 4.0 | 3.0 | 5.0 | 2.0 | 5.0 | 6.0 | 6.0 | 2.0 | 6.0 | 21.999999 | 25.0 | 24.0 | 4.0 | 8.0 | 6.0 | 17.0 | 8.0 | 6.0 | 17.0 | 16.0 | 19.0 | 22.0 | 16.0 | 19.0 | 21.0 | 20.0 | 9.0 | 15.0 | 17.0 | 23.0 | 11.0 | 21.0 | 21.0 | 24.0 | 4.0 | 5.0 | 12.0 | 19.0 | 10.0 | 11.0 | 12.0 | 20.0 | 13.0 | 20.0 | 4.0 | 7.0 | 4.0 | 3.0 | 9.0 | 17.0 | 2.000000 | 4.0 | 14.0 | 11.0 | 4.0 | 3.0 | 2.0 | 2.0 | 9.0 | 7.0 | 20.0 | 22.0 | 9.0 | 5.0 | 9.0 | 18.0 | 13.0 | 10.0 | 15.0 | 10.0 | 16.0 | 14.0 | 13.0 | 12.0 | 12.0 | 11.0 | 15.0 | 9.0 | 19.0 | 21.0 | 11.0 | 19.0 | 20.0 | 11.0 | 8.0 | 8.0 | 7.0 | 10.0 | 10.0 | 19.0 | 24.0 | 36.000000 | 2.000000 | 245.000000 | 1.409616e+09 | True | . 6 F | 564 | N | 13 | 22 | 13 | 23 | Y | E | 1 | 4 | 3 | V | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | N | 1 | 2 | 0 | 1 | 2 | XM | ZF | XZ | YF | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | R | 0 | 0 | 1 | -1 | 21 | 2 | 0 | B | 2 | 0 | 1 | 0 | 0 | 2 | 1 | 10 | B | N | N | Y | G | Y | 2 | N | Y | N | -1 | 21 | -1 | 25 | -1 | 16 | -1 | -1 | -1 | 16 | -1 | -1 | -1 | -1 | -1 | 8 | N | NJ | 2013 | 11 | 4 | False | False | False | False | False | False | True | True | 11.0 | 0.9919 | 0.0038 | 1.1886 | 1.0 | 1.0 | 1.0 | 1.0 | 2.0 | 1.0 | 1.0 | 1.0 | 24.000001 | 25.0 | 1.0 | 1.0 | 6.0 | 18.0 | 16.0 | 50545.000349 | 2.000000e+00 | 1.0 | 1.0 | 6.0 | 9.0 | 1.0 | 10.0 | 2.0 | 13.0 | 17.0 | 23.0 | 2.0 | 2.0 | 1.0 | 1.0 | 2.0 | 2.0 | 1.0 | 7.0 | 8.0 | -3.456605e-13 | 24.0 | 25.0 | 2.0 | 4.0 | 11.0 | 9.0 | 4.0 | 4.0 | 17.0 | 19.0 | 5.0 | 16.0 | 10.0 | 15.0 | 5.0 | 16.0 | 7.0 | 16.0 | 4.0 | 12.0 | 9.0 | 14.0 | 5.0 | 14.0 | 11.0 | 17.0 | 7.0 | 16.0 | 2.000000 | 9.0 | 13.0 | 19.0 | 18.0 | 20.0 | 22.0 | 16.0 | 19.0 | 18.0 | 17.0 | 14.0 | 13.0 | 7.0 | 10.0 | 19.0 | 18.0 | 8.0 | 13.0 | 11.0 | 16.0 | 12.0 | 22.0 | 4.0 | 4.0 | 6.0 | 10.0 | 6.0 | 7.0 | 10.0 | 12.0 | 12.0 | 20.0 | 17.0 | 23.0 | 6.0 | 15.0 | 12.0 | 20.0 | 9.0 | 18.0 | 10.000000 | 16.0 | 23.0 | 20.0 | 12.0 | 15.0 | 9.0 | 10.0 | 16.0 | 17.0 | 6.0 | 7.0 | 17.0 | 19.0 | 6.0 | 7.0 | 13.0 | 11.0 | 8.0 | 2.0 | 9.0 | 5.0 | 14.0 | 15.0 | 14.0 | 13.0 | 12.0 | 7.0 | 12.0 | 11.0 | 6.0 | 11.0 | 21.0 | 1.0 | 1.0 | 5.0 | 3.0 | 13.0 | 15.0 | 2.0 | 5.0 | 48.000001 | 29.000000 | 332.999998 | 1.385683e+09 | True | . 7 F | 548 | N | 13 | 22 | 13 | 23 | T | E | 1 | 5 | 5 | Q | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | N | 1 | 3 | 0 | 5 | 2 | XM | YV | ZW | XC | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | R | 0 | 0 | 1 | -1 | 21 | 3 | 0 | B | 8 | 0 | 1 | 1 | 0 | 2 | 8 | 4 | B | N | O | Y | H | N | 2 | N | Y | N | -1 | 15 | -1 | 25 | -1 | 14 | -1 | -1 | -1 | 22 | -1 | -1 | -1 | -1 | 25 | 25 | N | NJ | 2014 | 4 | 3 | False | False | False | False | False | False | True | True | 7.0 | 1.0006 | 0.0040 | 1.2433 | 8.0 | 14.0 | 8.0 | 14.0 | 7.0 | 12.0 | 8.0 | 14.0 | 3.000000 | 5.0 | 4.0 | 13.0 | 4.0 | 13.0 | 11.0 | 34038.000009 | -2.965073e-08 | 5.0 | 5.0 | -1.0 | -1.0 | 3.0 | 7.0 | 2.0 | 20.0 | 23.0 | 19.0 | 3.0 | 3.0 | 8.0 | 14.0 | 10.0 | 15.0 | 2.0 | 6.0 | 8.0 | -3.456605e-13 | 12.0 | 13.0 | 2.0 | 1.0 | 16.0 | 16.0 | 8.0 | 8.0 | 7.0 | 4.0 | 5.0 | 16.0 | 8.0 | 10.0 | 5.0 | 15.0 | 8.0 | 16.0 | 5.0 | 16.0 | 8.0 | 10.0 | 5.0 | 16.0 | 9.0 | 8.0 | 5.0 | 10.0 | 2.000000 | 9.0 | 5.0 | 22.0 | 21.0 | 18.0 | 20.0 | 21.0 | 17.0 | 11.0 | 5.0 | 8.0 | 3.0 | 4.0 | 3.0 | 11.0 | 3.0 | 5.0 | 5.0 | 2.0 | 1.0 | 10.0 | 19.0 | 3.0 | 3.0 | 6.0 | 9.0 | 3.0 | 2.0 | 12.0 | 16.0 | 5.0 | 5.0 | 9.0 | 14.0 | 7.0 | 16.0 | 12.0 | 19.0 | 10.0 | 19.0 | 11.000000 | 18.0 | 22.0 | 19.0 | 13.0 | 17.0 | 10.0 | 13.0 | 21.0 | 22.0 | 10.0 | 14.0 | 20.0 | 22.0 | 6.0 | 6.0 | 18.0 | 18.0 | 15.0 | 10.0 | 14.0 | 10.0 | 14.0 | 14.0 | 14.0 | 14.0 | 17.0 | 13.0 | 11.0 | 9.0 | 9.0 | 17.0 | 14.0 | 17.0 | 20.0 | 1.0 | 1.0 | 23.0 | 25.0 | 10.0 | 2.0 | 16.000000 | 17.000000 | 107.000001 | 1.397693e+09 | False | . 8 B | 965 | N | 13 | 22 | 1 | 6 | Y | E | 0 | 4 | 3 | P | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 3 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 3 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 3 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | O | 0 | 0 | 1 | -1 | 21 | 2 | 2 | C | 4 | 1 | 2 | 0 | 0 | 2 | 1 | 10 | B | N | O | Y | H | Y | 2 | N | N | N | -1 | 13 | -1 | 25 | -1 | 9 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 18 | N | CA | 2013 | 3 | 1 | False | False | False | False | False | False | False | True | 19.0 | 0.9403 | 0.0006 | 1.0200 | 7.0 | 12.0 | 7.0 | 12.0 | 10.0 | 18.0 | 7.0 | 12.0 | 8.000000 | 17.0 | 3.0 | 5.0 | 3.0 | 6.0 | 14.0 | 33851.000000 | -2.965073e-08 | 25.0 | 25.0 | 6.0 | 13.0 | 1.0 | 24.0 | 2.0 | 16.0 | 21.0 | 9.0 | 16.0 | 23.0 | 7.0 | 12.0 | 7.0 | 9.0 | 1.0 | 13.0 | 18.0 | -3.456605e-13 | 9.0 | 8.0 | 11.0 | 21.0 | 18.0 | 18.0 | 16.0 | 19.0 | 10.0 | 8.0 | 2.0 | 7.0 | 8.0 | 10.0 | 2.0 | 10.0 | 2.0 | 3.0 | 2.0 | 8.0 | 6.0 | 9.0 | 2.0 | 9.0 | 11.0 | 19.0 | 3.0 | 10.0 | 5.000000 | 20.0 | 20.0 | 5.0 | 9.0 | 8.0 | 18.0 | 4.0 | 4.0 | 13.0 | 8.0 | 14.0 | 11.0 | 11.0 | 16.0 | 14.0 | 9.0 | 5.0 | 6.0 | 17.0 | 23.0 | 7.0 | 10.0 | 16.0 | 23.0 | 10.0 | 16.0 | 17.0 | 23.0 | 7.0 | 5.0 | 25.0 | 25.0 | 15.0 | 22.0 | 8.0 | 18.0 | 13.0 | 20.0 | 2.0 | 2.0 | 3.000000 | 6.0 | 2.0 | 4.0 | 7.0 | 8.0 | 11.0 | 16.0 | 15.0 | 17.0 | 2.0 | 3.0 | 10.0 | 11.0 | 21.0 | 23.0 | 18.0 | 18.0 | 18.0 | 15.0 | 19.0 | 19.0 | 19.0 | 21.0 | 19.0 | 21.0 | 20.0 | 18.0 | 21.0 | 23.0 | 5.0 | 10.0 | 2.0 | 16.0 | 18.0 | 4.0 | 3.0 | 16.0 | 20.0 | 23.0 | 9.0 | 10.000000 | 5.000000 | 64.000001 | 1.362442e+09 | True | . 9 J | 1,113 | N | 1 | 2 | 1 | 6 | T | I | 1 | 5 | 5 | T | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 3 | 0 | 5 | 2 | ZD | XE | ZN | YF | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | N | N | Y | 0 | R | 1 | 0 | 1 | -1 | 21 | 4 | 2 | C | 4 | 0 | 2 | 1 | 0 | 2 | 1 | 6 | B | N | M | Y | F | N | 2 | N | Y | N | 25 | 25 | -1 | 25 | -1 | 22 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 9 | N | TX | 2014 | 9 | 2 | False | False | False | False | False | False | True | True | 26.0 | 0.8928 | 0.0004 | 1.2665 | 5.0 | 6.0 | 5.0 | 6.0 | 4.0 | 5.0 | 4.0 | 6.0 | 5.000000 | 10.0 | 12.0 | 23.0 | 16.0 | 24.0 | 11.0 | 23712.000008 | -2.965073e-08 | 3.0 | 3.0 | -1.0 | -1.0 | 4.0 | 13.0 | 2.0 | 3.0 | 4.0 | 13.0 | 3.0 | 4.0 | 5.0 | 6.0 | 12.0 | 18.0 | 2.0 | 8.0 | 10.0 | -3.456605e-13 | 25.0 | 25.0 | 2.0 | 3.0 | 6.0 | 4.0 | 8.0 | 8.0 | 16.0 | 18.0 | 13.0 | 21.0 | 20.0 | 19.0 | 14.0 | 20.0 | 21.0 | 23.0 | 11.0 | 20.0 | 19.0 | 19.0 | 12.0 | 20.0 | 23.0 | 20.0 | 15.0 | 23.0 | 2.000000 | 6.0 | 5.0 | 10.0 | 12.0 | 8.0 | 18.0 | 10.0 | 19.0 | 20.0 | 21.0 | 13.0 | 11.0 | 10.0 | 15.0 | 15.0 | 9.0 | 5.0 | 5.0 | 8.0 | 11.0 | 3.0 | 2.0 | 11.0 | 17.0 | 5.0 | 7.0 | 6.0 | 7.0 | 2.0 | 2.0 | 7.0 | 12.0 | 2.0 | 2.0 | 5.0 | 11.0 | 2.0 | 1.0 | 5.0 | 9.0 | 1.000000 | 1.0 | 14.0 | 12.0 | 7.0 | 7.0 | 18.0 | 23.0 | 8.0 | 5.0 | 12.0 | 20.0 | 5.0 | 2.0 | 8.0 | 15.0 | 7.0 | 4.0 | 19.0 | 17.0 | 18.0 | 18.0 | 9.0 | 6.0 | 9.0 | 7.0 | 18.0 | 15.0 | 5.0 | 2.0 | 5.0 | 10.0 | 18.0 | 17.0 | 20.0 | 13.0 | 16.0 | 5.0 | 4.0 | 4.0 | 11.0 | 37.000000 | 10.000000 | 253.000002 | 1.410307e+09 | False | . learn = tabular_learner(dls, metrics=roc_auc_binary) . learn.lr_find(start_lr=1e-07, end_lr=1000,suggest_funcs=(valley, slide, minimum, steep)) . SuggestedLRs(valley=tensor(0.0013), slide=tensor(0.0158), minimum=0.05011872053146362, steep=0.06309573352336884) . learn.fit_one_cycle(epochs,lr, wd=wd) . epoch train_loss valid_loss roc_auc_score time . 0 | 0.469378 | 0.328888 | 0.929120 | 00:05 | . 1 | 0.271902 | 0.188843 | 0.957057 | 00:05 | . 2 | 0.211680 | 0.181601 | 0.959578 | 00:05 | . 3 | 0.186571 | 0.176671 | 0.962401 | 00:05 | . 4 | 0.172869 | 0.175805 | 0.962872 | 00:05 | . preds, targs = learn.get_preds() . dl_roc_auc_score=roc_auc_score(to_np(targs), to_np(preds[:,1])) . DL Permutation Importance . class PermutationImportance(): &quot;Calculate and plot the permutation importance&quot; def __init__(self, learn:Learner, df=None, bs=None): &quot;Initialize with a test dataframe, a learner, and a metric&quot; self.learn = learn self.df = df if df is not None else None bs = bs if bs is not None else learn.dls.bs self.dl = learn.dls.test_dl(self.df, bs=bs) if self.df is not None else learn.dls[1] self.x_names = learn.dls.x_names.filter(lambda x: &#39;_na&#39; not in x) self.na = learn.dls.x_names.filter(lambda x: &#39;_na&#39; in x) self.y = dls.y_names self.results = self.calc_feat_importance() self.plot_importance(self.ord_dic_to_df(self.results)) def measure_col(self, name:str): &quot;Measures change after column shuffle&quot; col = [name] if f&#39;{name}_na&#39; in self.na: col.append(name) orig = self.dl.items[col].values perm = np.random.permutation(len(orig)) self.dl.items[col] = self.dl.items[col].values[perm] metric = learn.validate(dl=self.dl)[1] self.dl.items[col] = orig return metric def calc_feat_importance(self): &quot;Calculates permutation importance by shuffling a column on a percentage scale&quot; print(&#39;Getting base error&#39;) base_error = self.learn.validate(dl=self.dl)[1] self.importance = {} pbar = progress_bar(self.x_names) print(&#39;Calculating Permutation Importance&#39;) for col in pbar: self.importance[col] = self.measure_col(col) for key, value in self.importance.items(): self.importance[key] = (base_error-value)/base_error #this can be adjusted return OrderedDict(sorted(self.importance.items(), key=lambda kv: kv[1], reverse=True)) def ord_dic_to_df(self, dict:OrderedDict): return pd.DataFrame([[k, v] for k, v in dict.items()], columns=[&#39;feature&#39;, &#39;importance&#39;]) def plot_importance(self, df:pd.DataFrame, limit=30, asc=False, **kwargs): &quot;Plot importance with an optional limit to how many variables shown&quot; df_copy = df.copy() df_copy[&#39;feature&#39;] = df_copy[&#39;feature&#39;].str.slice(0,25) df_copy = df_copy.sort_values(by=&#39;importance&#39;, ascending=asc)[:limit].sort_values(by=&#39;importance&#39;, ascending=not(asc)) ax = df_copy.plot.barh(x=&#39;feature&#39;, y=&#39;importance&#39;, sort_columns=True, **kwargs) for p in ax.patches: ax.annotate(f&#39;{p.get_width():.4f}&#39;, ((p.get_width() * 1.005), p.get_y() * 1.005)) . imp = PermutationImportance(learn) . Getting base error . Calculating Permutation Importance . . 100.00% [309/309 08:51&lt;00:00] From this most important fields are PropertyField37, PersonalField2, PersonalField1, SalesField5 . Ensembling with Other Models . Adding in XGBoost . import xgboost as xgb . n_estimators = 100 max_depth = 8 learning_rate = 0.1 subsample = 0.5 . X_train, y_train = to.train.xs, to.train.ys.values.ravel() X_valid, y_valid = to.valid.xs, to.valid.ys.values.ravel() . model = xgb.XGBClassifier(n_estimators = n_estimators, max_depth=max_depth, learning_rate=0.1, subsample=subsample, tree_method=&#39;gpu_hist&#39;) . xgb_model = model.fit(X_train, y_train) . xgb_preds = xgb_model.predict_proba(X_valid) . xg_roc_auc_score=roc_auc_score(y_valid, xgb_preds[:,1]) . from xgboost import plot_importance . plot_importance(xgb_model, height=1,max_num_features=20,) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8b4558f150&gt; . From this most important fields were SalesField1A, PersonalField9, Original_Quote_Elapsed, PersonalField10A, PersonalField10B, PropertyField37 . Doing Ensemble . avgs = (preds + xgb_preds) / 2 . dl_roc_auc_score . 0.9628720530182229 . xg_roc_auc_score . 0.9641320936530293 . dlxg_roc_auc_score=roc_auc_score(y_valid, avgs[:,1]) dlxg_roc_auc_score . 0.9649946158658301 . So we have a slightly better performance with ensembling these two . Adding Random Forest . from sklearn.ensemble import RandomForestClassifier . tree = RandomForestClassifier(n_estimators=100) . tree.fit(X_train, y_train) . RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) . !pip install rfpimp from rfpimp import * . impTree = importances(tree, X_valid, to.valid.ys) . plot_importances(impTree) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; So here the most important are PropertyField37, Field7,PersonalField1, SalesField5, PersonalField9,PersonalField2 . forest_preds = tree.predict_proba(X_valid) . rf_roc_auc_score=roc_auc_score(y_valid, forest_preds[:,1]) rf_roc_auc_score . 0.9557626850997376 . new_avgs = (preds + xgb_preds + forest_preds) / 3 . dlxg_roc_auc_score . 0.9649946158658301 . dlxgrf_roc_auc_score=roc_auc_score(y_valid, new_avgs[:,1]) dlxgrf_roc_auc_score . 0.9642387694379251 . So it gets slightly worse when we add Random Forest to the ensemble. . Bayesian Optimisation . There are 4 common methods of hyperparameter optimisation for machine learning: . Manual - Manually test every single hyperparameter combination | Grid Search - Set up a grid of model hyperparameters and run an automatic loop through every single scenario | Random Search - Similar to Grid Search but Random | Bayesian Optimisation | . The biggest difference between Grid Search, Random Search and Bayesian Opt is the formers do not take into account past evaluations while the later does. . There is a whole field of research dedicated to this particular optimisation method. So if you want to understand the mathematical or statistical formulas behind this model, I would suggest to read a book about it. . But simply put, Bayesian Optimisation search builds a probability model of the objective function and use it to select the most promising hyperparameters to evaluate in the true objective function: . Build a surrogate probability model of the objective function | Find the hyperparameters that perform best on the surrogate | Apply these hyperparameters to the true objective function | Update the surrogate model incorporating the new results | Repeat steps 2–4 until max iterations or time is reached | Reference: https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f . You can easilly install a Bayesian Optimisation Python libary . pip install bayesian-optimization . from bayes_opt import BayesianOptimization . Define a objective function to optimise. In this case, we are going to test out learning rate, embedding dropout rate, number of layers and layer sizes. . def fit_with(lr:float, wd:float, dp:float, n_layers:float, layer_1:float, layer_2:float, layer_3:float): print(lr, wd, dp) if round(n_layers) == 2: layers = [round(layer_1), round(layer_2)] elif int(n_layers) == 3: layers = [round(layer_1), round(layer_2), round(layer_3)] else: layers = [round(layer_1)] config = tabular_config(embed_p=float(dp), ps=float(wd)) learn = tabular_learner(dls, layers=layers, metrics=roc_auc_binary, config = config) with learn.no_bar() and learn.no_logging(): learn.fit(5, lr=float(lr)) preds, targs = learn.get_preds() auc_score = roc_auc_score(to_np(targs), to_np(preds[:,1])) return auc_score . We can also specify a range of values for each hyperparameter that we want to tune. . hps = {&#39;lr&#39;: (1e-05, 1e-01), &#39;wd&#39;: (4e-4, 0.4), &#39;dp&#39;: (0.01, 0.5), &#39;n_layers&#39;: (1,3), &#39;layer_1&#39;: (50, 200), &#39;layer_2&#39;: (100, 1000), &#39;layer_3&#39;: (200, 2000)} . optim = BayesianOptimization( f = fit_with, # our fit function pbounds = hps, # our hyper parameters to tune verbose = 2, # 1 prints out when a maximum is observed, 0 for silent, 2 prints out everything random_state=1 ) . %time optim.maximize(n_iter=10) . | iter | target | dp | layer_1 | layer_2 | layer_3 | lr | n_layers | wd | - 0.014684121522803134 0.07482958046651729 0.21434078230426126 . | 1 | 0.9603 | 0.2143 | 158.0 | 100.1 | 744.2 | 0.01468 | 1.185 | 0.07483 | 0.06852509784467198 0.3512957275818218 0.1793247562510934 . | 2 | 0.9602 | 0.1793 | 109.5 | 584.9 | 954.6 | 0.06853 | 1.409 | 0.3513 | 0.014047289990137426 0.32037752964274446 0.02341992066698382 . | 3 | 0.9611 | 0.02342 | 150.6 | 475.6 | 1.206e+0 | 0.01405 | 1.396 | 0.3204 | 0.0894617202837497 0.016006291379859792 0.4844481721025048 . | 4 | 0.9574 | 0.4844 | 97.01 | 723.1 | 1.778e+0 | 0.08946 | 1.17 | 0.01601 | 0.0957893741197487 0.27687409473460917 0.09321690558663875 . | 5 | 0.9599 | 0.09322 | 181.7 | 188.5 | 958.0 | 0.09579 | 2.066 | 0.2769 | 0.010278165724320144 0.09525641811550664 0.039180893394315415 . | 6 | 0.9612 | 0.03918 | 169.8 | 995.0 | 206.4 | 0.01028 | 1.351 | 0.09526 | 0.07156047620301159 0.271888626159136 0.3620644418259379 . | 7 | 0.9588 | 0.3621 | 50.52 | 104.8 | 244.5 | 0.07156 | 1.178 | 0.2719 | 0.057910440131609765 0.16129406478624328 0.4828208474809104 . | 8 | 0.9592 | 0.4828 | 59.94 | 147.0 | 1.301e+0 | 0.05791 | 2.722 | 0.1613 | 0.02154668435098685 0.2935334962486139 0.17485814792145116 . | 9 | 0.962 | 0.1749 | 196.0 | 957.2 | 214.1 | 0.02155 | 1.024 | 0.2935 | 0.03610993906058106 0.3546354028405215 0.12958547535553333 . | 10 | 0.9614 | 0.1296 | 197.9 | 133.6 | 209.7 | 0.03611 | 2.237 | 0.3546 | 0.07160071668964274 0.11955317037046867 0.30174764680544724 . | 11 | 0.9594 | 0.3017 | 196.8 | 972.1 | 263.6 | 0.0716 | 1.003 | 0.1196 | 0.0758754095037137 0.26350352166402036 0.10013698396219092 . | 12 | 0.9595 | 0.1001 | 195.5 | 166.4 | 236.0 | 0.07588 | 1.276 | 0.2635 | 0.08706685388671734 0.2815475542319707 0.0263005243239511 . | 13 | 0.9594 | 0.0263 | 199.1 | 990.5 | 1.991e+0 | 0.08707 | 2.941 | 0.2815 | 0.05628262681087503 0.1085257299138718 0.26772478114338355 . | 14 | 0.9609 | 0.2677 | 152.2 | 105.5 | 755.5 | 0.05628 | 2.52 | 0.1085 | 0.0808374555333798 0.18564863221735886 0.2417705717996151 . | 15 | 0.9596 | 0.2418 | 199.4 | 125.6 | 235.7 | 0.08084 | 1.814 | 0.1856 | ============================================================================================================= CPU times: user 7min 40s, sys: 8.93 s, total: 7min 49s Wall time: 7min 40s . Print out the set of hyperparameters that produce the best accuracy score and use them to train our deep learning model again . print(optim.max) . {&#39;target&#39;: 0.9619591011409044, &#39;params&#39;: {&#39;dp&#39;: 0.17485814792145116, &#39;layer_1&#39;: 196.0396513158857, &#39;layer_2&#39;: 957.170989722186, &#39;layer_3&#39;: 214.1299530803173, &#39;lr&#39;: 0.02154668435098685, &#39;n_layers&#39;: 1.023739458890539, &#39;wd&#39;: 0.2935334962486139}} . random_seed = 42 bs = 4096 val_bs = 512 test_size = 0.3 epochs = 5 lr = 0.02 #0.03 wd=0.3 #0.32 layers = [200] #200,150[50,1000] dropout = [0.001, 0.01] y_block=CategoryBlock() emb_dropout=0.17 #0.13 #0.2 set_seed(42) roc_auc_binary = RocAucBinary() . dls = to.dataloaders(bs=bs, val_bs=val_bs, layers=layers, embed_ps=emb_dropout, ps=dropout) . learn = tabular_learner(dls, metrics=roc_auc_binary) . learn.fit_one_cycle(6,lr, wd=wd) . epoch train_loss valid_loss roc_auc_score time . 0 | 0.436900 | 0.235703 | 0.947294 | 00:05 | . 1 | 0.260621 | 0.191002 | 0.959327 | 00:05 | . 2 | 0.210429 | 0.185707 | 0.959293 | 00:05 | . 3 | 0.187610 | 0.176735 | 0.962208 | 00:05 | . 4 | 0.174427 | 0.174023 | 0.963673 | 00:05 | . 5 | 0.164969 | 0.174370 | 0.963667 | 00:05 | . preds, targs = learn.get_preds() . dl_opt_roc_au_score = roc_auc_score(to_np(targs), to_np(preds[:,1])) dl_roc_auc_score, dl_opt_roc_au_score . (0.9628720530182229, 0.963667492106761) . The final roc_au_score increases from 0.962 to 0.963, which is quite significant for this particular dataset. Because if you have a look at the Kaggle Leaderboard, the accuracy score starts at around 0.95 and the highest score is around 0.97 . Entity Embeddings . The most common variable types in structured data are: . continuous variables | categorical variables | To use cat variables in our model, we need to represent them as integers. . Sometimes, there are intrinsic ordering in those numbers (for example: low - 1, medium - 2, high - 3) and they are called ordinal numbers. . But more than often, those integers are random and don’t provide any information and they are called nominal numbers. For example, sex or states (we cannot really rank states). . Our model will perform better if those integers contain some relevant information of that paticular cat variable. . One way to deal with this problem is to use one hot end coding. But the issue with one hot end coding is that it is computationally expensive. . What entity embeddings can do is putting similar values of a cat variable closer together in the embedding space. You can first train a neural network with cat embeddings and then use those cat embeddings instead of raw categorical columns in the models. . learn.model.embeds . ModuleList( (0): Embedding(9, 5) (1): Embedding(9, 5) (2): Embedding(3, 3) (3): Embedding(4, 3) (4): Embedding(5, 4) (5): Embedding(4, 3) (6): Embedding(5, 4) (7): Embedding(8, 5) (8): Embedding(13, 7) (9): Embedding(3, 3) (10): Embedding(6, 4) (11): Embedding(6, 4) (12): Embedding(8, 5) (13): Embedding(3, 3) (14): Embedding(18, 8) (15): Embedding(19, 8) (16): Embedding(9, 5) (17): Embedding(11, 6) (18): Embedding(12, 6) (19): Embedding(3, 3) (20): Embedding(3, 3) (21): Embedding(10, 6) (22): Embedding(3, 3) (23): Embedding(3, 3) (24): Embedding(4, 3) (25): Embedding(4, 3) (26): Embedding(6, 4) (27): Embedding(6, 4) (28): Embedding(5, 4) (29): Embedding(51, 14) (30): Embedding(67, 17) (31): Embedding(62, 16) (32): Embedding(58, 16) (33): Embedding(8, 5) (34): Embedding(13, 7) (35): Embedding(13, 7) (36): Embedding(13, 7) (37): Embedding(14, 7) (38): Embedding(17, 8) (39): Embedding(8, 5) (40): Embedding(8, 5) (41): Embedding(11, 6) (42): Embedding(12, 6) (43): Embedding(12, 6) (44): Embedding(12, 6) (45): Embedding(6, 4) (46): Embedding(5, 4) (47): Embedding(6, 4) (48): Embedding(6, 4) (49): Embedding(6, 4) (50): Embedding(10, 6) (51): Embedding(10, 6) (52): Embedding(10, 6) (53): Embedding(10, 6) (54): Embedding(8, 5) (55): Embedding(11, 6) (56): Embedding(12, 6) (57): Embedding(12, 6) (58): Embedding(12, 6) (59): Embedding(8, 5) (60): Embedding(8, 5) (61): Embedding(8, 5) (62): Embedding(8, 5) (63): Embedding(8, 5) (64): Embedding(8, 5) (65): Embedding(12, 6) (66): Embedding(12, 6) (67): Embedding(12, 6) (68): Embedding(13, 7) (69): Embedding(8, 5) (70): Embedding(8, 5) (71): Embedding(8, 5) (72): Embedding(8, 5) (73): Embedding(8, 5) (74): Embedding(8, 5) (75): Embedding(4, 3) (76): Embedding(4, 3) (77): Embedding(4, 3) (78): Embedding(4, 3) (79): Embedding(5, 4) (80): Embedding(6, 4) (81): Embedding(7, 5) (82): Embedding(7, 5) (83): Embedding(7, 5) (84): Embedding(7, 5) (85): Embedding(9, 5) (86): Embedding(10, 6) (87): Embedding(11, 6) (88): Embedding(12, 6) (89): Embedding(8, 5) (90): Embedding(13, 7) (91): Embedding(13, 7) (92): Embedding(13, 7) (93): Embedding(15, 7) (94): Embedding(8, 5) (95): Embedding(3, 3) (96): Embedding(3, 3) (97): Embedding(3, 3) (98): Embedding(3, 3) (99): Embedding(2, 2) (100): Embedding(20, 9) (101): Embedding(3, 3) (102): Embedding(4, 3) (103): Embedding(6, 4) (104): Embedding(3, 3) (105): Embedding(6, 4) (106): Embedding(8, 5) (107): Embedding(5, 4) (108): Embedding(5, 4) (109): Embedding(14, 7) (110): Embedding(8, 5) (111): Embedding(11, 6) (112): Embedding(11, 6) (113): Embedding(4, 3) (114): Embedding(6, 4) (115): Embedding(14, 7) (116): Embedding(18, 8) (117): Embedding(5, 4) (118): Embedding(3, 3) (119): Embedding(5, 4) (120): Embedding(3, 3) (121): Embedding(5, 4) (122): Embedding(3, 3) (123): Embedding(4, 3) (124): Embedding(3, 3) (125): Embedding(3, 3) (126): Embedding(3, 3) (127): Embedding(3, 3) (128): Embedding(15, 7) (129): Embedding(2, 2) (130): Embedding(3, 3) (131): Embedding(3, 3) (132): Embedding(21, 9) (133): Embedding(3, 3) (134): Embedding(3, 3) (135): Embedding(3, 3) (136): Embedding(13, 7) (137): Embedding(3, 3) (138): Embedding(3, 3) (139): Embedding(3, 3) (140): Embedding(3, 3) (141): Embedding(3, 3) (142): Embedding(20, 9) (143): Embedding(4, 3) (144): Embedding(5, 4) (145): Embedding(4, 3) (146): Embedding(13, 7) (147): Embedding(8, 5) (148): Embedding(3, 3) (149): Embedding(3, 3) (150): Embedding(3, 3) (151): Embedding(3, 3) (152): Embedding(3, 3) (153): Embedding(3, 3) (154): Embedding(3, 3) (155): Embedding(3, 3) ) . Function to embed features, obtained from Fastai forums . def embed_features(learner, xs): xs = xs.copy() for i, feature in enumerate(learner.dls.cat_names): emb = learner.model.embeds[i] new_feat = pd.DataFrame(emb(tensor(xs[feature], dtype=torch.int32).to(&#39;cuda:0&#39;)), index=xs.index, columns=[f&#39;{feature}_{j}&#39; for j in range(emb.embedding_dim)]) xs.drop(columns=feature, inplace=True) xs = xs.join(new_feat) return xs . Run this function for both your training and valid sets . emb_xs = embed_features(learn, to.train.xs) . emb_valid_xs = embed_features(learn, to.valid.xs) . Use the embedded layers to train your gradient boosting model . model = xgb.XGBClassifier(n_estimators = n_estimators, max_depth=max_depth, learning_rate=0.1, subsample=subsample, tree_method=&#39;gpu_hist&#39;) . xgb_model_emb=model.fit(emb_xs, y_train) . xgb_emb_preds = xgb_model_emb.predict_proba(emb_valid_xs) . xg_emb_roc_auc_score=roc_auc_score(y_valid, xgb_emb_preds[:,1]) xg_roc_auc_score, xg_emb_roc_auc_score . (0.9641320936530293, 0.9640538678190417) . In this problem, it doesn&#39;t seem to change the accuracy score. However, you might be more sucessful with other datasets. .",
            "url": "https://redditech.github.io/team-fast-tabulous/kaggle/fastai/2021/07/08/HomeSite-Quote-A-Fastai-Tabular-Approach.html",
            "relUrl": "/kaggle/fastai/2021/07/08/HomeSite-Quote-A-Fastai-Tabular-Approach.html",
            "date": " • Jul 8, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Fastai EDA AND Xgboost Optuna Hyperparameters Tuninhg",
            "content": "Import Libraries . !pip install optuna . import pandas as pd import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.model_selection import KFold from sklearn.model_selection import StratifiedKFold from sklearn.model_selection import RepeatedKFold from sklearn.metrics import accuracy_score from sklearn.metrics import mean_squared_error from sklearn.metrics import plot_roc_curve from sklearn.metrics import plot_confusion_matrix from sklearn.metrics import roc_auc_score from sklearn import preprocessing from sklearn import model_selection import sklearn.datasets import xgboost as xgb from xgboost import XGBClassifier import optuna import matplotlib.pyplot as plt from fastbook import * from fastai.tabular.all import * from dtreeviz.trees import * from IPython.display import Image, display_svg, SVG import random as rd pd.options.display.max_rows = 20 pd.options.display.max_columns = 8 . Download Data . !mkdir -p ~/.kaggle !cp /content/gdrive/MyDrive/Kaggle/kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . path = Path(&#39;/content/gdrive/MyDrive/Kaggle/&#39; + &#39;data/homesite-quote&#39;) path.mkdir(parents=True, exist_ok=True) path . Path(&#39;/content/gdrive/MyDrive/Kaggle/data/homesite-quote&#39;) . !kaggle competitions download -c homesite-quote-conversion -p /content/gdrive/MyDrive/Kaggle/data/homesite-quote . ! unzip -q -n &#39;{path}/train.csv.zip&#39; -d &#39;{path}&#39; ! unzip -q -n &#39;{path}/test.csv.zip&#39; -d &#39;{path}&#39; ! unzip -q -n &#39;{path}/sample_submission.csv.zip&#39; -d &#39;{path}&#39; . df = pd.read_csv(path/&#39;train.csv&#39;, low_memory=False) test_df = pd.read_csv(path/&#39;test.csv&#39;, low_memory=False) . EDA with Fastai . dep_var=&#39;QuoteConversion_Flag&#39; . As &#39;QuoteNumber&#39; is unique, set it as index . df_train = df.set_index(&#39;QuoteNumber&#39;) df_test = test_df.set_index(&#39;QuoteNumber&#39;) . Use Fastai function to add relevant datetime fields . df_train[&#39;Original_Quote_Date&#39;] = pd.to_datetime(df_train[&#39;Original_Quote_Date&#39;]) df_test[&#39;Original_Quote_Date&#39;] = pd.to_datetime(df_test[&#39;Original_Quote_Date&#39;]) df_train = add_datepart(df_train, &#39;Original_Quote_Date&#39;) df_test = add_datepart(df_test, &#39;Original_Quote_Date&#39;) . Drop 2 below fields because they have constant values . df_train.drop(columns=[&#39;PropertyField6&#39;,&#39;GeographicField10A&#39;],axis=1,inplace=True) df_test.drop(columns=[&#39;PropertyField6&#39;,&#39;GeographicField10A&#39;],axis=1,inplace=True) . Use Fastai function to identify continuous and categorical variables . cont_names, cat_names = cont_cat_split(df_train,dep_var=dep_var) len(cont_names), len(cat_names) . (155, 152) . &#39;procs&#39; will take care of of categorifying categorical variables, fill in missing values and normalise data . procs = [Categorify, FillMissing, Normalize] splits = TrainTestSplitter(test_size=0.2, stratify=df_train[dep_var])(df_train) . Create a TabularPandas dataset . to = TabularPandas(df=df_train, procs=procs, cat_names=cat_names, cont_names=cont_names, y_names=dep_var,splits=splits, y_block=CategoryBlock()) . dls = to.dataloaders(bs=4096, val_bs=512, layers=[10000,500], embed_ps=0.02, ps=[0.001, 0.01]) . XGBoost with Otuna . Use Optuna to select best hyperparamters for XGboost model. Code is referenced from https://www.kaggle.com/hamzaghanmi/xgboost-hyperparameter-tuning-using-optuna . X_train_fa, y_train_fa = to.train.xs, to.train.ys.values.ravel() X_valid_fa, y_valid_fa = to.valid.xs, to.valid.ys.values.ravel() . Define parameter to test . def objective(trial): X_train_fa, y_train_fa = to.train.xs, to.train.ys.values.ravel() X_valid_fa, y_valid_fa = to.valid.xs, to.valid.ys.values.ravel() param = { &#39;tree_method&#39;:&#39;gpu_hist&#39;, # this parameter means using the GPU when training our model to speedup the training process &#39;lambda&#39;: trial.suggest_loguniform(&#39;lambda&#39;, 1e-3, 10.0), &#39;alpha&#39;: trial.suggest_loguniform(&#39;alpha&#39;, 1e-3, 10.0), &#39;colsample_bytree&#39;: trial.suggest_categorical(&#39;colsample_bytree&#39;, [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]), &#39;subsample&#39;: trial.suggest_categorical(&#39;subsample&#39;, [0.4,0.5,0.6,0.7,0.8,1.0]), &#39;learning_rate&#39;: trial.suggest_categorical(&#39;learning_rate&#39;, [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]), &#39;n_estimators&#39;: 1000, &#39;max_depth&#39;: trial.suggest_categorical(&#39;max_depth&#39;, [5,7,9,11,13,15,17,20]), &#39;random_state&#39;: trial.suggest_categorical(&#39;random_state&#39;, [24, 48,2020]), &#39;min_child_weight&#39;: trial.suggest_int(&#39;min_child_weight&#39;, 1, 300), } model = xgb.XGBClassifier(**param) model.fit(X_train_fa,y_train_fa,eval_set=[(X_valid_fa,y_valid_fa)],early_stopping_rounds=100,verbose=False) preds = model.predict_proba(X_valid_fa)[:,1] auc = roc_auc_score(y_valid_fa, preds) return auc . Fit model using Optuna . study = optuna.create_study(direction=&#39;maximize&#39;) study.optimize(objective, n_trials=50) . [I 2021-07-03 13:18:30,783] A new study created in memory with name: no-name-fe83554f-1bcf-4f1e-976a-1d2d12ba22da [I 2021-07-03 13:18:54,840] Trial 0 finished with value: 0.9614490168531203 and parameters: {&#39;lambda&#39;: 0.6618680618471974, &#39;alpha&#39;: 0.04239837985417904, &#39;colsample_bytree&#39;: 1.0, &#39;subsample&#39;: 0.5, &#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 7, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 253}. Best is trial 0 with value: 0.9614490168531203. [I 2021-07-03 13:19:22,792] Trial 1 finished with value: 0.96370909267955 and parameters: {&#39;lambda&#39;: 8.086230640660201, &#39;alpha&#39;: 0.004463392998945948, &#39;colsample_bytree&#39;: 0.9, &#39;subsample&#39;: 0.4, &#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 11, &#39;random_state&#39;: 24, &#39;min_child_weight&#39;: 81}. Best is trial 1 with value: 0.96370909267955. [I 2021-07-03 13:20:05,297] Trial 2 finished with value: 0.9663443798690221 and parameters: {&#39;lambda&#39;: 0.06952864689008562, &#39;alpha&#39;: 0.03710247783593982, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.014, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 44}. Best is trial 2 with value: 0.9663443798690221. [I 2021-07-03 13:20:33,443] Trial 3 finished with value: 0.9659511091468089 and parameters: {&#39;lambda&#39;: 0.1750300039236326, &#39;alpha&#39;: 0.16170199753683476, &#39;colsample_bytree&#39;: 0.6, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.016, &#39;max_depth&#39;: 13, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 66}. Best is trial 2 with value: 0.9663443798690221. [I 2021-07-03 13:21:05,326] Trial 4 finished with value: 0.9659325248764232 and parameters: {&#39;lambda&#39;: 0.01680518019972616, &#39;alpha&#39;: 0.00967393286707375, &#39;colsample_bytree&#39;: 0.8, &#39;subsample&#39;: 0.7, &#39;learning_rate&#39;: 0.018, &#39;max_depth&#39;: 17, &#39;random_state&#39;: 2020, &#39;min_child_weight&#39;: 30}. Best is trial 2 with value: 0.9663443798690221. [I 2021-07-03 13:21:29,690] Trial 5 finished with value: 0.9655972055590062 and parameters: {&#39;lambda&#39;: 0.020530533816826284, &#39;alpha&#39;: 1.1760096602282823, &#39;colsample_bytree&#39;: 1.0, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.02, &#39;max_depth&#39;: 7, &#39;random_state&#39;: 2020, &#39;min_child_weight&#39;: 44}. Best is trial 2 with value: 0.9663443798690221. [I 2021-07-03 13:21:35,195] Trial 6 finished with value: 0.9349120024842033 and parameters: {&#39;lambda&#39;: 0.42768461846209, &#39;alpha&#39;: 1.353454042817887, &#39;colsample_bytree&#39;: 0.9, &#39;subsample&#39;: 0.6, &#39;learning_rate&#39;: 0.009, &#39;max_depth&#39;: 5, &#39;random_state&#39;: 2020, &#39;min_child_weight&#39;: 78}. Best is trial 2 with value: 0.9663443798690221. [I 2021-07-03 13:21:40,078] Trial 7 finished with value: 0.9430147359254457 and parameters: {&#39;lambda&#39;: 2.5542913283137563, &#39;alpha&#39;: 1.013512477448444, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 0.7, &#39;learning_rate&#39;: 0.016, &#39;max_depth&#39;: 5, &#39;random_state&#39;: 2020, &#39;min_child_weight&#39;: 208}. Best is trial 2 with value: 0.9663443798690221. [I 2021-07-03 13:22:17,721] Trial 8 finished with value: 0.9633637626415875 and parameters: {&#39;lambda&#39;: 0.018688998050368725, &#39;alpha&#39;: 0.17392736957556704, &#39;colsample_bytree&#39;: 0.6, &#39;subsample&#39;: 0.5, &#39;learning_rate&#39;: 0.014, &#39;max_depth&#39;: 20, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 220}. Best is trial 2 with value: 0.9663443798690221. [I 2021-07-03 13:23:01,548] Trial 9 finished with value: 0.9652223224163096 and parameters: {&#39;lambda&#39;: 0.02137093874894116, &#39;alpha&#39;: 0.15451044816007484, &#39;colsample_bytree&#39;: 0.3, &#39;subsample&#39;: 0.6, &#39;learning_rate&#39;: 0.016, &#39;max_depth&#39;: 20, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 117}. Best is trial 2 with value: 0.9663443798690221. [I 2021-07-03 13:24:15,020] Trial 10 finished with value: 0.9661774617228225 and parameters: {&#39;lambda&#39;: 0.001620082617449181, &#39;alpha&#39;: 8.552250413676935, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 0.8, &#39;learning_rate&#39;: 0.012, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 24, &#39;min_child_weight&#39;: 7}. Best is trial 2 with value: 0.9663443798690221. [I 2021-07-03 13:25:49,103] Trial 11 finished with value: 0.9660047912737255 and parameters: {&#39;lambda&#39;: 0.0014240237450165209, &#39;alpha&#39;: 8.559648869065919, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 0.8, &#39;learning_rate&#39;: 0.012, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 24, &#39;min_child_weight&#39;: 4}. Best is trial 2 with value: 0.9663443798690221. [I 2021-07-03 13:26:50,907] Trial 12 finished with value: 0.9654791512167799 and parameters: {&#39;lambda&#39;: 0.002057460573404571, &#39;alpha&#39;: 0.0010463310681730537, &#39;colsample_bytree&#39;: 0.5, &#39;subsample&#39;: 0.8, &#39;learning_rate&#39;: 0.008, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 24, &#39;min_child_weight&#39;: 6}. Best is trial 2 with value: 0.9663443798690221. [I 2021-07-03 13:27:19,201] Trial 13 finished with value: 0.9646930333214024 and parameters: {&#39;lambda&#39;: 0.0038974148954420747, &#39;alpha&#39;: 0.02630961934977218, &#39;colsample_bytree&#39;: 0.7, &#39;subsample&#39;: 0.8, &#39;learning_rate&#39;: 0.012, &#39;max_depth&#39;: 9, &#39;random_state&#39;: 24, &#39;min_child_weight&#39;: 144}. Best is trial 2 with value: 0.9663443798690221. [I 2021-07-03 13:28:28,550] Trial 14 finished with value: 0.9657240892698717 and parameters: {&#39;lambda&#39;: 0.07619038564231667, &#39;alpha&#39;: 8.897569356075648, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.014, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 4}. Best is trial 2 with value: 0.9663443798690221. [I 2021-07-03 13:29:00,356] Trial 15 finished with value: 0.9641639083366684 and parameters: {&#39;lambda&#39;: 0.005467248640780835, &#39;alpha&#39;: 0.002206643068223733, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 0.4, &#39;learning_rate&#39;: 0.014, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 24, &#39;min_child_weight&#39;: 116}. Best is trial 2 with value: 0.9663443798690221. [I 2021-07-03 13:29:49,551] Trial 16 finished with value: 0.9664961781588531 and parameters: {&#39;lambda&#39;: 0.08287684030183871, &#39;alpha&#39;: 0.021800136799959794, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.012, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 44}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:30:19,110] Trial 17 finished with value: 0.9656477756261004 and parameters: {&#39;lambda&#39;: 0.0736986353742997, &#39;alpha&#39;: 0.017914630395936607, &#39;colsample_bytree&#39;: 0.7, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.018, &#39;max_depth&#39;: 11, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 166}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:30:58,831] Trial 18 finished with value: 0.9661479134448582 and parameters: {&#39;lambda&#39;: 0.20528790013698178, &#39;alpha&#39;: 0.05696439914463783, &#39;colsample_bytree&#39;: 0.3, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.02, &#39;max_depth&#39;: 17, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 49}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:31:35,970] Trial 19 finished with value: 0.9654636560790603 and parameters: {&#39;lambda&#39;: 1.8293301690601609, &#39;alpha&#39;: 0.0079463512796583, &#39;colsample_bytree&#39;: 0.5, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.009, &#39;max_depth&#39;: 13, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 109}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:32:03,872] Trial 20 finished with value: 0.9634307014917336 and parameters: {&#39;lambda&#39;: 0.05935031259030531, &#39;alpha&#39;: 0.37572455565093704, &#39;colsample_bytree&#39;: 0.8, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.008, &#39;max_depth&#39;: 9, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 180}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:32:58,662] Trial 21 finished with value: 0.966360010156296 and parameters: {&#39;lambda&#39;: 0.007760285328621599, &#39;alpha&#39;: 0.053988600576792355, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 0.8, &#39;learning_rate&#39;: 0.012, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 24, &#39;min_child_weight&#39;: 24}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:33:43,499] Trial 22 finished with value: 0.9661228311356425 and parameters: {&#39;lambda&#39;: 0.006653218220222155, &#39;alpha&#39;: 0.06371101614572536, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 0.8, &#39;learning_rate&#39;: 0.012, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 28}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:34:23,905] Trial 23 finished with value: 0.9658870820455391 and parameters: {&#39;lambda&#39;: 0.03774935662196809, &#39;alpha&#39;: 0.019484399312365754, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.012, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 24, &#39;min_child_weight&#39;: 92}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:34:49,917] Trial 24 finished with value: 0.960692411034934 and parameters: {&#39;lambda&#39;: 0.22050638749514634, &#39;alpha&#39;: 0.3497176109339869, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 0.4, &#39;learning_rate&#39;: 0.012, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 298}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:35:33,230] Trial 25 finished with value: 0.9661802371154521 and parameters: {&#39;lambda&#39;: 0.008553754604813191, &#39;alpha&#39;: 0.010383443637253126, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 0.8, &#39;learning_rate&#39;: 0.014, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 56}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:36:21,724] Trial 26 finished with value: 0.9663990659640549 and parameters: {&#39;lambda&#39;: 0.03785620605363892, &#39;alpha&#39;: 0.00366817865425139, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.012, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 24, &#39;min_child_weight&#39;: 28}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:37:09,025] Trial 27 finished with value: 0.9659814067807833 and parameters: {&#39;lambda&#39;: 0.011887161277513221, &#39;alpha&#39;: 0.0022334644633789213, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 0.5, &#39;learning_rate&#39;: 0.012, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 24, &#39;min_child_weight&#39;: 25}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:37:39,925] Trial 28 finished with value: 0.9657993844652173 and parameters: {&#39;lambda&#39;: 0.03269086093271246, &#39;alpha&#39;: 0.002591116596186556, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 0.6, &#39;learning_rate&#39;: 0.012, &#39;max_depth&#39;: 9, &#39;random_state&#39;: 24, &#39;min_child_weight&#39;: 23}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:38:06,678] Trial 29 finished with value: 0.9632682107001242 and parameters: {&#39;lambda&#39;: 0.0028161256757211007, &#39;alpha&#39;: 0.0010181643728821939, &#39;colsample_bytree&#39;: 1.0, &#39;subsample&#39;: 0.7, &#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 7, &#39;random_state&#39;: 24, &#39;min_child_weight&#39;: 99}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:38:29,798] Trial 30 finished with value: 0.9638549962767746 and parameters: {&#39;lambda&#39;: 0.5323894003316153, &#39;alpha&#39;: 0.005644746026283066, &#39;colsample_bytree&#39;: 0.9, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.012, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 24, &#39;min_child_weight&#39;: 129}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:39:10,746] Trial 31 finished with value: 0.9662165573514324 and parameters: {&#39;lambda&#39;: 0.1265430806657861, &#39;alpha&#39;: 0.03607744523557369, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.014, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 24, &#39;min_child_weight&#39;: 67}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:39:41,841] Trial 32 finished with value: 0.965805495155772 and parameters: {&#39;lambda&#39;: 0.03962702582341804, &#39;alpha&#39;: 0.06657948718332439, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.012, &#39;max_depth&#39;: 11, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 42}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:40:22,974] Trial 33 finished with value: 0.9658125989542109 and parameters: {&#39;lambda&#39;: 0.3263042905584334, &#39;alpha&#39;: 0.014621966829258828, &#39;colsample_bytree&#39;: 0.6, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 24, &#39;min_child_weight&#39;: 76}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:40:59,992] Trial 34 finished with value: 0.9660552117109559 and parameters: {&#39;lambda&#39;: 0.12103628446814962, &#39;alpha&#39;: 0.0356361328795397, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.012, &#39;max_depth&#39;: 13, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 58}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:41:22,886] Trial 35 finished with value: 0.9652533525125997 and parameters: {&#39;lambda&#39;: 0.05215091371717967, &#39;alpha&#39;: 0.11395752205090479, &#39;colsample_bytree&#39;: 0.8, &#39;subsample&#39;: 0.5, &#39;learning_rate&#39;: 0.018, &#39;max_depth&#39;: 17, &#39;random_state&#39;: 24, &#39;min_child_weight&#39;: 37}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:41:46,007] Trial 36 finished with value: 0.9653301078058069 and parameters: {&#39;lambda&#39;: 0.8300674259125883, &#39;alpha&#39;: 0.0038249812316278288, &#39;colsample_bytree&#39;: 1.0, &#39;subsample&#39;: 0.4, &#39;learning_rate&#39;: 0.02, &#39;max_depth&#39;: 7, &#39;random_state&#39;: 2020, &#39;min_child_weight&#39;: 16}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:41:50,974] Trial 37 finished with value: 0.9411099055288206 and parameters: {&#39;lambda&#39;: 0.010576441432516159, &#39;alpha&#39;: 0.2553780859410673, &#39;colsample_bytree&#39;: 0.9, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.009, &#39;max_depth&#39;: 5, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 81}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:42:35,777] Trial 38 finished with value: 0.9662987439673464 and parameters: {&#39;lambda&#39;: 0.023907742250035224, &#39;alpha&#39;: 0.0964642360899176, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 0.7, &#39;learning_rate&#39;: 0.014, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 2020, &#39;min_child_weight&#39;: 41}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:43:12,544] Trial 39 finished with value: 0.965898889531139 and parameters: {&#39;lambda&#39;: 0.013747145139813862, &#39;alpha&#39;: 0.01254077864827821, &#39;colsample_bytree&#39;: 0.3, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.016, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 65}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:43:43,219] Trial 40 finished with value: 0.9651608836703399 and parameters: {&#39;lambda&#39;: 0.12057384146600342, &#39;alpha&#39;: 0.006354843087402957, &#39;colsample_bytree&#39;: 0.6, &#39;subsample&#39;: 0.6, &#39;learning_rate&#39;: 0.012, &#39;max_depth&#39;: 11, &#39;random_state&#39;: 24, &#39;min_child_weight&#39;: 93}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:44:20,814] Trial 41 finished with value: 0.9658953671958843 and parameters: {&#39;lambda&#39;: 0.023312326888009435, &#39;alpha&#39;: 0.09590285438044613, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 0.7, &#39;learning_rate&#39;: 0.014, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 2020, &#39;min_child_weight&#39;: 42}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:44:34,794] Trial 42 finished with value: 0.9581546016680716 and parameters: {&#39;lambda&#39;: 0.03005460208149567, &#39;alpha&#39;: 0.025891473088036694, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 0.7, &#39;learning_rate&#39;: 0.014, &#39;max_depth&#39;: 20, &#39;random_state&#39;: 2020, &#39;min_child_weight&#39;: 19}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:45:16,910] Trial 43 finished with value: 0.9660153884468001 and parameters: {&#39;lambda&#39;: 0.017027501529365368, &#39;alpha&#39;: 0.0926033448201442, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 0.7, &#39;learning_rate&#39;: 0.014, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 2020, &#39;min_child_weight&#39;: 38}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:45:29,673] Trial 44 finished with value: 0.9582815119261712 and parameters: {&#39;lambda&#39;: 0.0498652563995772, &#39;alpha&#39;: 0.7044842190007566, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 0.7, &#39;learning_rate&#39;: 0.008, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 2020, &#39;min_child_weight&#39;: 14}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:45:51,223] Trial 45 finished with value: 0.9640948903554354 and parameters: {&#39;lambda&#39;: 0.09962512320523084, &#39;alpha&#39;: 0.04598224416610351, &#39;colsample_bytree&#39;: 0.5, &#39;subsample&#39;: 0.8, &#39;learning_rate&#39;: 0.014, &#39;max_depth&#39;: 5, &#39;random_state&#39;: 2020, &#39;min_child_weight&#39;: 1}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:46:26,321] Trial 46 finished with value: 0.9657404290923046 and parameters: {&#39;lambda&#39;: 0.02585297640370347, &#39;alpha&#39;: 0.16959923307006608, &#39;colsample_bytree&#39;: 0.7, &#39;subsample&#39;: 0.7, &#39;learning_rate&#39;: 0.012, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 2020, &#39;min_child_weight&#39;: 51}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:47:01,541] Trial 47 finished with value: 0.9660348729097518 and parameters: {&#39;lambda&#39;: 0.004220389968619446, &#39;alpha&#39;: 0.028392015763314685, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 0.8, &#39;learning_rate&#39;: 0.016, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 24, &#39;min_child_weight&#39;: 67}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:47:58,615] Trial 48 finished with value: 0.9660819761494438 and parameters: {&#39;lambda&#39;: 0.08026597900346424, &#39;alpha&#39;: 0.11850315623816536, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 0.5, &#39;learning_rate&#39;: 0.014, &#39;max_depth&#39;: 20, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 32}. Best is trial 16 with value: 0.9664961781588531. [I 2021-07-03 13:48:31,376] Trial 49 finished with value: 0.9650435738573538 and parameters: {&#39;lambda&#39;: 0.007245440807228051, &#39;alpha&#39;: 0.24203698386397063, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.018, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 2020, &#39;min_child_weight&#39;: 252}. Best is trial 16 with value: 0.9664961781588531. . Save best trials . print(&#39;Number of finished trials:&#39;, len(study.trials)) print(&#39;Best trial:&#39;, study.best_trial.params) . Number of finished trials: 50 Best trial: {&#39;lambda&#39;: 0.08287684030183871, &#39;alpha&#39;: 0.021800136799959794, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.012, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 44} . Best_trial_fastai= {&#39;lambda&#39;: 0.08287684030183871, &#39;alpha&#39;: 0.021800136799959794, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.012, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 44} . Optuna Visualisation . plot_optimization_histor: shows the scores from all trials as well as the best score so far at each point. . optuna.visualization.plot_optimization_history(study) . . . plot_parallel_coordinate: interactively visualizes the hyperparameters and scores . optuna.visualization.plot_parallel_coordinate(study) . . . plot_slice: shows the evolution of the search. You can see where in the hyperparameter space your search went and which parts of the space were explored more. . optuna.visualization.plot_slice(study) . . . plot_contour: plots parameter interactions on an interactive chart. You can choose which hyperparameters you would like to explore. . optuna.visualization.plot_contour(study, params=[&#39;alpha&#39;, #&#39;max_depth&#39;, &#39;lambda&#39;, &#39;subsample&#39;, &#39;learning_rate&#39;, &#39;subsample&#39;]) . . . Visualize parameter importances. . optuna.visualization.plot_param_importances(study) . . . Visualize empirical distribution function . optuna.visualization.plot_edf(study) . . . Model Best Optuna Trials . Best_trial_fastai= {&#39;lambda&#39;: 0.08287684030183871, &#39;alpha&#39;: 0.021800136799959794, &#39;colsample_bytree&#39;: 0.4, &#39;subsample&#39;: 1.0, &#39;learning_rate&#39;: 0.012, &#39;max_depth&#39;: 15, &#39;random_state&#39;: 48, &#39;min_child_weight&#39;: 44,&#39;n_estimators&#39;: 1000,&#39;tree_method&#39;:&#39;gpu_hist&#39;} . Using StratifiedKfold cross validation to test roc_auc_score . kf = StratifiedKFold(n_splits=5,random_state=48,shuffle=True) auc=[] # list contains auc for each fold n=0 for trn_idx, test_idx in kf.split(X_train_fa,y_train_fa): X_tr,X_val=X_train_fa.iloc[trn_idx],X_train_fa.iloc[test_idx] y_tr,y_val=y_train_fa[trn_idx],y_train_fa[test_idx] model = xgb.XGBClassifier(**Best_trial_fastai) model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False) # preds+=model.predict(test_df[columns])/kf.n_splits auc.append(roc_auc_score(y_val, model.predict_proba(X_val)[:,1])) print(n+1,auc[n]) n+=1 . 1 0.9670692718197905 2 0.9666783942446252 3 0.9624103790810723 4 0.9650869557007921 5 0.9661417297238831 . Calculate mean of all folds . np.mean(auc) . 0.9654773461140327 . Create inference for test set fastai . roc_auc_binary = RocAucBinary() learn = tabular_learner(dls, metrics=roc_auc_binary) . dl_test = learn.dls.test_dl(df_test.iloc[:]) . X_test=dl_test.dataset.xs . Save Kaggle Submission File . preds = model.predict_proba(X_test)[:,1] sample = pd.read_csv(path/&#39;sample_submission.csv&#39;) sample.QuoteConversion_Flag = preds sample.to_csv(path/&#39;xgb_optuna_fastaidata.csv&#39;, index=False) . Kaggle Score: 0.96633 .",
            "url": "https://redditech.github.io/team-fast-tabulous/kaggle/2021/07/04/Fastai-EDA-And-XGBoost-Optuna-Tuning.html",
            "relUrl": "/kaggle/2021/07/04/Fastai-EDA-And-XGBoost-Optuna-Tuning.html",
            "date": " • Jul 4, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Permutation Importance and Ensemble Experiments in Fastai Tabular For Homesite Competition",
            "content": "Introduction . Learning from WalkWithFastai&#39;s lesson on permutation importance and ensemble techniques we apply some of it to the homesite competition data . Notes: . Changed the categorize functions from last notebook to exclude any columns in y_names from being evaluated since these shouldn&#39;t be part of the model training as a parameter | . Setup . !pip install -Uqq fastai . !pip install kaggle . Requirement already satisfied: kaggle in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (1.5.12) Requirement already satisfied: certifi in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from kaggle) (2021.5.30) Requirement already satisfied: python-dateutil in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from kaggle) (2.8.1) Requirement already satisfied: urllib3 in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from kaggle) (1.26.4) Requirement already satisfied: python-slugify in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from kaggle) (5.0.2) Requirement already satisfied: requests in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from kaggle) (2.25.1) Requirement already satisfied: tqdm in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from kaggle) (4.61.1) Requirement already satisfied: six&gt;=1.10 in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from kaggle) (1.16.0) Requirement already satisfied: text-unidecode&gt;=1.3 in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from requests-&gt;kaggle) (4.0.0) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from requests-&gt;kaggle) (2.10) . from fastai.tabular.all import * from kaggle import api . Path.cwd() . Path(&#39;/mnt/d/Code/GitHub/team-fast-tabulous/_notebooks&#39;) . !touch .gitignore . !echo &quot;_data&quot; &gt; .gitignore . !mkdir _data . mkdir: cannot create directory ‘_data’: File exists . os.chdir(&#39;_data&#39;) Path.cwd() . Path(&#39;/mnt/d/Code/GitHub/team-fast-tabulous/_notebooks/_data&#39;) . path = Path.cwd()/&quot;homesite_competition_data&quot; path.mkdir(exist_ok=True) Path.BASE_PATH = path api.competition_download_cli(&#39;homesite-quote-conversion&#39;, path=path) file_extract(path/&quot;homesite-quote-conversion.zip&quot;) file_extract(path/&quot;train.csv.zip&quot;) file_extract(path/&quot;test.csv.zip&quot;) path.ls() . homesite-quote-conversion.zip: Skipping, found more recently modified local copy (use --force to force download) . (#6) [Path(&#39;homesite-quote-conversion.zip&#39;),Path(&#39;sample_submission.csv.zip&#39;),Path(&#39;test.csv&#39;),Path(&#39;test.csv.zip&#39;),Path(&#39;train.csv&#39;),Path(&#39;train.csv.zip&#39;)] . Set my parameter defaults (may change) . random_seed = 42 bs = 4096 val_bs = 512 test_size = 0.3 epochs = 3 lr = 1e-2 wd=0.002 layers = [10000,500] dropout = [0.001, 0.01] y_block=CategoryBlock() emb_dropout=0.02 set_seed(42) . df_train = pd.read_csv(path/&quot;train.csv&quot;, low_memory=False) df_train.head(2) . QuoteNumber Original_Quote_Date QuoteConversion_Flag Field6 Field7 Field8 Field9 Field10 Field11 Field12 ... GeographicField59A GeographicField59B GeographicField60A GeographicField60B GeographicField61A GeographicField61B GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 1 | 2013-08-16 | 0 | B | 23 | 0.9403 | 0.0006 | 965 | 1.0200 | N | ... | 9 | 9 | -1 | 8 | -1 | 18 | -1 | 10 | N | CA | . 1 2 | 2014-04-22 | 0 | F | 7 | 1.0006 | 0.0040 | 548 | 1.2433 | N | ... | 10 | 10 | -1 | 11 | -1 | 17 | -1 | 20 | N | NJ | . 2 rows × 299 columns . df_train.shape . (260753, 299) . df_test = pd.read_csv(path/&quot;test.csv&quot;, low_memory=False) df_test.head(2) . QuoteNumber Original_Quote_Date Field6 Field7 Field8 Field9 Field10 Field11 Field12 CoverageField1A ... GeographicField59A GeographicField59B GeographicField60A GeographicField60B GeographicField61A GeographicField61B GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 3 | 2014-08-12 | E | 16 | 0.9364 | 0.0006 | 1,487 | 1.3045 | N | 4 | ... | 1 | 1 | -1 | 1 | -1 | 20 | -1 | 25 | Y | IL | . 1 5 | 2013-09-07 | F | 11 | 0.9919 | 0.0038 | 564 | 1.1886 | N | 8 | ... | 10 | 10 | -1 | 5 | -1 | 5 | -1 | 21 | N | NJ | . 2 rows × 298 columns . df_test.shape . (173836, 298) . y_column = df_train.columns.difference(df_test.columns) y_column . Index([&#39;QuoteConversion_Flag&#39;], dtype=&#39;object&#39;) . df_train.QuoteConversion_Flag = df_train.QuoteConversion_Flag.astype(dtype=&#39;boolean&#39;) . train_data_balance = pd.DataFrame(df_train[&quot;QuoteConversion_Flag&quot;]).groupby(&quot;QuoteConversion_Flag&quot;) train_data_balance[&quot;QuoteConversion_Flag&quot;].describe() . count unique top freq . QuoteConversion_Flag . False 211859 | 1 | False | 211859 | . True 48894 | 1 | True | 48894 | . Adding Tim&#39;s bits of insight . df_train = df_train.set_index(&#39;QuoteNumber&#39;) df_test = df_test.set_index(&#39;QuoteNumber&#39;) . df_train[&#39;Original_Quote_Date&#39;] = pd.to_datetime(df_train[&#39;Original_Quote_Date&#39;]) df_test[&#39;Original_Quote_Date&#39;] = pd.to_datetime(df_test[&#39;Original_Quote_Date&#39;]) df_train = add_datepart(df_train, &#39;Original_Quote_Date&#39;) df_test = add_datepart(df_test, &#39;Original_Quote_Date&#39;) . y_names = [y_column[0]] cont_names, cat_names = cont_cat_split(df_train, dep_var=y_names) len(cont_names), len(cat_names) . (155, 154) . triage = L() . def reassign_to_categorical(field, df, y_names, continuous, categorical, triage): if ((df[field].isna().sum()==0) and (field not in y_names)): field_categories = df[field].unique() df[field] = df[field].astype(&#39;category&#39;) df[field].cat.set_categories(field_categories, inplace=True) if field in continuous: continuous.remove(field) if field not in categorical: categorical.append(field) else: if field in continuous: continuous.remove(field) if field in categorical: categorical.remove(field) triage.append(field) return df, continuous, categorical, triage . def categorize( df, y_names, cont_names, cat_names, triage, category_threshold): for field in df.columns: if ((len(df[field].unique()) &lt;= category_threshold) and (type(df[field].dtype) != pd.core.dtypes.dtypes.CategoricalDtype)): reassign_to_categorical(field, df, y_names, cont_names, cat_names, triage) return df, cont_names, cat_names, triage . df_train, cont_names, cat_names, triage = categorize(df_train, y_names, cont_names, cat_names, triage, 100) . &quot;QuoteConversion_Flag&quot; in cont_names, &quot;QuoteConversion_Flag&quot; in cat_names #Make sure we&#39;ve gotten our y-column excluded . (False, False) . procs = [Categorify, FillMissing, Normalize] splits = TrainTestSplitter(test_size=test_size, stratify=df_train[y_names])(df_train) . . to = TabularPandas(df=df_train, procs=procs, cat_names=cat_names, cont_names=cont_names, y_names=y_names,splits=splits, y_block=y_block) dls = to.dataloaders(bs=bs, val_bs=val_bs, layers=layers, embed_ps=emb_dropout, ps=dropout) dls.valid.show_batch() . Field6 Field10 Field12 CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 SalesField3 SalesField4 SalesField5 SalesField7 SalesField9 SalesField10 SalesField11 SalesField13 SalesField14 SalesField15 PersonalField1 PersonalField2 PersonalField5 PersonalField6 PersonalField8 PersonalField9 PersonalField11 PersonalField12 PersonalField13 PersonalField16 PersonalField17 PersonalField18 PersonalField19 PersonalField22 PersonalField23 PersonalField24 PersonalField25 PersonalField26 PersonalField27 PersonalField28 PersonalField29 PersonalField30 PersonalField31 PersonalField32 PersonalField33 PersonalField34 PersonalField35 PersonalField36 PersonalField37 PersonalField38 PersonalField39 PersonalField40 PersonalField41 PersonalField42 PersonalField43 PersonalField44 PersonalField45 PersonalField46 PersonalField47 PersonalField48 PersonalField49 PersonalField50 PersonalField51 PersonalField52 PersonalField53 PersonalField54 PersonalField55 PersonalField56 PersonalField57 PersonalField58 PersonalField59 PersonalField60 PersonalField61 PersonalField62 PersonalField63 PersonalField64 PersonalField65 PersonalField66 PersonalField67 PersonalField68 PersonalField69 PersonalField70 PersonalField71 PersonalField72 PersonalField73 PersonalField74 PersonalField75 PersonalField76 PersonalField77 PersonalField78 PersonalField79 PersonalField80 PersonalField81 PersonalField82 PersonalField83 PropertyField2A PropertyField5 PropertyField6 PropertyField7 PropertyField8 PropertyField9 PropertyField10 PropertyField11A PropertyField11B PropertyField12 PropertyField13 PropertyField14 PropertyField15 PropertyField17 PropertyField18 PropertyField19 PropertyField20 PropertyField22 PropertyField23 PropertyField27 PropertyField28 PropertyField30 PropertyField31 PropertyField33 PropertyField35 PropertyField37 GeographicField5A GeographicField5B GeographicField10A GeographicField10B GeographicField14A GeographicField14B GeographicField18A GeographicField21A GeographicField22A GeographicField22B GeographicField23A GeographicField56A GeographicField60A GeographicField61A GeographicField62A GeographicField62B GeographicField63 GeographicField64 Original_Quote_Year Original_Quote_Month Original_Quote_Dayofweek Original_Quote_Is_month_end Original_Quote_Is_month_start Original_Quote_Is_quarter_end Original_Quote_Is_quarter_start Original_Quote_Is_year_end Original_Quote_Is_year_start Field7 Field8 Field9 Field11 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField6 SalesField12 PersonalField4A PersonalField4B PersonalField10A PersonalField10B PersonalField14 PersonalField15 PropertyField1A PropertyField1B PropertyField2B PropertyField16A PropertyField16B PropertyField21A PropertyField21B PropertyField24A PropertyField24B PropertyField25 PropertyField26A PropertyField26B PropertyField39A PropertyField39B GeographicField1A GeographicField1B GeographicField2A GeographicField2B GeographicField3A GeographicField3B GeographicField4A GeographicField4B GeographicField6A GeographicField6B GeographicField7A GeographicField7B GeographicField8A GeographicField8B GeographicField9A GeographicField9B GeographicField11A GeographicField11B GeographicField12A GeographicField12B GeographicField13A GeographicField13B GeographicField15A GeographicField15B GeographicField16A GeographicField16B GeographicField17A GeographicField17B GeographicField18B GeographicField19A GeographicField19B GeographicField20A GeographicField20B GeographicField21B GeographicField23B GeographicField24A GeographicField24B GeographicField25A GeographicField25B GeographicField26A GeographicField26B GeographicField27A GeographicField27B GeographicField28A GeographicField28B GeographicField29A GeographicField29B GeographicField30A GeographicField30B GeographicField31A GeographicField31B GeographicField32A GeographicField32B GeographicField33A GeographicField33B GeographicField34A GeographicField34B GeographicField35A GeographicField35B GeographicField36A GeographicField36B GeographicField37A GeographicField37B GeographicField38A GeographicField38B GeographicField39A GeographicField39B GeographicField40A GeographicField40B GeographicField41A GeographicField41B GeographicField42A GeographicField42B GeographicField43A GeographicField43B GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60B GeographicField61B Original_Quote_Week Original_Quote_Day SalesField8 Original_Quote_Dayofyear Original_Quote_Elapsed QuoteConversion_Flag . 0 B | 935 | N | 13 | 22 | 13 | 23 | T | D | 1 | 5 | 5 | M | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | O | 1 | 0 | 1 | -1 | 21 | 4 | 2 | C | 1 | 1 | 2 | 0 | 0 | 2 | 4 | 4 | B | N | K | G | 2 | N | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 9 | N | CA | 2014 | 2 | 6 | False | False | False | False | False | False | 25 | 0.9403 | 0.0007 | 1.02 | 9 | 15 | 9 | 15 | 8 | 13 | 8 | 14 | 3 | 5 | 4 | 11 | 3 | 10 | 11 | 0 | 4 | 5 | 14 | 21 | 2 | 24 | 7 | 10 | 23 | 3 | 6 | 9 | 15 | 8 | 11 | 1.0 | 14 | 20 | 8 | 7 | 7 | 15 | 11 | 10 | 5 | 4 | 13 | 13 | 2 | 5 | 6 | 7 | 2 | 7 | 2 | 2 | 2 | 7 | 4 | 7 | 2 | 7 | 9 | 8 | 3 | 7 | 2 | 18 | 2 | 2 | 4 | 3 | 7 | 8 | 8 | 16 | 14 | 18 | 18 | 20 | 22 | 15 | 11 | 10 | 17 | 14 | 21 | 5 | 6 | 10 | 15 | 11 | 18 | 7 | 9 | 16 | 21 | 3 | 3 | 6 | 5 | 2 | 3 | 7 | 10 | 7 | 15 | 3 | 8 | 18 | 16 | 7 | 7 | 1 | 1 | 7 | 2 | 7 | 7 | 13 | 15 | 1 | 1 | 10 | 7 | 12 | 6 | 8 | 4 | 9 | 7 | 9 | 7 | 13 | 7 | 13 | 12 | 14 | 22 | 18 | 9 | 6 | 15 | 20 | 10 | 10 | 3 | 23 | 6 | 9 | 61662.999557 | 40.000002 | 1.391904e+09 | False | . 1 E | 1,480 | N | 13 | 22 | 13 | 23 | T | F | 0 | 5 | 5 | R | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | 1 | 2 | 0 | 1 | 2 | YF | ZK | XK | XC | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | R | 1 | 0 | 1 | -1 | 21 | 2 | 2 | C | 4 | 1 | 2 | 0 | 0 | 2 | 1 | 10 | B | N | O | H | 2 | N | -1 | 13 | -1 | 25 | -1 | 23 | -1 | -1 | -1 | 17 | -1 | -1 | -1 | -1 | -1 | 8 | N | IL | 2013 | 8 | 5 | True | False | False | False | False | False | 14 | 0.9487 | 0.0006 | 1.3045 | 6 | 9 | 6 | 9 | 5 | 8 | 6 | 9 | 4 | 8 | 3 | 8 | 4 | 12 | 4 | 0 | 19 | 22 | 6 | 12 | 2 | 7 | 6 | 9 | 3 | 4 | 8 | 6 | 9 | 10 | 15 | 1.0 | 17 | 23 | 18 | 22 | 6 | 13 | 14 | 13 | 13 | 16 | 9 | 7 | 13 | 22 | 16 | 19 | 13 | 20 | 25 | 25 | 11 | 19 | 16 | 19 | 11 | 19 | 15 | 19 | 15 | 23 | 3 | 18 | 18 | 15 | 16 | 2 | 5 | 24 | 19 | 2 | 1 | 2 | 2 | 2 | 2 | 2 | 1 | 6 | 8 | 6 | 6 | 8 | 15 | 12 | 20 | 7 | 11 | 5 | 5 | 21 | 24 | 21 | 24 | 7 | 8 | 4 | 9 | 11 | 18 | 14 | 23 | 25 | 25 | 15 | 13 | 25 | 25 | 12 | 20 | 13 | 13 | 7 | 7 | 21 | 23 | 5 | 3 | 18 | 18 | 17 | 13 | 13 | 9 | 17 | 18 | 17 | 19 | 20 | 18 | 8 | 5 | 4 | 6 | 12 | 10 | 8 | 12 | 14 | 18 | 22 | 16 | 24 | 35 | 31 | 1129.999604 | 242.999997 | 1.377907e+09 | False | . 2 F | 548 | N | 13 | 22 | 25 | 25 | Y | E | 1 | 5 | 5 | P | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 1 | 7 | 0 | 1 | 2 | 0 | 1 | 2 | ZH | XV | YF | XX | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | A | 1 | 0 | 1 | -1 | 21 | 1 | 2 | A | 1 | 1 | 1 | 1 | 0 | 2 | 1 | 4 | B | N | O | H | 2 | Y | -1 | 18 | -1 | 25 | -1 | 17 | -1 | -1 | -1 | 18 | -1 | -1 | -1 | -1 | -1 | 8 | N | NJ | 2015 | 2 | 3 | False | False | False | False | False | False | 22 | 0.9893 | 0.004 | 1.2433 | 7 | 12 | 7 | 12 | 10 | 17 | 7 | 11 | 8 | 17 | 4 | 10 | 4 | 12 | 11 | 1 | 3 | 3 | 6 | 13 | 1 | 10 | 24 | 25 | 12 | 3 | 6 | 7 | 12 | 9 | 13 | 2.0 | 6 | 7 | 13 | 15 | 8 | 17 | 14 | 13 | 7 | 6 | 13 | 13 | 5 | 16 | 10 | 16 | 6 | 16 | 6 | 14 | 4 | 13 | 10 | 16 | 5 | 16 | 11 | 17 | 7 | 16 | 2 | 13 | 16 | 25 | 25 | 25 | 25 | 18 | 17 | 21 | 22 | 18 | 19 | 13 | 17 | 22 | 21 | 8 | 14 | 8 | 12 | 10 | 20 | 8 | 11 | 12 | 19 | 5 | 5 | 11 | 13 | 14 | 22 | 12 | 20 | 4 | 7 | 12 | 20 | 14 | 22 | 10 | 17 | 23 | 20 | 14 | 18 | 9 | 11 | 20 | 21 | 9 | 12 | 20 | 22 | 6 | 7 | 17 | 16 | 14 | 8 | 14 | 10 | 17 | 19 | 17 | 18 | 17 | 13 | 9 | 6 | 7 | 14 | 2 | 15 | 17 | 17 | 22 | 16 | 20 | 8 | 24 | 9 | 26 | 30835.999977 | 56.999998 | 1.424909e+09 | True | . 3 B | 935 | N | 13 | 22 | 13 | 23 | Y | D | 1 | 5 | 5 | T | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | 1 | 1 | 1 | 4 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | O | 1 | 0 | 1 | -1 | 21 | 4 | 2 | C | 1 | 1 | 2 | 0 | 0 | 2 | 1 | 10 | B | N | O | H | 2 | N | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 21 | N | CA | 2014 | 11 | 2 | False | False | False | False | False | False | 25 | 0.9153 | 0.0007 | 1.02 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 14 | 23 | 2 | 1 | 3 | 10 | 11 | 0 | 20 | 24 | 3 | 1 | 2 | 24 | 12 | 17 | 25 | 3 | 4 | 3 | 3 | 3 | 3 | 1.0 | 8 | 10 | 12 | 15 | 14 | 23 | 11 | 10 | 5 | 4 | 20 | 22 | 2 | 4 | 8 | 10 | 2 | 10 | 1 | 1 | 2 | 7 | 6 | 10 | 2 | 10 | 11 | 18 | 3 | 10 | 2 | 17 | 14 | 3 | 5 | 3 | 7 | 9 | 8 | 22 | 23 | 25 | 25 | 25 | 25 | 24 | 23 | 11 | 18 | 7 | 9 | 11 | 21 | 2 | 1 | 6 | 10 | 4 | 4 | 16 | 21 | 9 | 16 | 14 | 21 | 19 | 24 | 13 | 20 | 3 | 3 | 2 | 2 | 1 | 1 | 6 | 6 | 1 | 1 | 8 | 3 | 9 | 9 | 16 | 17 | 1 | 1 | 10 | 7 | 15 | 10 | 12 | 8 | 9 | 6 | 8 | 6 | 11 | 5 | 13 | 12 | 15 | 22 | 19 | 9 | 6 | 5 | 3 | 4 | 2 | 6 | 24 | 47 | 19 | 21377.999465 | 322.999999 | 1.416355e+09 | False | . 4 J | 1,113 | N | 13 | 22 | 1 | 6 | X | G | 0 | 5 | 5 | T | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | 1 | 2 | 0 | 1 | 2 | YF | ZK | XN | XC | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | J | 1 | 0 | 1 | -1 | 21 | 4 | 2 | C | 1 | 1 | 2 | 1 | 0 | 2 | 4 | 10 | B | N | O | G | 0 | N | 25 | 25 | -1 | 25 | -1 | 17 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 8 | N | TX | 2014 | 11 | 1 | False | False | False | False | False | False | 26 | 0.887 | 0.0004 | 1.2665 | 10 | 16 | 10 | 16 | 8 | 14 | 9 | 16 | 18 | 24 | 17 | 24 | 14 | 24 | 22 | 1 | 17 | 21 | 15 | 21 | 2 | 7 | 10 | 14 | 8 | 5 | 10 | 10 | 16 | 11 | 16 | 1.0 | 18 | 23 | 11 | 12 | 2 | 2 | 10 | 8 | 2 | 2 | 14 | 15 | 11 | 19 | 21 | 20 | 12 | 19 | 17 | 20 | 12 | 20 | 19 | 19 | 13 | 20 | 23 | 22 | 10 | 19 | 2 | 6 | 3 | 7 | 9 | 3 | 5 | 5 | 19 | 20 | 21 | 14 | 11 | 7 | 8 | 18 | 17 | 4 | 4 | 9 | 12 | 5 | 5 | 5 | 5 | 4 | 4 | 6 | 7 | 3 | 2 | 4 | 5 | 5 | 4 | 4 | 9 | 5 | 5 | 6 | 10 | 8 | 13 | 14 | 10 | 14 | 18 | 25 | 25 | 8 | 4 | 12 | 20 | 13 | 15 | 8 | 15 | 13 | 10 | 18 | 16 | 16 | 14 | 13 | 12 | 12 | 12 | 17 | 12 | 8 | 5 | 4 | 8 | 15 | 18 | 22 | 1 | 1 | 7 | 6 | 1 | 10 | 45 | 4 | 29808.999885 | 307.999995 | 1.415059e+09 | False | . 5 B | 935 | N | 13 | 22 | 13 | 23 | Y | E | 1 | 5 | 4 | T | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | 1 | 2 | 0 | 3 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | O | 0 | 0 | 1 | -1 | 21 | 2 | 1 | C | 2 | 1 | 2 | 0 | 0 | 2 | 1 | 10 | D | N | O | G | 2 | Y | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 21 | N | CA | 2014 | 9 | 1 | False | False | False | False | False | False | 25 | 0.9153 | 0.0007 | 1.02 | 9 | 15 | 9 | 15 | 8 | 13 | 8 | 15 | 7 | 15 | 5 | 14 | 4 | 13 | 11 | 0 | 4 | 5 | 5 | 4 | 4 | 24 | 17 | 21 | 22 | 14 | 22 | 9 | 15 | 9 | 13 | 1.0 | 16 | 21 | 10 | 10 | 11 | 21 | 15 | 14 | 14 | 16 | 6 | 3 | 2 | 6 | 4 | 6 | 2 | 6 | 2 | 8 | 2 | 4 | 3 | 5 | 2 | 5 | 6 | 6 | 2 | 6 | 22 | 25 | 24 | 4 | 8 | 6 | 17 | 8 | 6 | 17 | 16 | 19 | 22 | 16 | 19 | 21 | 20 | 9 | 15 | 17 | 23 | 11 | 21 | 21 | 24 | 4 | 5 | 12 | 19 | 10 | 11 | 12 | 20 | 13 | 20 | 4 | 7 | 4 | 3 | 9 | 17 | 2 | 4 | 14 | 11 | 4 | 3 | 2 | 2 | 9 | 7 | 20 | 22 | 9 | 5 | 9 | 18 | 13 | 10 | 15 | 10 | 16 | 14 | 13 | 12 | 12 | 11 | 15 | 9 | 19 | 21 | 11 | 19 | 20 | 11 | 8 | 8 | 7 | 10 | 10 | 19 | 24 | 36 | 2 | 20190.000282 | 245.000000 | 1.409616e+09 | True | . 6 F | 564 | N | 13 | 22 | 13 | 23 | Y | E | 1 | 4 | 3 | V | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | 1 | 2 | 0 | 1 | 2 | XM | ZF | XZ | YF | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | R | 0 | 0 | 1 | -1 | 21 | 2 | 0 | B | 2 | 0 | 1 | 0 | 0 | 2 | 1 | 10 | B | N | N | G | 2 | Y | -1 | 21 | -1 | 25 | -1 | 16 | -1 | -1 | -1 | 16 | -1 | -1 | -1 | -1 | -1 | 8 | N | NJ | 2013 | 11 | 4 | False | False | False | False | False | False | 11 | 0.9919 | 0.0038 | 1.1886 | 1 | 1 | 1 | 1 | 2 | 1 | 1 | 1 | 24 | 25 | 1 | 1 | 6 | 18 | 16 | 2 | 1 | 1 | 6 | 9 | 1 | 10 | 13 | 17 | 23 | 2 | 2 | 1 | 1 | 2 | 2 | 1.0 | 7 | 8 | 24 | 25 | 2 | 4 | 11 | 9 | 4 | 4 | 17 | 19 | 5 | 16 | 10 | 15 | 5 | 16 | 7 | 16 | 4 | 12 | 9 | 14 | 5 | 14 | 11 | 17 | 7 | 16 | 2 | 9 | 13 | 19 | 18 | 20 | 22 | 16 | 19 | 18 | 17 | 14 | 13 | 7 | 10 | 19 | 18 | 8 | 13 | 11 | 16 | 12 | 22 | 4 | 4 | 6 | 10 | 6 | 7 | 10 | 12 | 12 | 20 | 17 | 23 | 6 | 15 | 12 | 20 | 9 | 18 | 10 | 16 | 23 | 20 | 12 | 15 | 9 | 10 | 16 | 17 | 6 | 7 | 17 | 19 | 6 | 7 | 13 | 11 | 8 | 2 | 9 | 5 | 14 | 15 | 14 | 13 | 12 | 7 | 12 | 11 | 6 | 11 | 21 | 1 | 1 | 5 | 3 | 13 | 15 | 2 | 5 | 48 | 29 | 50545.000349 | 332.999998 | 1.385683e+09 | True | . 7 F | 548 | N | 13 | 22 | 13 | 23 | T | E | 1 | 5 | 5 | Q | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | 1 | 3 | 0 | 5 | 2 | XM | YV | ZW | XC | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | R | 0 | 0 | 1 | -1 | 21 | 3 | 0 | B | 8 | 0 | 1 | 1 | 0 | 2 | 8 | 4 | B | N | O | H | 2 | Y | -1 | 15 | -1 | 25 | -1 | 14 | -1 | -1 | -1 | 22 | -1 | -1 | -1 | -1 | 25 | 25 | N | NJ | 2014 | 4 | 3 | False | False | False | False | False | False | 7 | 1.0006 | 0.004 | 1.2433 | 8 | 14 | 8 | 14 | 7 | 12 | 8 | 14 | 3 | 5 | 4 | 13 | 4 | 13 | 11 | 0 | 5 | 5 | -1 | -1 | 3 | 7 | 20 | 23 | 19 | 3 | 3 | 8 | 14 | 10 | 15 | 2.0 | 6 | 8 | 12 | 13 | 2 | 1 | 16 | 16 | 8 | 8 | 7 | 4 | 5 | 16 | 8 | 10 | 5 | 15 | 8 | 16 | 5 | 16 | 8 | 10 | 5 | 16 | 9 | 8 | 5 | 10 | 2 | 9 | 5 | 22 | 21 | 18 | 20 | 21 | 17 | 11 | 5 | 8 | 3 | 4 | 3 | 11 | 3 | 5 | 5 | 2 | 1 | 10 | 19 | 3 | 3 | 6 | 9 | 3 | 2 | 12 | 16 | 5 | 5 | 9 | 14 | 7 | 16 | 12 | 19 | 10 | 19 | 11 | 18 | 22 | 19 | 13 | 17 | 10 | 13 | 21 | 22 | 10 | 14 | 20 | 22 | 6 | 6 | 18 | 18 | 15 | 10 | 14 | 10 | 14 | 14 | 14 | 14 | 17 | 13 | 11 | 9 | 9 | 17 | 14 | 17 | 20 | 1 | 1 | 23 | 25 | 10 | 2 | 16 | 17 | 34038.000009 | 107.000001 | 1.397693e+09 | False | . 8 B | 965 | N | 13 | 22 | 1 | 6 | Y | E | 0 | 4 | 3 | P | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 0 | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 3 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 3 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 3 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | O | 0 | 0 | 1 | -1 | 21 | 2 | 2 | C | 4 | 1 | 2 | 0 | 0 | 2 | 1 | 10 | B | N | O | H | 2 | N | -1 | 13 | -1 | 25 | -1 | 9 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 18 | N | CA | 2013 | 3 | 1 | False | False | False | False | False | False | 19 | 0.9403 | 0.0006 | 1.02 | 7 | 12 | 7 | 12 | 10 | 18 | 7 | 12 | 8 | 17 | 3 | 5 | 3 | 6 | 14 | 0 | 25 | 25 | 6 | 13 | 1 | 24 | 16 | 21 | 9 | 16 | 23 | 7 | 12 | 7 | 9 | 1.0 | 13 | 18 | 9 | 8 | 11 | 21 | 18 | 18 | 16 | 19 | 10 | 8 | 2 | 7 | 8 | 10 | 2 | 10 | 2 | 3 | 2 | 8 | 6 | 9 | 2 | 9 | 11 | 19 | 3 | 10 | 5 | 20 | 20 | 5 | 9 | 8 | 18 | 4 | 4 | 13 | 8 | 14 | 11 | 11 | 16 | 14 | 9 | 5 | 6 | 17 | 23 | 7 | 10 | 16 | 23 | 10 | 16 | 17 | 23 | 7 | 5 | 25 | 25 | 15 | 22 | 8 | 18 | 13 | 20 | 2 | 2 | 3 | 6 | 2 | 4 | 7 | 8 | 11 | 16 | 15 | 17 | 2 | 3 | 10 | 11 | 21 | 23 | 18 | 18 | 18 | 15 | 19 | 19 | 19 | 21 | 19 | 21 | 20 | 18 | 21 | 23 | 5 | 10 | 2 | 16 | 18 | 4 | 3 | 16 | 20 | 23 | 9 | 10 | 5 | 33851.000000 | 64.000001 | 1.362442e+09 | True | . 9 J | 1,113 | N | 1 | 2 | 1 | 6 | T | I | 1 | 5 | 5 | T | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | 1 | 3 | 0 | 5 | 2 | ZD | XE | ZN | YF | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | R | 1 | 0 | 1 | -1 | 21 | 4 | 2 | C | 4 | 0 | 2 | 1 | 0 | 2 | 1 | 6 | B | N | M | F | 2 | Y | 25 | 25 | -1 | 25 | -1 | 22 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 9 | N | TX | 2014 | 9 | 2 | False | False | False | False | False | False | 26 | 0.8928 | 0.0004 | 1.2665 | 5 | 6 | 5 | 6 | 4 | 5 | 4 | 6 | 5 | 10 | 12 | 23 | 16 | 24 | 11 | 0 | 3 | 3 | -1 | -1 | 4 | 13 | 3 | 4 | 13 | 3 | 4 | 5 | 6 | 12 | 18 | 2.0 | 8 | 10 | 25 | 25 | 2 | 3 | 6 | 4 | 8 | 8 | 16 | 18 | 13 | 21 | 20 | 19 | 14 | 20 | 21 | 23 | 11 | 20 | 19 | 19 | 12 | 20 | 23 | 20 | 15 | 23 | 2 | 6 | 5 | 10 | 12 | 8 | 18 | 10 | 19 | 20 | 21 | 13 | 11 | 10 | 15 | 15 | 9 | 5 | 5 | 8 | 11 | 3 | 2 | 11 | 17 | 5 | 7 | 6 | 7 | 2 | 2 | 7 | 12 | 2 | 2 | 5 | 11 | 2 | 1 | 5 | 9 | 1 | 1 | 14 | 12 | 7 | 7 | 18 | 23 | 8 | 5 | 12 | 20 | 5 | 2 | 8 | 15 | 7 | 4 | 19 | 17 | 18 | 18 | 9 | 6 | 9 | 7 | 18 | 15 | 5 | 2 | 5 | 10 | 18 | 17 | 20 | 13 | 16 | 5 | 4 | 4 | 11 | 37 | 10 | 23712.000008 | 253.000002 | 1.410307e+09 | False | . learn = tabular_learner(dls, metrics=accuracy) . . learn.lr_find(suggest_funcs=(valley, slide, minimum, steep)) . SuggestedLRs(valley=tensor(0.0008), slide=tensor(0.0229), minimum=0.06309573650360108, steep=0.013182567432522774) . learn.fit_one_cycle(epochs,lr, wd=wd) . epoch train_loss valid_loss accuracy time . 0 | 0.342952 | 0.215471 | 0.909276 | 00:15 | . 1 | 0.227020 | 0.178934 | 0.922762 | 00:15 | . 2 | 0.184288 | 0.179401 | 0.922698 | 00:14 | . preds, targs = learn.get_preds() . accuracy(preds,targs) . TensorBase(0.9227) . class PermutationImportance(): &quot;Calculate and plot the permutation importance&quot; def __init__(self, learn:Learner, df=None, bs=None): &quot;Initialize with a test dataframe, a learner, and a metric&quot; self.learn = learn self.df = df if df is not None else None bs = bs if bs is not None else learn.dls.bs self.dl = learn.dls.test_dl(self.df, bs=bs) if self.df is not None else learn.dls[1] self.x_names = learn.dls.x_names.filter(lambda x: &#39;_na&#39; not in x) self.na = learn.dls.x_names.filter(lambda x: &#39;_na&#39; in x) self.y = dls.y_names self.results = self.calc_feat_importance() self.plot_importance(self.ord_dic_to_df(self.results)) def measure_col(self, name:str): &quot;Measures change after column shuffle&quot; col = [name] if f&#39;{name}_na&#39; in self.na: col.append(name) orig = self.dl.items[col].values perm = np.random.permutation(len(orig)) self.dl.items[col] = self.dl.items[col].values[perm] metric = learn.validate(dl=self.dl)[1] self.dl.items[col] = orig return metric def calc_feat_importance(self): &quot;Calculates permutation importance by shuffling a column on a percentage scale&quot; print(&#39;Getting base error&#39;) base_error = self.learn.validate(dl=self.dl)[1] self.importance = {} pbar = progress_bar(self.x_names) print(&#39;Calculating Permutation Importance&#39;) for col in pbar: self.importance[col] = self.measure_col(col) for key, value in self.importance.items(): self.importance[key] = (base_error-value)/base_error #this can be adjusted return OrderedDict(sorted(self.importance.items(), key=lambda kv: kv[1], reverse=True)) def ord_dic_to_df(self, dict:OrderedDict): return pd.DataFrame([[k, v] for k, v in dict.items()], columns=[&#39;feature&#39;, &#39;importance&#39;]) def plot_importance(self, df:pd.DataFrame, limit=20, asc=False, **kwargs): &quot;Plot importance with an optional limit to how many variables shown&quot; df_copy = df.copy() df_copy[&#39;feature&#39;] = df_copy[&#39;feature&#39;].str.slice(0,25) df_copy = df_copy.sort_values(by=&#39;importance&#39;, ascending=asc)[:limit].sort_values(by=&#39;importance&#39;, ascending=not(asc)) ax = df_copy.plot.barh(x=&#39;feature&#39;, y=&#39;importance&#39;, sort_columns=True, **kwargs) for p in ax.patches: ax.annotate(f&#39;{p.get_width():.4f}&#39;, ((p.get_width() * 1.005), p.get_y() * 1.005)) . imp = PermutationImportance(learn) . Getting base error . Calculating Permutation Importance . . 100.00% [300/300 13:11&lt;00:00] From this most important fields are PropertyField37, PersonalField2, PersonalField1, SalesField5 . Adding in XGBoost . import xgboost as xgb . n_estimators = 100 max_depth = 8 learning_rate = 0.1 subsample = 0.5 . X_train, y_train = to.train.xs, to.train.ys.values.ravel() X_valid, y_valid = to.valid.xs, to.valid.ys.values.ravel() . model = xgb.XGBClassifier(n_estimators = n_estimators, max_depth=max_depth, learning_rate=0.1, subsample=subsample) . xgb_model = model.fit(X_train, y_train) . [22:49:10] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. . xgb_preds = xgb_model.predict_proba(X_valid) . xgb_preds . array([[0.96717715, 0.03282287], [0.96896064, 0.03103935], [0.00723386, 0.99276614], ..., [0.87994975, 0.12005027], [0.00716239, 0.9928376 ], [0.90019906, 0.09980095]], dtype=float32) . accuracy(tensor(xgb_preds), tensor(y_valid)) . TensorBase(0.9237) . from xgboost import plot_importance . plot_importance(xgb_model, height=1,max_num_features=20,) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Feature importance&#39;}, xlabel=&#39;F score&#39;, ylabel=&#39;Features&#39;&gt; . From this most important fields were SalesField1A, PersonalField9, Original_Quote_Elapsed, PersonalField10A, PersonalField10B, PropertyField37 . Doing Ensemble . avgs = (preds + xgb_preds) / 2 . avgs . tensor([[0.9761, 0.0239], [0.9612, 0.0388], [0.0040, 0.9960], ..., [0.9249, 0.0751], [0.0036, 0.9964], [0.9015, 0.0985]]) . argmax = avgs.argmax(dim=1) . argmax . tensor([0, 0, 1, ..., 0, 1, 0]) . y_valid . array([0, 0, 1, ..., 0, 1, 0], dtype=int8) . accuracy(tensor(preds), tensor(y_valid)) . TensorBase(0.9227) . accuracy(tensor(xgb_preds), tensor(y_valid)) . TensorBase(0.9237) . accuracy(tensor(avgs), tensor(y_valid)) . TensorBase(0.9246) . So we have a slightly better performance with ensembling these two . Adding Random Forest . from sklearn.ensemble import RandomForestClassifier . tree = RandomForestClassifier(n_estimators=100) . tree.fit(X_train, y_train) . RandomForestClassifier() . !pip install rfpimp . Collecting rfpimp Downloading rfpimp-1.3.7.tar.gz (10 kB) Requirement already satisfied: numpy in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from rfpimp) (1.19.2) Requirement already satisfied: pandas in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from rfpimp) (1.2.4) Requirement already satisfied: scikit-learn in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from rfpimp) (0.24.2) Requirement already satisfied: matplotlib in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from rfpimp) (3.3.4) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from matplotlib-&gt;rfpimp) (1.3.1) Requirement already satisfied: pillow&gt;=6.2.0 in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from matplotlib-&gt;rfpimp) (8.2.0) Requirement already satisfied: python-dateutil&gt;=2.1 in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from matplotlib-&gt;rfpimp) (2.8.1) Requirement already satisfied: cycler&gt;=0.10 in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from matplotlib-&gt;rfpimp) (0.10.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.3 in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from matplotlib-&gt;rfpimp) (2.4.7) Requirement already satisfied: six in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from cycler&gt;=0.10-&gt;matplotlib-&gt;rfpimp) (1.16.0) Requirement already satisfied: pytz&gt;=2017.3 in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from pandas-&gt;rfpimp) (2021.1) Requirement already satisfied: joblib&gt;=0.11 in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from scikit-learn-&gt;rfpimp) (1.0.1) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from scikit-learn-&gt;rfpimp) (2.1.0) Requirement already satisfied: scipy&gt;=0.19.1 in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from scikit-learn-&gt;rfpimp) (1.6.2) Building wheels for collected packages: rfpimp Building wheel for rfpimp (setup.py) ... done Created wheel for rfpimp: filename=rfpimp-1.3.7-py3-none-any.whl size=10668 sha256=34224b08f42a6e9b6ed748abc732400eaff9bf484bb215a1f3b6fea8cabd1d49 Stored in directory: /home/nissan/.cache/pip/wheels/d9/f2/53/6d8c73011f73fc347598d683ff8b2343605ad43474ae083816 Successfully built rfpimp Installing collected packages: rfpimp Successfully installed rfpimp-1.3.7 . from rfpimp import * . impTree = importances(tree, X_valid, to.valid.ys) . plot_importances(impTree) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-29T23:40:10.173326 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ So here the most important are PropertyField37, Field7,PersonalField1, SalesField5, PersonalField9,PersonalField2 . forest_preds = tree.predict_proba(X_valid) . forest_preds . array([[0.95, 0.05], [0.91, 0.09], [0.32, 0.68], ..., [0.85, 0.15], [0.34, 0.66], [0.88, 0.12]]) . accuracy(tensor(forest_preds), tensor(y_valid)) . TensorBase(0.9152) . new_avgs = (preds + xgb_preds + forest_preds) / 3 . accuracy(tensor(new_avgs), tensor(y_valid)) . TensorBase(0.9242) . So it gets slightly worse when we add Random Forest to the ensemble. . Next step will be to apply the models to the test set from Kaggle and try submissions to see how they score .",
            "url": "https://redditech.github.io/team-fast-tabulous/kaggle/fastai/2021/06/29/Permutation-Importance-And-Ensemble-Experiment.html",
            "relUrl": "/kaggle/fastai/2021/06/29/Permutation-Importance-And-Ensemble-Experiment.html",
            "date": " • Jun 29, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Improving `cont_cat_split` in Fastai Tabular For Homesite Competition",
            "content": "Introduction . This is an additional modification of the &quot;first pass&quot; submission and subsequent additional work to the Homesite Competition on Kaggle Competition using Google Colab. Modification of some of the default parameters is done with some learning from the initial exploratory data analysis at this time, as well as other sources of readings. The major addition is listed below, but all are to see if I can improve the baseline after applying what we learnt so far to see how it improves (or not) our submission then. . Changes made: . Use a function to better split categorical and continuous fields, and set up the categories needed for training automatically | Included in the function is a new list triage which would require a manual analysis to determine a filling strategy that best suits modelling, or if to ignore the field for training | Changed from RandomSplitter() to TrainTestSplitter() for making test and validation sets more fairly weighted based on the bias of the input data towards negative results | Increase batch size to 1024 to make training shorter, but to still hopefully get a better predictor for it. Set a separate validator batch size to 128. | Increase the validation percentage to 0.25 | Fix the learning rate to 1e-2 | Increase epochs to 7, see if it overfits and what effect that has | Modified the cat_names and cont_names arrays with the initial insights from the EDA notebook post | Add a date part for dates | Add weight decay of 0.2 | . Old stuff, read about in other notebooks . Setup fastai and Google drive . !pip install -Uqq fastai . from fastai.tabular.all import * . The snippet below is only useful in Colab for accessing my Google Drive and is straight out the fastbook source code in Github . global gdrive gdrive = Path(&#39;/content/gdrive/My Drive&#39;) from google.colab import drive if not gdrive.exists(): drive.mount(str(gdrive.parent)) . Only add the Kaggle bits below if I&#39;m running locally, in Collab they&#39;re already here . . !ls /content/gdrive/MyDrive/Kaggle/kaggle.json . /content/gdrive/MyDrive/Kaggle/kaggle.json . Useful links here: . Documentation on Path library | Documentation on fastai extensions to Path library | . Path.cwd() . Path(&#39;/content&#39;) . Setup kaggle environment parameters . !mkdir -p ~/.kaggle !cp /content/gdrive/MyDrive/Kaggle/kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . from kaggle import api . path = Path.cwd() path.ls() . (#4) [Path(&#39;/content/.config&#39;),Path(&#39;/content/gdrive&#39;),Path(&#39;/content/models&#39;),Path(&#39;/content/sample_data&#39;)] . path = path/&quot;gdrive/MyDrive/Kaggle/homesite_data&quot; path.mkdir(exist_ok=True) Path.BASE_PATH = path api.competition_download_cli(&#39;homesite-quote-conversion&#39;, path=path) file_extract(path/&quot;homesite-quote-conversion.zip&quot;) file_extract(path/&quot;train.csv.zip&quot;) file_extract(path/&quot;test.csv.zip&quot;) . homesite-quote-conversion.zip: Skipping, found more recently modified local copy (use --force to force download) . path . Path(&#39;.&#39;) . path.ls() . NameError Traceback (most recent call last) &lt;ipython-input-4-4e20bcf9cc48&gt; in &lt;module&gt; -&gt; 1 path.ls() NameError: name &#39;path&#39; is not defined . Exploring the Homesite data . Set the random seed so that the results are reproducible, set other parameters so changes can be made quickly. Trying to avoid &#39;magic numbers&#39;) where possible . set_seed(42) bs = 1024 val_bs = 128 test_size = 0.25 epochs = 5 lr = 0.0012 wd=0.2 . df_train = pd.read_csv(path/&quot;train.csv&quot;, low_memory=False) df_train.head() . QuoteNumber Original_Quote_Date QuoteConversion_Flag Field6 Field7 Field8 Field9 Field10 Field11 Field12 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField3 SalesField4 SalesField5 SalesField6 SalesField7 SalesField8 SalesField9 SalesField10 SalesField11 SalesField12 ... GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56A GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60A GeographicField60B GeographicField61A GeographicField61B GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 1 | 2013-08-16 | 0 | B | 23 | 0.9403 | 0.0006 | 965 | 1.0200 | N | 17 | 23 | 17 | 23 | 15 | 22 | 16 | 22 | 13 | 22 | 13 | 23 | T | D | 2 | 1 | 7 | 18 | 3 | 8 | 0 | 5 | 5 | 24 | V | 48649 | 0 | 0 | 0 | 0 | ... | 8 | 4 | 20 | 22 | 10 | 8 | 6 | 5 | 15 | 13 | 19 | 18 | 16 | 14 | 21 | 23 | 21 | 23 | 16 | 11 | 22 | 24 | 7 | 14 | -1 | 17 | 15 | 17 | 14 | 18 | 9 | 9 | -1 | 8 | -1 | 18 | -1 | 10 | N | CA | . 1 2 | 2014-04-22 | 0 | F | 7 | 1.0006 | 0.0040 | 548 | 1.2433 | N | 6 | 8 | 6 | 8 | 5 | 7 | 5 | 8 | 13 | 22 | 13 | 23 | T | E | 5 | 9 | 5 | 14 | 6 | 18 | 1 | 5 | 5 | 11 | P | 26778 | 0 | 0 | 1 | 1 | ... | 23 | 24 | 11 | 15 | 21 | 24 | 6 | 11 | 21 | 21 | 18 | 15 | 20 | 20 | 13 | 12 | 12 | 12 | 15 | 9 | 13 | 11 | 11 | 20 | -1 | 9 | 18 | 21 | 8 | 7 | 10 | 10 | -1 | 11 | -1 | 17 | -1 | 20 | N | NJ | . 2 4 | 2014-08-25 | 0 | F | 7 | 1.0006 | 0.0040 | 548 | 1.2433 | N | 7 | 12 | 7 | 12 | 6 | 10 | 7 | 11 | 25 | 25 | 13 | 23 | T | J | 4 | 6 | 3 | 10 | 4 | 11 | 1 | 5 | 5 | 11 | K | 8751 | 0 | 0 | 2 | 2 | ... | 21 | 22 | 24 | 25 | 20 | 22 | 7 | 13 | 23 | 23 | 20 | 19 | 20 | 20 | 18 | 20 | 19 | 21 | 20 | 19 | 11 | 8 | 3 | 3 | -1 | 5 | 21 | 24 | 12 | 15 | 15 | 18 | -1 | 21 | -1 | 11 | -1 | 8 | N | NJ | . 3 6 | 2013-04-15 | 0 | J | 10 | 0.9769 | 0.0004 | 1,165 | 1.2665 | N | 3 | 2 | 3 | 2 | 2 | 2 | 3 | 2 | 13 | 22 | 13 | 23 | Y | F | 15 | 23 | 8 | 19 | 14 | 24 | 0 | 5 | 5 | 23 | V | 43854 | 0 | 0 | 0 | 0 | ... | 3 | 1 | 14 | 22 | 6 | 2 | 7 | 14 | 11 | 8 | 19 | 18 | 18 | 16 | 13 | 12 | 13 | 12 | 17 | 13 | 5 | 2 | 3 | 4 | -1 | 7 | 14 | 14 | 14 | 18 | 6 | 5 | -1 | 10 | -1 | 9 | -1 | 21 | N | TX | . 4 8 | 2014-01-25 | 0 | E | 23 | 0.9472 | 0.0006 | 1,487 | 1.3045 | N | 8 | 13 | 8 | 13 | 7 | 11 | 7 | 13 | 13 | 22 | 13 | 23 | T | F | 4 | 6 | 3 | 6 | 3 | 6 | 1 | 5 | 5 | 7 | R | 12505 | 1 | 0 | 0 | 0 | ... | 24 | 25 | 9 | 11 | 25 | 25 | 5 | 3 | 22 | 22 | 21 | 21 | 17 | 15 | 25 | 25 | 25 | 25 | 17 | 13 | 13 | 11 | 3 | 4 | -1 | 7 | 11 | 9 | 10 | 10 | 18 | 22 | -1 | 10 | -1 | 11 | -1 | 12 | N | IL | . 5 rows × 299 columns . df_train.shape . (260753, 299) . df_test = pd.read_csv(path/&quot;test.csv&quot;, low_memory=False) df_test.head() . QuoteNumber Original_Quote_Date Field6 Field7 Field8 Field9 Field10 Field11 Field12 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField3 SalesField4 SalesField5 SalesField6 SalesField7 SalesField8 SalesField9 SalesField10 SalesField11 SalesField12 SalesField13 ... GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56A GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60A GeographicField60B GeographicField61A GeographicField61B GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 3 | 2014-08-12 | E | 16 | 0.9364 | 0.0006 | 1,487 | 1.3045 | N | 4 | 4 | 4 | 4 | 3 | 3 | 3 | 4 | 13 | 22 | 13 | 23 | Y | K | 13 | 22 | 6 | 16 | 9 | 21 | 0 | 5 | 5 | 11 | P | 67052 | 0 | 0 | 0 | 0 | 0 | ... | 22 | 23 | 9 | 12 | 25 | 25 | 6 | 9 | 4 | 2 | 16 | 12 | 20 | 20 | 2 | 2 | 2 | 1 | 1 | 1 | 10 | 7 | 25 | 25 | -1 | 19 | 19 | 22 | 12 | 15 | 1 | 1 | -1 | 1 | -1 | 20 | -1 | 25 | Y | IL | . 1 5 | 2013-09-07 | F | 11 | 0.9919 | 0.0038 | 564 | 1.1886 | N | 8 | 14 | 8 | 14 | 7 | 12 | 8 | 13 | 13 | 22 | 13 | 23 | T | E | 4 | 5 | 3 | 6 | 3 | 6 | 1 | 5 | 5 | 4 | R | 27288 | 1 | 0 | 0 | 0 | 0 | ... | 23 | 24 | 12 | 21 | 23 | 25 | 7 | 11 | 16 | 14 | 13 | 6 | 17 | 15 | 7 | 5 | 7 | 5 | 13 | 7 | 14 | 14 | 7 | 14 | -1 | 4 | 1 | 1 | 5 | 3 | 10 | 10 | -1 | 5 | -1 | 5 | -1 | 21 | N | NJ | . 2 7 | 2013-03-29 | F | 15 | 0.8945 | 0.0038 | 564 | 1.0670 | N | 11 | 18 | 11 | 18 | 10 | 16 | 10 | 18 | 13 | 22 | 13 | 23 | T | E | 3 | 3 | 5 | 14 | 3 | 9 | 1 | 5 | 5 | 23 | V | 65264 | 0 | 1 | 2 | 2 | 0 | ... | 16 | 18 | 9 | 10 | 14 | 16 | 6 | 8 | 20 | 19 | 17 | 14 | 16 | 13 | 20 | 22 | 20 | 22 | 20 | 18 | 10 | 7 | 4 | 7 | -1 | 11 | 13 | 12 | 18 | 22 | 10 | 11 | -1 | 20 | -1 | 22 | -1 | 11 | N | NJ | . 3 9 | 2015-03-21 | K | 21 | 0.8870 | 0.0004 | 1,113 | 1.2665 | Y | 14 | 22 | 15 | 22 | 13 | 20 | 22 | 25 | 13 | 22 | 13 | 23 | Y | F | 5 | 9 | 9 | 20 | 5 | 16 | 1 | 5 | 5 | 11 | R | 32725 | 1 | 1 | 1 | 1 | 0 | ... | 11 | 11 | 9 | 10 | 11 | 13 | 15 | 21 | 14 | 12 | 17 | 13 | 10 | 6 | 20 | 22 | 20 | 22 | 19 | 16 | 12 | 11 | 4 | 6 | -1 | 13 | 10 | 8 | 5 | 3 | 8 | 8 | -1 | 13 | -1 | 8 | -1 | 21 | N | TX | . 4 10 | 2014-12-10 | B | 25 | 0.9153 | 0.0007 | 935 | 1.0200 | N | 4 | 5 | 4 | 5 | 4 | 4 | 4 | 5 | 13 | 22 | 13 | 23 | Y | D | 12 | 21 | 1 | 1 | 3 | 6 | 0 | 5 | 5 | 11 | T | 56025 | 0 | 1 | 1 | 1 | 0 | ... | 9 | 8 | 25 | 25 | 9 | 3 | 9 | 18 | 7 | 4 | 16 | 12 | 13 | 9 | 8 | 6 | 8 | 6 | 11 | 5 | 19 | 21 | 13 | 21 | -1 | 23 | 11 | 8 | 5 | 3 | 7 | 7 | -1 | 3 | -1 | 22 | -1 | 21 | N | CA | . 5 rows × 298 columns . df_test.shape . (173836, 298) . y_column = df_train.columns.difference(df_test.columns) . y_column . Index([&#39;QuoteConversion_Flag&#39;], dtype=&#39;object&#39;) . From this it looks like QuoteConversion_Flag is the value we want to predict. Let&#39;s take a look at this . type(df_train.QuoteConversion_Flag) . pandas.core.series.Series . df_train.QuoteConversion_Flag.unique() . array([0, 1]) . type(df_train.QuoteConversion_Flag.unique()[0]) . numpy.int64 . Make this a boolean for the purpose of generating predictions as a binary classification . df_train.QuoteConversion_Flag = df_train.QuoteConversion_Flag.astype(dtype=&#39;boolean&#39;) . Let&#39;s see how the training data outcomes are balanced . df_train.QuoteConversion_Flag.describe() . count 260753 unique 2 top False freq 211859 Name: QuoteConversion_Flag, dtype: object . train_data_balance = pd.DataFrame(df_train[&quot;QuoteConversion_Flag&quot;]).groupby(&quot;QuoteConversion_Flag&quot;) . train_data_balance[&quot;QuoteConversion_Flag&quot;].describe() . count unique top freq . QuoteConversion_Flag . False 211859 | 1 | False | 211859 | . True 48894 | 1 | True | 48894 | . We have about 5 times as many &quot;No Sale&quot; data rows as we do data that shows a successful sale happened. This data bias may have an impact on the effectiveness of our model to predict positive sales results . First things first . Learning from my colleague Tim&#39;s work already we know: . Quotenumber is unique so we can make it the index | Original_Quote_Date column should be set as a date type | . Additionally, we should make sure to apply any changes to data types to both train and test data so predictions don&#39;t fail later on . df_train = df_train.set_index(&#39;QuoteNumber&#39;) df_test = df_test.set_index(&#39;QuoteNumber&#39;) . We may have some NaN values for Original_Quote_Date in either the training or test dataset, but let&#39;s confirm there are none. . df_train[&#39;Original_Quote_Date&#39;].isna().sum(), df_test[&#39;Original_Quote_Date&#39;].isna().sum() . (0, 0) . df_train[&#39;Original_Quote_Date&#39;] = pd.to_datetime(df_train[&#39;Original_Quote_Date&#39;]) df_test[&#39;Original_Quote_Date&#39;] = pd.to_datetime(df_test[&#39;Original_Quote_Date&#39;]) . Add the date_part to see if this helps improve modeling . df_train = add_datepart(df_train, &#39;Original_Quote_Date&#39;) df_test = add_datepart(df_test, &#39;Original_Quote_Date&#39;) . Goal: Automate the bits we can, continue refining fastai parameters, continue using EDA insights gathered to date . y_names = [y_column[0]] y_names . [&#39;QuoteConversion_Flag&#39;] . cont_names, cat_names = cont_cat_split(df_train, dep_var=y_names) len(cont_names), len(cat_names) . (155, 154) . # df_train.drop(&#39;PersonalField84&#39;, axis=1, inplace=True) . New Work - A function to improve fastai&#39;s cont_cat_split choices . First I&#39;ll create a triage list for any fields that can&#39;t be programmatically optimized. These are the ones we have to do manual steps for until I find a way to do better . triage = L() . Let&#39;s take a quick look at the descriptions of the categorical and continuous splits automatically done . df_train[cont_names].astype(&#39;object&#39;).describe() . Field7 Field8 Field9 Field11 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField6 SalesField8 SalesField12 PersonalField4A PersonalField4B PersonalField10A PersonalField10B PersonalField14 PersonalField15 PersonalField84 PropertyField1A PropertyField1B PropertyField2B PropertyField16A PropertyField16B PropertyField21A PropertyField21B PropertyField24A PropertyField24B PropertyField25 PropertyField26A PropertyField26B ... GeographicField42B GeographicField43A GeographicField43B GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60B GeographicField61B Original_Quote_Week Original_Quote_Day Original_Quote_Dayofyear Original_Quote_Elapsed . count 260753 | 260753.0000 | 260753.0000 | 260753.00 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 136545.0 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753.0 | 260753 | 260753 | ... | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 2.607530e+05 | . unique 28 | 38.0000 | 5.0000 | 11.00 | 26 | 26 | 25 | 25 | 25 | 25 | 25 | 25 | 26 | 26 | 25 | 25 | 26 | 26 | 24 | 61530 | 22 | 26 | 26 | 26 | 26 | 30 | 22 | 7.0 | 26 | 26 | 21 | 26 | 26 | 26 | 26 | 26 | 26 | 11.0 | 26 | 26 | ... | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 25 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 25 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 25 | 52 | 31 | 365 | 8.680000e+02 | . top 23 | 0.9403 | 0.0006 | 1.02 | 6 | 9 | 6 | 7 | 5 | 9 | 6 | 4 | 4 | 15 | 3 | 5 | 3 | 1 | 11 | 36088 | 0 | -1 | -1 | -1 | -1 | 2 | 24 | 2.0 | 4 | 20 | 15 | 3 | 1 | 6 | 3 | 7 | 4 | 1.0 | 9 | 10 | ... | 21 | 11 | 5 | 9 | 22 | 11 | 4 | 10 | 7 | 6 | 5 | 19 | 8 | 18 | 25 | 17 | 23 | 14 | 25 | 14 | 17 | 20 | 25 | 15 | 2 | 4 | 9 | 2 | 13 | 19 | 11 | 1 | 11 | 6 | 18 | 2 | 11 | 3 | 70 | 1.393805e+09 | . freq 66474 | 48161.0000 | 74096.0000 | 92359.00 | 24736 | 11089 | 25026 | 11137 | 23663 | 10881 | 24614 | 10756 | 28378 | 10923 | 50250 | 10581 | 59953 | 10487 | 89335 | 19 | 167612 | 24526 | 24526 | 54785 | 54785 | 85608 | 124015 | 134906.0 | 23038 | 12620 | 22598 | 30574 | 10571 | 24336 | 10548 | 20522 | 11055 | 121563.0 | 20538 | 11664 | ... | 10547 | 41899 | 10553 | 32537 | 10552 | 51208 | 10595 | 64704 | 10670 | 76413 | 18736 | 15213 | 10760 | 21011 | 10547 | 18093 | 10525 | 17007 | 10472 | 16667 | 10517 | 21298 | 10501 | 17134 | 10487 | 28655 | 10545 | 20725 | 22086 | 10625 | 19660 | 10570 | 17608 | 11216 | 10583 | 20717 | 7523 | 9314 | 1502 | 6.460000e+02 | . 4 rows × 155 columns . The first thing I notice, is that for a number of these fields, there is quite a low number of unique values. I also notice some, like PersonalField84 are missing quite a bit of data. These are a lot of fields to go through and manually recategorize and then get their categories, but I am hoping I can do this programmatically. . Let&#39;s have a look at the existing categoricals to make sure there&#39;s nothing suspeicious about these here either . df_train[cat_names].astype(&#39;object&#39;).describe() . Field6 Field10 Field12 CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 SalesField3 SalesField4 SalesField5 SalesField7 SalesField9 SalesField10 SalesField11 SalesField13 SalesField14 SalesField15 PersonalField1 PersonalField2 PersonalField5 PersonalField6 PersonalField7 PersonalField8 PersonalField9 PersonalField11 PersonalField12 PersonalField13 PersonalField16 PersonalField17 PersonalField18 PersonalField19 PersonalField22 PersonalField23 PersonalField24 PersonalField25 PersonalField26 PersonalField27 PersonalField28 ... PropertyField22 PropertyField23 PropertyField27 PropertyField28 PropertyField30 PropertyField31 PropertyField32 PropertyField33 PropertyField34 PropertyField35 PropertyField36 PropertyField37 PropertyField38 GeographicField5A GeographicField5B GeographicField10A GeographicField10B GeographicField14A GeographicField14B GeographicField18A GeographicField21A GeographicField22A GeographicField22B GeographicField23A GeographicField56A GeographicField60A GeographicField61A GeographicField62A GeographicField62B GeographicField63 GeographicField64 Original_Quote_Year Original_Quote_Month Original_Quote_Dayofweek Original_Quote_Is_month_end Original_Quote_Is_month_start Original_Quote_Is_quarter_end Original_Quote_Is_quarter_start Original_Quote_Is_year_end Original_Quote_Is_year_start . count 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260640 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | ... | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260683 | 260753 | 260683 | 260753 | 260640 | 260753 | 259533 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | . unique 8 | 8 | 2 | 3 | 4 | 3 | 4 | 7 | 12 | 2 | 5 | 5 | 7 | 2 | 19 | 20 | 8 | 12 | 12 | 2 | 2 | 9 | 2 | 2 | 3 | 3 | 5 | 5 | 4 | 50 | 66 | 61 | 57 | 7 | 13 | 14 | 14 | 14 | 17 | 7 | ... | 5 | 13 | 17 | 4 | 2 | 4 | 2 | 4 | 2 | 3 | 2 | 2 | 2 | 2 | 14 | 1 | 2 | 2 | 20 | 2 | 2 | 2 | 12 | 2 | 2 | 2 | 2 | 2 | 19 | 3 | 4 | 3 | 12 | 7 | 2 | 2 | 2 | 2 | 2 | 2 | . top B | 935 | N | 13 | 22 | 13 | 23 | Y | E | 1 | 5 | 5 | K | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 1 | ... | 2 | 1 | 4 | B | N | O | Y | H | Y | 2 | N | N | N | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 8 | N | CA | 2014 | 3 | 0 | False | False | False | False | False | False | . freq 94694 | 52353 | 241867 | 212390 | 212390 | 170589 | 170589 | 143152 | 85017 | 194250 | 158481 | 147571 | 50261 | 222421 | 200758 | 170439 | 246479 | 240315 | 239868 | 189441 | 182307 | 164202 | 164202 | 259379 | 260715 | 190879 | 254940 | 160257 | 258621 | 124015 | 124015 | 124015 | 124015 | 254447 | 256107 | 255215 | 254447 | 253407 | 129036 | 134167 | ... | 259655 | 205119 | 92797 | 243374 | 245807 | 156042 | 191627 | 123098 | 141185 | 166293 | 248302 | 185976 | 254032 | 254208 | 135320 | 260753 | 260751 | 253814 | 71697 | 254135 | 254285 | 254158 | 156405 | 254096 | 254153 | 254239 | 254170 | 254144 | 83404 | 254712 | 94725 | 110107 | 31742 | 49924 | 252323 | 252730 | 257614 | 257381 | 260240 | 260404 | . 4 rows × 154 columns . So there may be some fields with missing data here, like PropertyField38 we will have to look at a strategy for these too . Question for later:Should those with only two categories be mapped as Categorical or Booleans? Would there be any impact here? See an example field below . field = &quot;Field12&quot; df_train[field].unique() . array([&#39;N&#39;, &#39;Y&#39;], dtype=object) . Here I define two functions, which will help . to reset the cont_names and cat_names arrays with better fits of the actual data fields for those that are categorical, but were put into the continuous array. | For all cateogircal fields, it will also setup the categories for these fields and change their dtype | For any fields that have null values, it will remove them from their respective field, and place them in the triage list | . def reassign_to_categorical(field, df, continuous, categorical, triage): if df[field].isna().sum()==0: field_categories = df[field].unique() df[field] = df[field].astype(&#39;category&#39;) df[field].cat.set_categories(field_categories, inplace=True) if field in continuous: continuous.remove(field) if field not in categorical: categorical.append(field) else: if field in continuous: continuous.remove(field) if field in categorical: categorical.remove(field) triage.append(field) return df, continuous, categorical, triage . def categorize( df, cont_names, cat_names, triage, category_threshold): for field in df.columns: if ((len(df[field].unique()) &lt;= category_threshold) and (type(df[field].dtype) != pd.core.dtypes.dtypes.CategoricalDtype)): reassign_to_categorical(field, df, cont_names, cat_names, triage) return df, cont_names, cat_names, triage . field = &#39;Field8&#39; df_train[field].unique() . array([0.9403, 1.0006, 0.9769, 0.9472, 0.9258, 0.9153, 0.9691, 0.9919, 0.9497, 0.9893, 0.8793, 0.9485, 0.8922, 1.0101, 0.893 , 0.9219, 0.9392, 0.9685, 0.887 , 0.8746, 0.8928, 0.9838, 1.0005, 0.9487, 0.9313, 0.9525, 0.9559, 0.9482, 0.9223, 0.9566, 0.9108, 0.8945, 0.9023, 0.9489, 0.9194, 0.9364, 0.9368, 0.9375]) . df_train, cont_names, cat_names, triage = categorize(df_train, cont_names, cat_names, triage, 100) . len(cont_names), len(cat_names) . (3, 298) . So this is a big rebalancing between continuous and categorical fields. I saved alot of time with this function rather than doing this manually like I had for my initial data exploration. Let&#39;s take a look at how many came up for triaging still though . triage . (#9) [&#39;PersonalField7&#39;,&#39;PersonalField84&#39;,&#39;PropertyField3&#39;,&#39;PropertyField4&#39;,&#39;PropertyField29&#39;,&#39;PropertyField32&#39;,&#39;PropertyField34&#39;,&#39;PropertyField36&#39;,&#39;PropertyField38&#39;] . And let&#39;s look again at Field8 to see how it did the categorizations for me . field = &#39;Field8&#39; df_train[field].unique() . [0.9403, 1.0006, 0.9769, 0.9472, 0.9258, ..., 0.9489, 0.9194, 0.9364, 0.9368, 0.9375] Length: 38 Categories (38, float64): [0.9403, 1.0006, 0.9769, 0.9472, ..., 0.9194, 0.9364, 0.9368, 0.9375] . ToDo: Put in bits that triage those 9 fields that popped up. For now I will run the modelling ignoring these fields and see if any improvements happen. . Triaging . triage . (#9) [&#39;PersonalField7&#39;,&#39;PersonalField84&#39;,&#39;PropertyField3&#39;,&#39;PropertyField4&#39;,&#39;PropertyField29&#39;,&#39;PropertyField32&#39;,&#39;PropertyField34&#39;,&#39;PropertyField36&#39;,&#39;PropertyField38&#39;] . PersonalField7 . field = &#39;PersonalField7&#39; True in df_train[field].isna() . True . df_train[field].value_counts() . N 259379 Y 1261 Name: PersonalField7, dtype: int64 . Back to the training pipeline I had originally . . &quot;QuoteConversion_Flag&quot; in cont_names, &quot;QuoteConversion_Flag&quot; in cat_names #Make sure we&#39;ve gotten our y-column excluded . (False, True) . if (y_column in cont_names): cont_names.remove(y_column) if (y_column in cat_names): cat_names.remove(y_column) . procs = [Categorify, FillMissing, Normalize] splits = TrainTestSplitter(test_size=test_size, stratify=df_train[y_names])(df_train) . to = TabularPandas(df=df_train, procs=procs, cat_names=cat_names, cont_names=cont_names, y_names=y_names,splits=splits) dls = to.dataloaders(bs=bs, val_bs=val_bs) dls.valid.show_batch() . Field6 Field10 Field12 CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 SalesField3 SalesField4 SalesField5 SalesField7 SalesField9 SalesField10 SalesField11 SalesField13 SalesField14 SalesField15 PersonalField1 PersonalField2 PersonalField5 PersonalField6 PersonalField8 PersonalField9 PersonalField11 PersonalField12 PersonalField13 PersonalField16 PersonalField17 PersonalField18 PersonalField19 PersonalField22 PersonalField23 PersonalField24 PersonalField25 PersonalField26 PersonalField27 PersonalField28 PersonalField29 PersonalField30 PersonalField31 PersonalField32 PersonalField33 PersonalField34 PersonalField35 PersonalField36 PersonalField37 PersonalField38 PersonalField39 PersonalField40 PersonalField41 PersonalField42 PersonalField43 PersonalField44 PersonalField45 PersonalField46 PersonalField47 PersonalField48 PersonalField49 PersonalField50 PersonalField51 PersonalField52 PersonalField53 PersonalField54 PersonalField55 PersonalField56 PersonalField57 PersonalField58 PersonalField59 PersonalField60 PersonalField61 PersonalField62 PersonalField63 PersonalField64 PersonalField65 PersonalField66 PersonalField67 PersonalField68 PersonalField69 PersonalField70 PersonalField71 PersonalField72 PersonalField73 PersonalField74 PersonalField75 PersonalField76 PersonalField77 PersonalField78 PersonalField79 PersonalField80 PersonalField81 PersonalField82 PersonalField83 PropertyField2A PropertyField5 PropertyField6 PropertyField7 PropertyField8 PropertyField9 PropertyField10 PropertyField11A PropertyField11B PropertyField12 PropertyField13 PropertyField14 PropertyField15 PropertyField17 PropertyField18 PropertyField19 PropertyField20 PropertyField22 PropertyField23 PropertyField27 PropertyField28 PropertyField30 PropertyField31 PropertyField33 PropertyField35 PropertyField37 GeographicField5A GeographicField5B GeographicField10A GeographicField10B GeographicField14A GeographicField14B GeographicField18A GeographicField21A GeographicField22A GeographicField22B GeographicField23A GeographicField56A GeographicField60A GeographicField61A GeographicField62A GeographicField62B GeographicField63 GeographicField64 Original_Quote_Year Original_Quote_Month Original_Quote_Dayofweek Original_Quote_Is_month_end Original_Quote_Is_month_start Original_Quote_Is_quarter_end Original_Quote_Is_quarter_start Original_Quote_Is_year_end Original_Quote_Is_year_start Field7 Field8 Field9 Field11 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField6 SalesField12 PersonalField4A PersonalField4B PersonalField10A PersonalField10B PersonalField14 PersonalField15 PropertyField1A PropertyField1B PropertyField2B PropertyField16A PropertyField16B PropertyField21A PropertyField21B PropertyField24A PropertyField24B PropertyField25 PropertyField26A PropertyField26B PropertyField39A PropertyField39B GeographicField1A GeographicField1B GeographicField2A GeographicField2B GeographicField3A GeographicField3B GeographicField4A GeographicField4B GeographicField6A GeographicField6B GeographicField7A GeographicField7B GeographicField8A GeographicField8B GeographicField9A GeographicField9B GeographicField11A GeographicField11B GeographicField12A GeographicField12B GeographicField13A GeographicField13B GeographicField15A GeographicField15B GeographicField16A GeographicField16B GeographicField17A GeographicField17B GeographicField18B GeographicField19A GeographicField19B GeographicField20A GeographicField20B GeographicField21B GeographicField23B GeographicField24A GeographicField24B GeographicField25A GeographicField25B GeographicField26A GeographicField26B GeographicField27A GeographicField27B GeographicField28A GeographicField28B GeographicField29A GeographicField29B GeographicField30A GeographicField30B GeographicField31A GeographicField31B GeographicField32A GeographicField32B GeographicField33A GeographicField33B GeographicField34A GeographicField34B GeographicField35A GeographicField35B GeographicField36A GeographicField36B GeographicField37A GeographicField37B GeographicField38A GeographicField38B GeographicField39A GeographicField39B GeographicField40A GeographicField40B GeographicField41A GeographicField41B GeographicField42A GeographicField42B GeographicField43A GeographicField43B GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60B GeographicField61B Original_Quote_Week Original_Quote_Day SalesField8 Original_Quote_Dayofyear Original_Quote_Elapsed QuoteConversion_Flag . 0 B | 935 | N | 13 | 22 | 13 | 23 | T | D | 1 | 5 | 5 | K | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | O | 0 | 0 | 1 | -1 | 21 | 4 | 3 | C | 4 | 1 | 3 | 0 | 0 | 2 | 2 | 4 | B | N | N | G | 1 | N | -1 | 13 | -1 | 25 | -1 | 9 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 12 | N | CA | 2014 | 2 | 0 | False | False | False | False | False | False | 23 | 0.9403 | 0.0007 | 1.02 | 25 | 25 | 25 | 25 | 22 | 25 | 24 | 25 | 1 | 1 | 8 | 19 | 2 | 4 | 11 | 1 | 3 | 3 | 9 | 18 | 4 | 24 | 4 | 6 | 17 | 10 | 19 | 25 | 25 | 18 | 23 | 3 | 7 | 8 | 3 | 2 | 7 | 15 | 11 | 9 | 11 | 13 | 25 | 25 | 2 | 9 | 6 | 7 | 2 | 7 | 2 | 5 | 2 | 9 | 4 | 7 | 2 | 8 | 8 | 7 | 3 | 7 | 10 | 21 | 9 | 2 | 3 | 3 | 6 | 4 | 2 | 13 | 8 | 16 | 15 | 16 | 20 | 15 | 11 | 6 | 9 | 6 | 6 | 5 | 4 | 7 | 8 | 16 | 22 | 14 | 21 | 6 | 5 | 6 | 9 | 5 | 4 | 7 | 17 | 12 | 19 | 8 | 16 | 3 | 8 | 2 | 1 | 5 | 3 | 10 | 11 | 15 | 17 | 2 | 4 | 10 | 12 | 21 | 23 | 13 | 10 | 14 | 7 | 6 | 3 | 19 | 21 | 18 | 20 | 14 | 8 | 21 | 23 | 12 | 20 | 21 | 6 | 3 | 6 | 4 | 9 | 9 | 16 | 12 | 7 | 10 | 42316.000014 | 40.999994 | 1.391990e+09 | False | . 1 B | 965 | N | 13 | 22 | 13 | 23 | T | D | 1 | 5 | 5 | Q | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | O | 0 | 0 | 1 | -1 | 21 | 4 | 2 | C | 4 | 1 | 3 | 0 | 0 | 2 | 4 | 10 | B | N | N | E | 1 | N | -1 | 13 | -1 | 25 | -1 | 8 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 24 | N | CA | 2013 | 11 | 3 | False | False | False | False | False | False | 23 | 0.9403 | 0.0006 | 1.02 | 24 | 25 | 24 | 25 | 21 | 24 | 23 | 25 | 1 | 1 | 12 | 22 | 4 | 13 | 7 | 1 | 19 | 23 | 16 | 21 | 4 | 24 | 17 | 22 | 4 | 9 | 18 | 24 | 25 | 17 | 22 | 1 | 25 | 25 | 3 | 2 | 11 | 21 | 12 | 11 | 4 | 3 | 11 | 10 | 2 | 10 | 2 | 3 | 2 | 4 | 2 | 7 | 2 | 10 | 2 | 4 | 2 | 7 | 2 | 2 | 2 | 2 | 21 | 25 | 24 | 2 | 4 | 3 | 7 | 4 | 5 | 13 | 8 | 16 | 15 | 16 | 20 | 15 | 11 | 13 | 20 | 20 | 24 | 11 | 20 | 16 | 23 | 7 | 11 | 20 | 24 | 13 | 17 | 17 | 24 | 14 | 22 | 10 | 21 | 10 | 15 | 8 | 15 | 3 | 8 | 1 | 1 | 7 | 8 | 7 | 6 | 15 | 16 | 2 | 2 | 9 | 4 | 7 | 14 | 8 | 5 | 12 | 5 | 12 | 8 | 4 | 3 | 4 | 3 | 9 | 4 | 12 | 10 | 18 | 23 | 21 | 16 | 19 | 16 | 21 | 5 | 3 | 6 | 17 | 46 | 14 | 59943.999147 | 317.999998 | 1.384387e+09 | False | . 2 B | 965 | N | 13 | 22 | 13 | 23 | X | D | 1 | 5 | 5 | Q | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | 1 | 2 | 0 | 3 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | O | 1 | 0 | 2 | -1 | 21 | 4 | 2 | C | 1 | 1 | 2 | 0 | 0 | 2 | 4 | 10 | B | N | N | H | 1 | N | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 12 | N | CA | 2013 | 11 | 1 | False | False | False | False | False | False | 25 | 0.9403 | 0.0006 | 1.02 | 14 | 21 | 14 | 21 | 12 | 19 | 13 | 20 | 14 | 22 | 4 | 11 | 2 | 3 | 20 | 0 | 6 | 8 | 5 | 4 | 5 | 24 | 6 | 9 | 22 | 4 | 9 | 14 | 21 | 9 | 14 | 1 | 16 | 22 | 4 | 2 | 7 | 14 | 8 | 5 | 8 | 8 | 20 | 22 | 2 | 4 | 2 | 2 | 2 | 2 | 2 | 3 | 2 | 7 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 18 | 24 | 21 | 2 | 2 | 2 | 3 | 11 | 5 | 15 | 11 | 19 | 20 | 20 | 22 | 17 | 16 | 6 | 10 | 11 | 17 | 23 | 25 | 3 | 2 | 13 | 21 | 19 | 24 | 6 | 5 | 24 | 25 | 6 | 6 | 7 | 17 | 18 | 24 | 8 | 16 | 2 | 3 | 3 | 4 | 4 | 3 | 8 | 8 | 10 | 9 | 3 | 5 | 10 | 8 | 25 | 25 | 8 | 5 | 14 | 8 | 12 | 7 | 12 | 11 | 11 | 10 | 7 | 3 | 20 | 22 | 16 | 23 | 19 | 9 | 6 | 22 | 24 | 6 | 5 | 7 | 17 | 45 | 5 | 3553.999261 | 308.999998 | 1.383610e+09 | True | . 3 C | 1,487 | Y | 13 | 22 | 25 | 25 | V | A | 1 | 5 | 5 | P | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | 1 | 2 | 0 | 1 | 2 | XR | XS | YP | XC | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | A | 1 | 0 | 1 | -1 | 25 | 1 | 3 | C | 5 | 1 | 3 | 1 | 0 | 2 | 13 | 7 | B | Y | K | E | 0 | N | -1 | 13 | -1 | 25 | -1 | 20 | -1 | 25 | -1 | 23 | -1 | -1 | -1 | -1 | -1 | 8 | N | IL | 2015 | 5 | 6 | False | False | False | False | False | False | 17 | 0.8746 | 0.0006 | 1.3045 | 25 | 25 | 25 | 25 | 25 | 25 | 25 | 25 | 12 | 21 | 7 | 18 | 1 | 1 | 11 | 0 | 16 | 20 | 8 | 18 | 3 | 4 | 6 | 9 | 23 | 12 | 21 | 25 | 25 | 23 | 25 | 2 | 16 | 21 | 2 | 1 | 3 | 7 | 16 | 16 | 24 | 25 | 19 | 21 | 10 | 18 | 12 | 17 | 10 | 18 | 18 | 21 | 9 | 18 | 13 | 17 | 9 | 18 | 10 | 12 | 9 | 18 | 2 | 9 | 19 | 20 | 19 | 5 | 16 | 25 | 16 | 5 | 2 | 5 | 2 | 3 | 3 | 6 | 2 | 4 | 4 | 7 | 8 | 7 | 10 | 7 | 8 | 10 | 17 | 13 | 20 | 10 | 12 | 14 | 22 | 9 | 13 | 5 | 12 | 7 | 8 | 6 | 11 | 5 | 10 | 16 | 14 | 9 | 10 | 10 | 13 | 19 | 20 | 10 | 13 | 13 | 15 | 6 | 7 | 20 | 20 | 20 | 20 | 17 | 15 | 21 | 23 | 21 | 23 | 20 | 19 | 19 | 21 | 5 | 9 | 9 | 22 | 24 | 14 | 17 | 13 | 16 | 24 | 2 | 19 | 10 | 7220.998942 | 130.000000 | 1.431216e+09 | False | . 4 F | 548 | N | 13 | 22 | 13 | 23 | T | E | 1 | 5 | 5 | Q | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | 1 | 2 | 0 | 1 | 2 | XH | XG | XF | ZT | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | R | 1 | 0 | 1 | -1 | 21 | 2 | 2 | C | 4 | 0 | 1 | 0 | 0 | 2 | 1 | 10 | B | N | O | H | 1 | N | -1 | 22 | -1 | 25 | -1 | 18 | -1 | -1 | -1 | 16 | -1 | -1 | -1 | -1 | -1 | 17 | N | NJ | 2014 | 3 | 3 | False | False | False | False | False | False | 11 | 0.9566 | 0.004 | 1.1886 | 3 | 2 | 3 | 3 | 2 | 2 | 3 | 2 | 7 | 14 | 4 | 11 | 7 | 20 | 11 | 0 | 8 | 10 | 11 | 19 | 5 | 10 | 8 | 12 | 15 | 4 | 6 | 3 | 2 | 3 | 2 | 1 | 7 | 9 | 12 | 13 | 3 | 7 | 8 | 6 | 8 | 8 | 10 | 9 | 5 | 15 | 11 | 16 | 5 | 15 | 6 | 12 | 4 | 10 | 10 | 16 | 4 | 10 | 12 | 19 | 8 | 17 | 2 | 8 | 11 | 24 | 25 | 25 | 25 | 16 | 18 | 25 | 25 | 24 | 25 | 16 | 18 | 25 | 25 | 1 | 1 | 4 | 3 | 5 | 5 | 8 | 12 | 4 | 4 | 5 | 5 | 7 | 6 | 11 | 19 | 5 | 4 | 6 | 15 | 3 | 2 | 7 | 13 | 9 | 14 | 24 | 24 | 12 | 15 | 10 | 13 | 12 | 12 | 8 | 8 | 10 | 11 | 7 | 11 | 10 | 7 | 5 | 2 | 3 | 1 | 12 | 11 | 10 | 9 | 6 | 2 | 20 | 22 | 12 | 20 | 18 | 9 | 5 | 8 | 7 | 13 | 16 | 5 | 11 | 11 | 13 | 64502.998849 | 71.999999 | 1.394669e+09 | False | . 5 B | 965 | N | 13 | 22 | 13 | 23 | T | D | 0 | 5 | 5 | T | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | 1 | 3 | 0 | 5 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | O | 1 | 0 | 1 | -1 | 21 | 2 | 1 | A | 1 | 0 | 1 | 0 | 0 | 2 | 1 | 10 | B | N | O | G | 2 | Y | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 19 | N | CA | 2013 | 9 | 3 | False | False | False | False | False | False | 24 | 0.9403 | 0.0006 | 1.02 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 10 | 20 | 2 | 1 | 5 | 17 | 4 | 1 | 1 | 1 | -1 | -1 | 4 | 24 | 19 | 23 | 15 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 4 | 4 | 12 | 15 | 9 | 18 | 10 | 8 | 2 | 1 | 17 | 19 | 2 | 3 | 7 | 8 | 2 | 8 | 2 | 5 | 2 | 4 | 5 | 8 | 2 | 8 | 9 | 12 | 3 | 8 | 2 | 16 | 9 | 4 | 8 | 6 | 16 | 4 | 9 | 25 | 25 | 25 | 25 | 25 | 25 | 24 | 25 | 9 | 16 | 10 | 14 | 13 | 23 | 7 | 10 | 6 | 9 | 7 | 8 | 16 | 21 | 2 | 2 | 14 | 21 | 7 | 16 | 13 | 21 | 2 | 2 | 9 | 14 | 18 | 17 | 10 | 11 | 6 | 6 | 12 | 12 | 25 | 25 | 10 | 9 | 5 | 3 | 8 | 5 | 12 | 5 | 12 | 8 | 7 | 5 | 7 | 5 | 13 | 7 | 16 | 16 | 11 | 19 | 16 | 15 | 17 | 1 | 1 | 5 | 4 | 3 | 24 | 37 | 12 | 37300.000122 | 255.000001 | 1.378944e+09 | False | . 6 B | 965 | N | 13 | 22 | 13 | 23 | T | D | 1 | 5 | 5 | T | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 3 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 3 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 3 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | O | 0 | 0 | 1 | -1 | 21 | 2 | 2 | C | 1 | 1 | 2 | 0 | 0 | 2 | 1 | 10 | B | N | N | G | 1 | N | -1 | 13 | -1 | 25 | -1 | 8 | -1 | -1 | -1 | 15 | -1 | -1 | 25 | -1 | -1 | 9 | N | CA | 2013 | 3 | 2 | False | False | False | False | False | False | 23 | 0.9403 | 0.0006 | 1.02 | 9 | 15 | 9 | 15 | 8 | 13 | 8 | 14 | 3 | 5 | 6 | 17 | 5 | 17 | 23 | 0 | 11 | 14 | 8 | 17 | 1 | 24 | 15 | 20 | 8 | 15 | 23 | 9 | 15 | 9 | 14 | 1 | 16 | 22 | 10 | 11 | 3 | 6 | 23 | 24 | 12 | 14 | 11 | 9 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 3 | 2 | 4 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 14 | 23 | 22 | 2 | 4 | 3 | 8 | 11 | 3 | 15 | 11 | 19 | 20 | 20 | 22 | 17 | 16 | 18 | 23 | 25 | 25 | 7 | 9 | 21 | 24 | 19 | 24 | 25 | 25 | 23 | 25 | 25 | 25 | 13 | 21 | 11 | 22 | 13 | 20 | 22 | 25 | 2 | 3 | 3 | 5 | 8 | 9 | 14 | 22 | 14 | 15 | 2 | 3 | 8 | 3 | 24 | 25 | 23 | 23 | 22 | 23 | 23 | 24 | 17 | 18 | 18 | 19 | 22 | 21 | 14 | 13 | 3 | 5 | 4 | 10 | 7 | 12 | 14 | 17 | 21 | 25 | 4 | 11 | 13 | 48178.000355 | 71.999999 | 1.363133e+09 | False | . 7 J | 1,113 | N | 13 | 22 | 13 | 23 | Y | K | 1 | 5 | 5 | Q | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 6 | 1 | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | J | 0 | 0 | 1 | -1 | 21 | 4 | 2 | C | 2 | 0 | 2 | 0 | 0 | 2 | 1 | 10 | B | N | N | G | 0 | N | -1 | 23 | -1 | 25 | -1 | 10 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 16 | N | TX | 2014 | 7 | 3 | False | False | False | False | False | False | 23 | 0.8928 | 0.0004 | 1.2665 | 6 | 8 | 6 | 8 | 5 | 7 | 5 | 8 | 10 | 19 | 19 | 24 | 23 | 25 | 11 | 1 | 7 | 9 | 25 | 25 | 4 | 24 | 6 | 8 | 3 | 3 | 3 | 6 | 8 | 8 | 12 | 1 | 14 | 20 | 15 | 19 | 4 | 7 | 6 | 3 | 10 | 11 | 14 | 15 | 3 | 10 | 23 | 23 | 6 | 16 | 6 | 14 | 3 | 10 | 23 | 22 | 5 | 16 | 24 | 23 | 7 | 15 | 2 | 9 | 5 | 6 | 9 | 7 | 18 | 5 | 10 | 21 | 23 | 19 | 21 | 10 | 13 | 24 | 25 | 6 | 10 | 3 | 2 | 3 | 2 | 2 | 2 | 11 | 18 | 6 | 7 | 6 | 4 | 4 | 3 | 3 | 2 | 6 | 13 | 2 | 1 | 2 | 1 | 16 | 23 | 19 | 18 | 6 | 5 | 15 | 22 | 9 | 6 | 1 | 1 | 20 | 22 | 6 | 5 | 9 | 6 | 21 | 21 | 19 | 19 | 10 | 9 | 11 | 10 | 24 | 25 | 5 | 3 | 6 | 12 | 20 | 17 | 20 | 18 | 22 | 7 | 7 | 9 | 4 | 29 | 17 | 60277.000961 | 198.000000 | 1.405555e+09 | False | . 8 B | 935 | N | 13 | 22 | 1 | 6 | T | J | 0 | 4 | 3 | P | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 6 | 1 | 1 | 2 | 0 | 3 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | O | 1 | 0 | 1 | -1 | 21 | 2 | 2 | A | 4 | 1 | 3 | 0 | 0 | 2 | 1 | 10 | B | N | N | G | 1 | N | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 16 | N | CA | 2014 | 1 | 1 | False | False | False | False | False | False | 25 | 0.9403 | 0.0007 | 1.02 | 7 | 11 | 7 | 11 | 6 | 9 | 6 | 10 | 4 | 7 | 5 | 16 | 6 | 18 | 20 | 0 | 16 | 20 | 4 | 3 | 1 | 24 | 16 | 21 | 20 | 9 | 18 | 7 | 11 | 7 | 10 | 1 | 13 | 19 | 10 | 12 | 12 | 22 | 12 | 11 | 4 | 3 | 24 | 25 | 2 | 3 | 4 | 5 | 2 | 5 | 2 | 2 | 2 | 6 | 3 | 6 | 2 | 6 | 5 | 5 | 2 | 5 | 15 | 23 | 24 | 2 | 2 | 2 | 4 | 7 | 2 | 13 | 8 | 14 | 11 | 11 | 16 | 14 | 9 | 11 | 19 | 10 | 15 | 10 | 18 | 21 | 24 | 8 | 13 | 12 | 19 | 14 | 18 | 15 | 23 | 17 | 23 | 5 | 12 | 12 | 18 | 4 | 6 | 3 | 7 | 2 | 3 | 6 | 6 | 8 | 8 | 14 | 16 | 2 | 4 | 10 | 10 | 21 | 23 | 13 | 10 | 16 | 11 | 16 | 14 | 9 | 7 | 9 | 7 | 14 | 9 | 17 | 18 | 13 | 21 | 21 | 13 | 12 | 20 | 24 | 1 | 1 | 6 | 6 | 3 | 14 | 16438.000261 | 14.000005 | 1.389658e+09 | False | . 9 E | 1,480 | N | 13 | 22 | 13 | 23 | T | F | 1 | 5 | 5 | P | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | 1 | 2 | 0 | 1 | 2 | ZG | ZF | ZN | ZQ | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | R | 1 | 0 | 1 | -1 | 23 | 3 | 2 | C | 4 | 0 | 2 | 1 | 0 | 2 | 1 | 14 | A | N | K | E | 0 | N | -1 | 13 | -1 | 25 | -1 | 18 | -1 | -1 | 25 | 25 | -1 | -1 | -1 | -1 | -1 | 18 | N | IL | 2013 | 8 | 1 | False | False | False | False | False | False | 14 | 0.9487 | 0.0006 | 1.3045 | 8 | 14 | 2 | 2 | 2 | 2 | 8 | 13 | 4 | 6 | 6 | 17 | 6 | 18 | 4 | 1 | 7 | 9 | 5 | 6 | 4 | 11 | 6 | 8 | 3 | 3 | 5 | 8 | 14 | 7 | 9 | 2 | 4 | 4 | 8 | 7 | 5 | 10 | 14 | 13 | 10 | 11 | 21 | 23 | 9 | 17 | 11 | 17 | 9 | 17 | 14 | 18 | 9 | 17 | 13 | 17 | 9 | 17 | 9 | 12 | 7 | 16 | 2 | 9 | 16 | 20 | 19 | 4 | 9 | 24 | 17 | 1 | 1 | 1 | 1 | 1 | 1 | 2 | 1 | 6 | 8 | 4 | 3 | 9 | 15 | 9 | 13 | 9 | 16 | 11 | 17 | 15 | 20 | 9 | 16 | 11 | 17 | 8 | 18 | 9 | 13 | 4 | 7 | 12 | 22 | 18 | 16 | 19 | 23 | 14 | 22 | 22 | 23 | 8 | 8 | 16 | 18 | 6 | 8 | 17 | 16 | 17 | 13 | 12 | 8 | 20 | 22 | 19 | 21 | 17 | 13 | 10 | 7 | 6 | 13 | 6 | 17 | 19 | 15 | 19 | 9 | 9 | 14 | 23 | 33 | 13 | 49660.999846 | 225.000001 | 1.376352e+09 | False | . len(dls.train)*bs, len(dls.valid)*val_bs . (194560, 65280) . roc_auc_binary = RocAucBinary() learn = tabular_learner(dls, metrics=roc_auc_binary) . type(roc_auc_binary) . fastai.metrics.AccumMetric . learn.lr_find() . SuggestedLRs(valley=tensor(0.0006)) . Reference to why we use fit_one_cycle . Note I ran fit_one_cycle with a value of 10 when prepping this notebook for publishing, but the test results came out suspiciously high on 1 outputs, given that the test submission I ran before was heavily weighted with 0 outputs and got a 0.83 score when I ran with 5 but I didn&#39;t set the random seed value then. What happened I think was changing the splitter, I got a different tensor output and I was looking at the alternate value column instead column with the prediction value. . learn.fit_one_cycle(epochs, lr, wd=wd) . epoch train_loss valid_loss roc_auc_score time . 0 | 0.279747 | 0.202921 | 0.954094 | 01:45 | . 1 | 0.188010 | 0.182800 | 0.960526 | 01:46 | . 2 | 0.170447 | 0.182175 | 0.960804 | 01:51 | . 3 | 0.147315 | 0.189143 | 0.958946 | 01:48 | . 4 | 0.112068 | 0.201419 | 0.956670 | 01:46 | . Referenced another Kaggle notebook for this, we don&#39;t need it but it&#39;s good to see what fastai metrics is actually packaging up for you . preds, targs = learn.get_preds() . preds[0:1][0][0], preds[0:1][0][1] . (tensor(0.9987), tensor(0.0013)) . Here was my mistake, I was looking at the wrong classifier value . len(preds) . 65189 . (preds[:][:][:,1] &gt;= 0.5).sum(), (preds[:][:][:,1] &lt; 0.5).sum() . (tensor(10698), tensor(54491)) . from sklearn.metrics import roc_auc_score valid_score = roc_auc_score(to_np(targs), to_np(preds[:][:][:,1])) valid_score . 0.9566703126953837 . Doing inferences based on this blog post from Walk With Fastai initially, but then experimenting to get this . dl_test = dls.test_dl(df_test) . preds, _ = learn.get_preds(dl=dl_test) . (preds[:][:][:,1] &gt;= 0.5).sum(), (preds[:][:][:,1] &lt; 0.5).sum() . (tensor(28902), tensor(144934)) . Submission To Kaggle . path.ls() . (#20) [Path(&#39;homesite-quote-conversion.zip&#39;),Path(&#39;models&#39;),Path(&#39;submission.csv&#39;),Path(&#39;submission2.csv&#39;),Path(&#39;submission3.csv&#39;),Path(&#39;submission4.csv&#39;),Path(&#39;submission5.csv&#39;),Path(&#39;submission6.csv&#39;),Path(&#39;submission7.csv&#39;),Path(&#39;submission8.csv&#39;)...] . len(df_test.index), len(preds[:][:][:,1]) . (173836, 173836) . preds[:1][:1] . tensor([[0.9976, 0.0024]]) . We want the 2nd value, this is what gives us our prediction value of how likely our model thinks this is going to be a sale . preds[:1][:1][:,1] . tensor([0.0024]) . submission = pd.DataFrame({&#39;QuoteNumber&#39;: df_test.index, &#39;QuoteConversion_Flag&#39;: preds[:][:][:,1].tolist()}, columns=[&#39;QuoteNumber&#39;, &#39;QuoteConversion_Flag&#39;]) . Played around with the example on list comprehension here to get it to work with what I had to work with . submission.QuoteConversion_Flag = [float(qcf) for qcf in submission.QuoteConversion_Flag] . submission.head() . QuoteNumber QuoteConversion_Flag . 0 3 | 0.002424 | . 1 5 | 0.012709 | . 2 7 | 0.005221 | . 3 9 | 0.001084 | . 4 10 | 0.571628 | . submission.QuoteConversion_Flag = round(submission.QuoteConversion_Flag).astype(&#39;Int64&#39;) . submission.head() . QuoteNumber QuoteConversion_Flag . 0 3 | 0 | . 1 5 | 0 | . 2 7 | 0 | . 3 9 | 0 | . 4 10 | 1 | . len(submission[submission.QuoteConversion_Flag==1]) . 28902 . len(submission[submission.QuoteConversion_Flag==0]) . 144934 . submission.to_csv(path/&#39;submission13.csv&#39;, index=False) . api.competition_submit(path/&#39;submission13.csv&#39;,message=&quot;Thirteenth pass&quot;, competition=&#39;homesite-quote-conversion&#39;) . 100%|██████████| 1.45M/1.45M [00:00&lt;00:00, 7.07MB/s] . Successfully submitted to Homesite Quote Conversion . learn.save(&#39;homesite_fastai_nn13&#39;) . Path(&#39;models/homesite_fastai_nn13.pth&#39;) .",
            "url": "https://redditech.github.io/team-fast-tabulous/kaggle/fastai/2021/06/27/Improving-Fastai-split-choices.html",
            "relUrl": "/kaggle/fastai/2021/06/27/Improving-Fastai-split-choices.html",
            "date": " • Jun 27, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Improving `cont_cat_split` in Fastai Tabular For Homesite Competition",
            "content": "Introduction . This is an additional modification of the &quot;first pass&quot; submission and subsequent additional work to the Homesite Competition on Kaggle Competition using Google Colab. Modification of some of the default parameters is done with some learning from the initial exploratory data analysis at this time, as well as other sources of readings. The major addition is listed below, but all are to see if I can improve the baseline after applying what we learnt so far to see how it improves (or not) our submission then. . Changes made: . Use a function to better split categorical and continuous fields, and set up the categories needed for training automatically | Included in the function is a new list triage which would require a manual analysis to determine a filling strategy that best suits modelling, or if to ignore the field for training | Changed from RandomSplitter() to TrainTestSplitter() for making test and validation sets more fairly weighted based on the bias of the input data towards negative results | Increase batch size to 1024 to make training shorter, but to still hopefully get a better predictor for it. Set a separate validator batch size to 128. | Increase the validation percentage to 0.25 | Fix the learning rate to 1e-2 | Increase epochs to 7, see if it overfits and what effect that has | Modified the cat_names and cont_names arrays with the initial insights from the EDA notebook post | Add a date part for dates | Add weight decay of 0.2 | . Old stuff, read about in other notebooks . Setup fastai and Google drive . !pip install -Uqq fastai . from fastai.tabular.all import * . The snippet below is only useful in Colab for accessing my Google Drive and is straight out the fastbook source code in Github . global gdrive gdrive = Path(&#39;/content/gdrive/My Drive&#39;) from google.colab import drive if not gdrive.exists(): drive.mount(str(gdrive.parent)) . Only add the Kaggle bits below if I&#39;m running locally, in Collab they&#39;re already here . . !ls /content/gdrive/MyDrive/Kaggle/kaggle.json . /content/gdrive/MyDrive/Kaggle/kaggle.json . Useful links here: . Documentation on Path library | Documentation on fastai extensions to Path library | . Path.cwd() . Path(&#39;/content&#39;) . Setup kaggle environment parameters . !mkdir -p ~/.kaggle !cp /content/gdrive/MyDrive/Kaggle/kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . from kaggle import api . path = Path.cwd() path.ls() . (#4) [Path(&#39;/content/.config&#39;),Path(&#39;/content/gdrive&#39;),Path(&#39;/content/models&#39;),Path(&#39;/content/sample_data&#39;)] . path = path/&quot;gdrive/MyDrive/Kaggle/homesite_data&quot; path.mkdir(exist_ok=True) Path.BASE_PATH = path api.competition_download_cli(&#39;homesite-quote-conversion&#39;, path=path) file_extract(path/&quot;homesite-quote-conversion.zip&quot;) file_extract(path/&quot;train.csv.zip&quot;) file_extract(path/&quot;test.csv.zip&quot;) . homesite-quote-conversion.zip: Skipping, found more recently modified local copy (use --force to force download) . path . Path(&#39;.&#39;) . path.ls() . (#20) [Path(&#39;homesite-quote-conversion.zip&#39;),Path(&#39;models&#39;),Path(&#39;submission.csv&#39;),Path(&#39;submission2.csv&#39;),Path(&#39;submission3.csv&#39;),Path(&#39;submission4.csv&#39;),Path(&#39;submission5.csv&#39;),Path(&#39;submission6.csv&#39;),Path(&#39;submission7.csv&#39;),Path(&#39;submission8.csv&#39;)...] . Exploring the Homesite data . Set the random seed so that the results are reproducible, set other parameters so changes can be made quickly. Trying to avoid &#39;magic numbers&#39;) where possible . set_seed(42) bs = 1024 val_bs = 128 test_size = 0.25 epochs = 5 lr = 0.0012 wd=0.2 . df_train = pd.read_csv(path/&quot;train.csv&quot;, low_memory=False) df_train.head() . QuoteNumber Original_Quote_Date QuoteConversion_Flag Field6 Field7 Field8 Field9 Field10 Field11 Field12 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField3 SalesField4 SalesField5 SalesField6 SalesField7 SalesField8 SalesField9 SalesField10 SalesField11 SalesField12 ... GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56A GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60A GeographicField60B GeographicField61A GeographicField61B GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 1 | 2013-08-16 | 0 | B | 23 | 0.9403 | 0.0006 | 965 | 1.0200 | N | 17 | 23 | 17 | 23 | 15 | 22 | 16 | 22 | 13 | 22 | 13 | 23 | T | D | 2 | 1 | 7 | 18 | 3 | 8 | 0 | 5 | 5 | 24 | V | 48649 | 0 | 0 | 0 | 0 | ... | 8 | 4 | 20 | 22 | 10 | 8 | 6 | 5 | 15 | 13 | 19 | 18 | 16 | 14 | 21 | 23 | 21 | 23 | 16 | 11 | 22 | 24 | 7 | 14 | -1 | 17 | 15 | 17 | 14 | 18 | 9 | 9 | -1 | 8 | -1 | 18 | -1 | 10 | N | CA | . 1 2 | 2014-04-22 | 0 | F | 7 | 1.0006 | 0.0040 | 548 | 1.2433 | N | 6 | 8 | 6 | 8 | 5 | 7 | 5 | 8 | 13 | 22 | 13 | 23 | T | E | 5 | 9 | 5 | 14 | 6 | 18 | 1 | 5 | 5 | 11 | P | 26778 | 0 | 0 | 1 | 1 | ... | 23 | 24 | 11 | 15 | 21 | 24 | 6 | 11 | 21 | 21 | 18 | 15 | 20 | 20 | 13 | 12 | 12 | 12 | 15 | 9 | 13 | 11 | 11 | 20 | -1 | 9 | 18 | 21 | 8 | 7 | 10 | 10 | -1 | 11 | -1 | 17 | -1 | 20 | N | NJ | . 2 4 | 2014-08-25 | 0 | F | 7 | 1.0006 | 0.0040 | 548 | 1.2433 | N | 7 | 12 | 7 | 12 | 6 | 10 | 7 | 11 | 25 | 25 | 13 | 23 | T | J | 4 | 6 | 3 | 10 | 4 | 11 | 1 | 5 | 5 | 11 | K | 8751 | 0 | 0 | 2 | 2 | ... | 21 | 22 | 24 | 25 | 20 | 22 | 7 | 13 | 23 | 23 | 20 | 19 | 20 | 20 | 18 | 20 | 19 | 21 | 20 | 19 | 11 | 8 | 3 | 3 | -1 | 5 | 21 | 24 | 12 | 15 | 15 | 18 | -1 | 21 | -1 | 11 | -1 | 8 | N | NJ | . 3 6 | 2013-04-15 | 0 | J | 10 | 0.9769 | 0.0004 | 1,165 | 1.2665 | N | 3 | 2 | 3 | 2 | 2 | 2 | 3 | 2 | 13 | 22 | 13 | 23 | Y | F | 15 | 23 | 8 | 19 | 14 | 24 | 0 | 5 | 5 | 23 | V | 43854 | 0 | 0 | 0 | 0 | ... | 3 | 1 | 14 | 22 | 6 | 2 | 7 | 14 | 11 | 8 | 19 | 18 | 18 | 16 | 13 | 12 | 13 | 12 | 17 | 13 | 5 | 2 | 3 | 4 | -1 | 7 | 14 | 14 | 14 | 18 | 6 | 5 | -1 | 10 | -1 | 9 | -1 | 21 | N | TX | . 4 8 | 2014-01-25 | 0 | E | 23 | 0.9472 | 0.0006 | 1,487 | 1.3045 | N | 8 | 13 | 8 | 13 | 7 | 11 | 7 | 13 | 13 | 22 | 13 | 23 | T | F | 4 | 6 | 3 | 6 | 3 | 6 | 1 | 5 | 5 | 7 | R | 12505 | 1 | 0 | 0 | 0 | ... | 24 | 25 | 9 | 11 | 25 | 25 | 5 | 3 | 22 | 22 | 21 | 21 | 17 | 15 | 25 | 25 | 25 | 25 | 17 | 13 | 13 | 11 | 3 | 4 | -1 | 7 | 11 | 9 | 10 | 10 | 18 | 22 | -1 | 10 | -1 | 11 | -1 | 12 | N | IL | . 5 rows × 299 columns . df_train.shape . (260753, 299) . df_test = pd.read_csv(path/&quot;test.csv&quot;, low_memory=False) df_test.head() . QuoteNumber Original_Quote_Date Field6 Field7 Field8 Field9 Field10 Field11 Field12 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField3 SalesField4 SalesField5 SalesField6 SalesField7 SalesField8 SalesField9 SalesField10 SalesField11 SalesField12 SalesField13 ... GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56A GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60A GeographicField60B GeographicField61A GeographicField61B GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 3 | 2014-08-12 | E | 16 | 0.9364 | 0.0006 | 1,487 | 1.3045 | N | 4 | 4 | 4 | 4 | 3 | 3 | 3 | 4 | 13 | 22 | 13 | 23 | Y | K | 13 | 22 | 6 | 16 | 9 | 21 | 0 | 5 | 5 | 11 | P | 67052 | 0 | 0 | 0 | 0 | 0 | ... | 22 | 23 | 9 | 12 | 25 | 25 | 6 | 9 | 4 | 2 | 16 | 12 | 20 | 20 | 2 | 2 | 2 | 1 | 1 | 1 | 10 | 7 | 25 | 25 | -1 | 19 | 19 | 22 | 12 | 15 | 1 | 1 | -1 | 1 | -1 | 20 | -1 | 25 | Y | IL | . 1 5 | 2013-09-07 | F | 11 | 0.9919 | 0.0038 | 564 | 1.1886 | N | 8 | 14 | 8 | 14 | 7 | 12 | 8 | 13 | 13 | 22 | 13 | 23 | T | E | 4 | 5 | 3 | 6 | 3 | 6 | 1 | 5 | 5 | 4 | R | 27288 | 1 | 0 | 0 | 0 | 0 | ... | 23 | 24 | 12 | 21 | 23 | 25 | 7 | 11 | 16 | 14 | 13 | 6 | 17 | 15 | 7 | 5 | 7 | 5 | 13 | 7 | 14 | 14 | 7 | 14 | -1 | 4 | 1 | 1 | 5 | 3 | 10 | 10 | -1 | 5 | -1 | 5 | -1 | 21 | N | NJ | . 2 7 | 2013-03-29 | F | 15 | 0.8945 | 0.0038 | 564 | 1.0670 | N | 11 | 18 | 11 | 18 | 10 | 16 | 10 | 18 | 13 | 22 | 13 | 23 | T | E | 3 | 3 | 5 | 14 | 3 | 9 | 1 | 5 | 5 | 23 | V | 65264 | 0 | 1 | 2 | 2 | 0 | ... | 16 | 18 | 9 | 10 | 14 | 16 | 6 | 8 | 20 | 19 | 17 | 14 | 16 | 13 | 20 | 22 | 20 | 22 | 20 | 18 | 10 | 7 | 4 | 7 | -1 | 11 | 13 | 12 | 18 | 22 | 10 | 11 | -1 | 20 | -1 | 22 | -1 | 11 | N | NJ | . 3 9 | 2015-03-21 | K | 21 | 0.8870 | 0.0004 | 1,113 | 1.2665 | Y | 14 | 22 | 15 | 22 | 13 | 20 | 22 | 25 | 13 | 22 | 13 | 23 | Y | F | 5 | 9 | 9 | 20 | 5 | 16 | 1 | 5 | 5 | 11 | R | 32725 | 1 | 1 | 1 | 1 | 0 | ... | 11 | 11 | 9 | 10 | 11 | 13 | 15 | 21 | 14 | 12 | 17 | 13 | 10 | 6 | 20 | 22 | 20 | 22 | 19 | 16 | 12 | 11 | 4 | 6 | -1 | 13 | 10 | 8 | 5 | 3 | 8 | 8 | -1 | 13 | -1 | 8 | -1 | 21 | N | TX | . 4 10 | 2014-12-10 | B | 25 | 0.9153 | 0.0007 | 935 | 1.0200 | N | 4 | 5 | 4 | 5 | 4 | 4 | 4 | 5 | 13 | 22 | 13 | 23 | Y | D | 12 | 21 | 1 | 1 | 3 | 6 | 0 | 5 | 5 | 11 | T | 56025 | 0 | 1 | 1 | 1 | 0 | ... | 9 | 8 | 25 | 25 | 9 | 3 | 9 | 18 | 7 | 4 | 16 | 12 | 13 | 9 | 8 | 6 | 8 | 6 | 11 | 5 | 19 | 21 | 13 | 21 | -1 | 23 | 11 | 8 | 5 | 3 | 7 | 7 | -1 | 3 | -1 | 22 | -1 | 21 | N | CA | . 5 rows × 298 columns . df_test.shape . (173836, 298) . y_column = df_train.columns.difference(df_test.columns) . y_column . Index([&#39;QuoteConversion_Flag&#39;], dtype=&#39;object&#39;) . From this it looks like QuoteConversion_Flag is the value we want to predict. Let&#39;s take a look at this . type(df_train.QuoteConversion_Flag) . pandas.core.series.Series . df_train.QuoteConversion_Flag.unique() . array([0, 1]) . type(df_train.QuoteConversion_Flag.unique()[0]) . numpy.int64 . Make this a boolean for the purpose of generating predictions as a binary classification . df_train.QuoteConversion_Flag = df_train.QuoteConversion_Flag.astype(dtype=&#39;boolean&#39;) . Let&#39;s see how the training data outcomes are balanced . df_train.QuoteConversion_Flag.describe() . count 260753 unique 2 top False freq 211859 Name: QuoteConversion_Flag, dtype: object . train_data_balance = pd.DataFrame(df_train[&quot;QuoteConversion_Flag&quot;]).groupby(&quot;QuoteConversion_Flag&quot;) . train_data_balance[&quot;QuoteConversion_Flag&quot;].describe() . count unique top freq . QuoteConversion_Flag . False 211859 | 1 | False | 211859 | . True 48894 | 1 | True | 48894 | . We have about 5 times as many &quot;No Sale&quot; data rows as we do data that shows a successful sale happened. This data bias may have an impact on the effectiveness of our model to predict positive sales results . First things first . Learning from my colleague Tim&#39;s work already we know: . Quotenumber is unique so we can make it the index | Original_Quote_Date column should be set as a date type | . Additionally, we should make sure to apply any changes to data types to both train and test data so predictions don&#39;t fail later on . df_train = df_train.set_index(&#39;QuoteNumber&#39;) df_test = df_test.set_index(&#39;QuoteNumber&#39;) . We may have some NaN values for Original_Quote_Date in either the training or test dataset, but let&#39;s confirm there are none. . df_train[&#39;Original_Quote_Date&#39;].isna().sum(), df_test[&#39;Original_Quote_Date&#39;].isna().sum() . (0, 0) . df_train[&#39;Original_Quote_Date&#39;] = pd.to_datetime(df_train[&#39;Original_Quote_Date&#39;]) df_test[&#39;Original_Quote_Date&#39;] = pd.to_datetime(df_test[&#39;Original_Quote_Date&#39;]) . Add the date_part to see if this helps improve modeling . df_train = add_datepart(df_train, &#39;Original_Quote_Date&#39;) df_test = add_datepart(df_test, &#39;Original_Quote_Date&#39;) . Goal: Automate the bits we can, continue refining fastai parameters, continue using EDA insights gathered to date . y_names = [y_column[0]] y_names . [&#39;QuoteConversion_Flag&#39;] . cont_names, cat_names = cont_cat_split(df_train, dep_var=y_names) len(cont_names), len(cat_names) . (155, 154) . # df_train.drop(&#39;PersonalField84&#39;, axis=1, inplace=True) . New Work - A function to improve fastai&#39;s cont_cat_split choices . First I&#39;ll create a triage list for any fields that can&#39;t be programmatically optimized. These are the ones we have to do manual steps for until I find a way to do better . triage = L() . Let&#39;s take a quick look at the descriptions of the categorical and continuous splits automatically done . df_train[cont_names].astype(&#39;object&#39;).describe() . Field7 Field8 Field9 Field11 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField6 SalesField8 SalesField12 PersonalField4A PersonalField4B PersonalField10A PersonalField10B PersonalField14 PersonalField15 PersonalField84 PropertyField1A PropertyField1B PropertyField2B PropertyField16A PropertyField16B PropertyField21A PropertyField21B PropertyField24A PropertyField24B PropertyField25 PropertyField26A PropertyField26B ... GeographicField42B GeographicField43A GeographicField43B GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60B GeographicField61B Original_Quote_Week Original_Quote_Day Original_Quote_Dayofyear Original_Quote_Elapsed . count 260753 | 260753.0000 | 260753.0000 | 260753.00 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 136545.0 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753.0 | 260753 | 260753 | ... | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 2.607530e+05 | . unique 28 | 38.0000 | 5.0000 | 11.00 | 26 | 26 | 25 | 25 | 25 | 25 | 25 | 25 | 26 | 26 | 25 | 25 | 26 | 26 | 24 | 61530 | 22 | 26 | 26 | 26 | 26 | 30 | 22 | 7.0 | 26 | 26 | 21 | 26 | 26 | 26 | 26 | 26 | 26 | 11.0 | 26 | 26 | ... | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 25 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 25 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 25 | 52 | 31 | 365 | 8.680000e+02 | . top 23 | 0.9403 | 0.0006 | 1.02 | 6 | 9 | 6 | 7 | 5 | 9 | 6 | 4 | 4 | 15 | 3 | 5 | 3 | 1 | 11 | 36088 | 0 | -1 | -1 | -1 | -1 | 2 | 24 | 2.0 | 4 | 20 | 15 | 3 | 1 | 6 | 3 | 7 | 4 | 1.0 | 9 | 10 | ... | 21 | 11 | 5 | 9 | 22 | 11 | 4 | 10 | 7 | 6 | 5 | 19 | 8 | 18 | 25 | 17 | 23 | 14 | 25 | 14 | 17 | 20 | 25 | 15 | 2 | 4 | 9 | 2 | 13 | 19 | 11 | 1 | 11 | 6 | 18 | 2 | 11 | 3 | 70 | 1.393805e+09 | . freq 66474 | 48161.0000 | 74096.0000 | 92359.00 | 24736 | 11089 | 25026 | 11137 | 23663 | 10881 | 24614 | 10756 | 28378 | 10923 | 50250 | 10581 | 59953 | 10487 | 89335 | 19 | 167612 | 24526 | 24526 | 54785 | 54785 | 85608 | 124015 | 134906.0 | 23038 | 12620 | 22598 | 30574 | 10571 | 24336 | 10548 | 20522 | 11055 | 121563.0 | 20538 | 11664 | ... | 10547 | 41899 | 10553 | 32537 | 10552 | 51208 | 10595 | 64704 | 10670 | 76413 | 18736 | 15213 | 10760 | 21011 | 10547 | 18093 | 10525 | 17007 | 10472 | 16667 | 10517 | 21298 | 10501 | 17134 | 10487 | 28655 | 10545 | 20725 | 22086 | 10625 | 19660 | 10570 | 17608 | 11216 | 10583 | 20717 | 7523 | 9314 | 1502 | 6.460000e+02 | . 4 rows × 155 columns . The first thing I notice, is that for a number of these fields, there is quite a low number of unique values. I also notice some, like PersonalField84 are missing quite a bit of data. These are a lot of fields to go through and manually recategorize and then get their categories, but I am hoping I can do this programmatically. . Let&#39;s have a look at the existing categoricals to make sure there&#39;s nothing suspeicious about these here either . df_train[cat_names].astype(&#39;object&#39;).describe() . Field6 Field10 Field12 CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 SalesField3 SalesField4 SalesField5 SalesField7 SalesField9 SalesField10 SalesField11 SalesField13 SalesField14 SalesField15 PersonalField1 PersonalField2 PersonalField5 PersonalField6 PersonalField7 PersonalField8 PersonalField9 PersonalField11 PersonalField12 PersonalField13 PersonalField16 PersonalField17 PersonalField18 PersonalField19 PersonalField22 PersonalField23 PersonalField24 PersonalField25 PersonalField26 PersonalField27 PersonalField28 ... PropertyField22 PropertyField23 PropertyField27 PropertyField28 PropertyField30 PropertyField31 PropertyField32 PropertyField33 PropertyField34 PropertyField35 PropertyField36 PropertyField37 PropertyField38 GeographicField5A GeographicField5B GeographicField10A GeographicField10B GeographicField14A GeographicField14B GeographicField18A GeographicField21A GeographicField22A GeographicField22B GeographicField23A GeographicField56A GeographicField60A GeographicField61A GeographicField62A GeographicField62B GeographicField63 GeographicField64 Original_Quote_Year Original_Quote_Month Original_Quote_Dayofweek Original_Quote_Is_month_end Original_Quote_Is_month_start Original_Quote_Is_quarter_end Original_Quote_Is_quarter_start Original_Quote_Is_year_end Original_Quote_Is_year_start . count 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260640 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | ... | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260683 | 260753 | 260683 | 260753 | 260640 | 260753 | 259533 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | 260753 | . unique 8 | 8 | 2 | 3 | 4 | 3 | 4 | 7 | 12 | 2 | 5 | 5 | 7 | 2 | 19 | 20 | 8 | 12 | 12 | 2 | 2 | 9 | 2 | 2 | 3 | 3 | 5 | 5 | 4 | 50 | 66 | 61 | 57 | 7 | 13 | 14 | 14 | 14 | 17 | 7 | ... | 5 | 13 | 17 | 4 | 2 | 4 | 2 | 4 | 2 | 3 | 2 | 2 | 2 | 2 | 14 | 1 | 2 | 2 | 20 | 2 | 2 | 2 | 12 | 2 | 2 | 2 | 2 | 2 | 19 | 3 | 4 | 3 | 12 | 7 | 2 | 2 | 2 | 2 | 2 | 2 | . top B | 935 | N | 13 | 22 | 13 | 23 | Y | E | 1 | 5 | 5 | K | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | N | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 1 | ... | 2 | 1 | 4 | B | N | O | Y | H | Y | 2 | N | N | N | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 8 | N | CA | 2014 | 3 | 0 | False | False | False | False | False | False | . freq 94694 | 52353 | 241867 | 212390 | 212390 | 170589 | 170589 | 143152 | 85017 | 194250 | 158481 | 147571 | 50261 | 222421 | 200758 | 170439 | 246479 | 240315 | 239868 | 189441 | 182307 | 164202 | 164202 | 259379 | 260715 | 190879 | 254940 | 160257 | 258621 | 124015 | 124015 | 124015 | 124015 | 254447 | 256107 | 255215 | 254447 | 253407 | 129036 | 134167 | ... | 259655 | 205119 | 92797 | 243374 | 245807 | 156042 | 191627 | 123098 | 141185 | 166293 | 248302 | 185976 | 254032 | 254208 | 135320 | 260753 | 260751 | 253814 | 71697 | 254135 | 254285 | 254158 | 156405 | 254096 | 254153 | 254239 | 254170 | 254144 | 83404 | 254712 | 94725 | 110107 | 31742 | 49924 | 252323 | 252730 | 257614 | 257381 | 260240 | 260404 | . 4 rows × 154 columns . So there may be some fields with missing data here, like PropertyField38 we will have to look at a strategy for these too . Question for later:Should those with only two categories be mapped as Categorical or Booleans? Would there be any impact here? See an example field below . field = &quot;Field12&quot; df_train[field].unique() . array([&#39;N&#39;, &#39;Y&#39;], dtype=object) . Here I define two functions, which will help . to reset the cont_names and cat_names arrays with better fits of the actual data fields for those that are categorical, but were put into the continuous array. | For all cateogircal fields, it will also setup the categories for these fields and change their dtype | For any fields that have null values, it will remove them from their respective field, and place them in the triage list | . def reassign_to_categorical(field, df, continuous, categorical, triage): if df[field].isna().sum()==0: field_categories = df[field].unique() df[field] = df[field].astype(&#39;category&#39;) df[field].cat.set_categories(field_categories, inplace=True) if field in continuous: continuous.remove(field) if field not in categorical: categorical.append(field) else: if field in continuous: continuous.remove(field) if field in categorical: categorical.remove(field) triage.append(field) return df, continuous, categorical, triage . def categorize( df, cont_names, cat_names, triage, category_threshold): for field in df.columns: if ((len(df[field].unique()) &lt;= category_threshold) and (type(df[field].dtype) != pd.core.dtypes.dtypes.CategoricalDtype)): reassign_to_categorical(field, df, cont_names, cat_names, triage) return df, cont_names, cat_names, triage . field = &#39;Field8&#39; df_train[field].unique() . array([0.9403, 1.0006, 0.9769, 0.9472, 0.9258, 0.9153, 0.9691, 0.9919, 0.9497, 0.9893, 0.8793, 0.9485, 0.8922, 1.0101, 0.893 , 0.9219, 0.9392, 0.9685, 0.887 , 0.8746, 0.8928, 0.9838, 1.0005, 0.9487, 0.9313, 0.9525, 0.9559, 0.9482, 0.9223, 0.9566, 0.9108, 0.8945, 0.9023, 0.9489, 0.9194, 0.9364, 0.9368, 0.9375]) . df_train, cont_names, cat_names, triage = categorize(df_train, cont_names, cat_names, triage, 100) . len(cont_names), len(cat_names) . (3, 298) . So this is a big rebalancing between continuous and categorical fields. I saved alot of time with this function rather than doing this manually like I had for my initial data exploration. Let&#39;s take a look at how many came up for triaging still though . triage . (#9) [&#39;PersonalField7&#39;,&#39;PersonalField84&#39;,&#39;PropertyField3&#39;,&#39;PropertyField4&#39;,&#39;PropertyField29&#39;,&#39;PropertyField32&#39;,&#39;PropertyField34&#39;,&#39;PropertyField36&#39;,&#39;PropertyField38&#39;] . And let&#39;s look again at Field8 to see how it did the categorizations for me . field = &#39;Field8&#39; df_train[field].unique() . [0.9403, 1.0006, 0.9769, 0.9472, 0.9258, ..., 0.9489, 0.9194, 0.9364, 0.9368, 0.9375] Length: 38 Categories (38, float64): [0.9403, 1.0006, 0.9769, 0.9472, ..., 0.9194, 0.9364, 0.9368, 0.9375] . ToDo: Put in bits that triage those 9 fields that popped up. For now I will run the modelling ignoring these fields and see if any improvements happen. . Triaging . triage . (#9) [&#39;PersonalField7&#39;,&#39;PersonalField84&#39;,&#39;PropertyField3&#39;,&#39;PropertyField4&#39;,&#39;PropertyField29&#39;,&#39;PropertyField32&#39;,&#39;PropertyField34&#39;,&#39;PropertyField36&#39;,&#39;PropertyField38&#39;] . PersonalField7 . field = &#39;PersonalField7&#39; True in df_train[field].isna() . True . df_train[field].value_counts() . N 259379 Y 1261 Name: PersonalField7, dtype: int64 . Back to the training pipeline I had originally . . &quot;QuoteConversion_Flag&quot; in cont_names, &quot;QuoteConversion_Flag&quot; in cat_names #Make sure we&#39;ve gotten our y-column excluded . (False, True) . if (y_column in cont_names): cont_names.remove(y_column) if (y_column in cat_names): cat_names.remove(y_column) . procs = [Categorify, FillMissing, Normalize] splits = TrainTestSplitter(test_size=test_size, stratify=df_train[y_names])(df_train) . to = TabularPandas(df=df_train, procs=procs, cat_names=cat_names, cont_names=cont_names, y_names=y_names,splits=splits) dls = to.dataloaders(bs=bs, val_bs=val_bs) dls.valid.show_batch() . Field6 Field10 Field12 CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 SalesField3 SalesField4 SalesField5 SalesField7 SalesField9 SalesField10 SalesField11 SalesField13 SalesField14 SalesField15 PersonalField1 PersonalField2 PersonalField5 PersonalField6 PersonalField8 PersonalField9 PersonalField11 PersonalField12 PersonalField13 PersonalField16 PersonalField17 PersonalField18 PersonalField19 PersonalField22 PersonalField23 PersonalField24 PersonalField25 PersonalField26 PersonalField27 PersonalField28 PersonalField29 PersonalField30 PersonalField31 PersonalField32 PersonalField33 PersonalField34 PersonalField35 PersonalField36 PersonalField37 PersonalField38 PersonalField39 PersonalField40 PersonalField41 PersonalField42 PersonalField43 PersonalField44 PersonalField45 PersonalField46 PersonalField47 PersonalField48 PersonalField49 PersonalField50 PersonalField51 PersonalField52 PersonalField53 PersonalField54 PersonalField55 PersonalField56 PersonalField57 PersonalField58 PersonalField59 PersonalField60 PersonalField61 PersonalField62 PersonalField63 PersonalField64 PersonalField65 PersonalField66 PersonalField67 PersonalField68 PersonalField69 PersonalField70 PersonalField71 PersonalField72 PersonalField73 PersonalField74 PersonalField75 PersonalField76 PersonalField77 PersonalField78 PersonalField79 PersonalField80 PersonalField81 PersonalField82 PersonalField83 PropertyField2A PropertyField5 PropertyField6 PropertyField7 PropertyField8 PropertyField9 PropertyField10 PropertyField11A PropertyField11B PropertyField12 PropertyField13 PropertyField14 PropertyField15 PropertyField17 PropertyField18 PropertyField19 PropertyField20 PropertyField22 PropertyField23 PropertyField27 PropertyField28 PropertyField30 PropertyField31 PropertyField33 PropertyField35 PropertyField37 GeographicField5A GeographicField5B GeographicField10A GeographicField10B GeographicField14A GeographicField14B GeographicField18A GeographicField21A GeographicField22A GeographicField22B GeographicField23A GeographicField56A GeographicField60A GeographicField61A GeographicField62A GeographicField62B GeographicField63 GeographicField64 Original_Quote_Year Original_Quote_Month Original_Quote_Dayofweek Original_Quote_Is_month_end Original_Quote_Is_month_start Original_Quote_Is_quarter_end Original_Quote_Is_quarter_start Original_Quote_Is_year_end Original_Quote_Is_year_start Field7 Field8 Field9 Field11 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField6 SalesField12 PersonalField4A PersonalField4B PersonalField10A PersonalField10B PersonalField14 PersonalField15 PropertyField1A PropertyField1B PropertyField2B PropertyField16A PropertyField16B PropertyField21A PropertyField21B PropertyField24A PropertyField24B PropertyField25 PropertyField26A PropertyField26B PropertyField39A PropertyField39B GeographicField1A GeographicField1B GeographicField2A GeographicField2B GeographicField3A GeographicField3B GeographicField4A GeographicField4B GeographicField6A GeographicField6B GeographicField7A GeographicField7B GeographicField8A GeographicField8B GeographicField9A GeographicField9B GeographicField11A GeographicField11B GeographicField12A GeographicField12B GeographicField13A GeographicField13B GeographicField15A GeographicField15B GeographicField16A GeographicField16B GeographicField17A GeographicField17B GeographicField18B GeographicField19A GeographicField19B GeographicField20A GeographicField20B GeographicField21B GeographicField23B GeographicField24A GeographicField24B GeographicField25A GeographicField25B GeographicField26A GeographicField26B GeographicField27A GeographicField27B GeographicField28A GeographicField28B GeographicField29A GeographicField29B GeographicField30A GeographicField30B GeographicField31A GeographicField31B GeographicField32A GeographicField32B GeographicField33A GeographicField33B GeographicField34A GeographicField34B GeographicField35A GeographicField35B GeographicField36A GeographicField36B GeographicField37A GeographicField37B GeographicField38A GeographicField38B GeographicField39A GeographicField39B GeographicField40A GeographicField40B GeographicField41A GeographicField41B GeographicField42A GeographicField42B GeographicField43A GeographicField43B GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60B GeographicField61B Original_Quote_Week Original_Quote_Day SalesField8 Original_Quote_Dayofyear Original_Quote_Elapsed QuoteConversion_Flag . 0 B | 935 | N | 13 | 22 | 13 | 23 | T | D | 1 | 5 | 5 | K | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | O | 0 | 0 | 1 | -1 | 21 | 4 | 3 | C | 4 | 1 | 3 | 0 | 0 | 2 | 2 | 4 | B | N | N | G | 1 | N | -1 | 13 | -1 | 25 | -1 | 9 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 12 | N | CA | 2014 | 2 | 0 | False | False | False | False | False | False | 23 | 0.9403 | 0.0007 | 1.02 | 25 | 25 | 25 | 25 | 22 | 25 | 24 | 25 | 1 | 1 | 8 | 19 | 2 | 4 | 11 | 1 | 3 | 3 | 9 | 18 | 4 | 24 | 4 | 6 | 17 | 10 | 19 | 25 | 25 | 18 | 23 | 3 | 7 | 8 | 3 | 2 | 7 | 15 | 11 | 9 | 11 | 13 | 25 | 25 | 2 | 9 | 6 | 7 | 2 | 7 | 2 | 5 | 2 | 9 | 4 | 7 | 2 | 8 | 8 | 7 | 3 | 7 | 10 | 21 | 9 | 2 | 3 | 3 | 6 | 4 | 2 | 13 | 8 | 16 | 15 | 16 | 20 | 15 | 11 | 6 | 9 | 6 | 6 | 5 | 4 | 7 | 8 | 16 | 22 | 14 | 21 | 6 | 5 | 6 | 9 | 5 | 4 | 7 | 17 | 12 | 19 | 8 | 16 | 3 | 8 | 2 | 1 | 5 | 3 | 10 | 11 | 15 | 17 | 2 | 4 | 10 | 12 | 21 | 23 | 13 | 10 | 14 | 7 | 6 | 3 | 19 | 21 | 18 | 20 | 14 | 8 | 21 | 23 | 12 | 20 | 21 | 6 | 3 | 6 | 4 | 9 | 9 | 16 | 12 | 7 | 10 | 42316.000014 | 40.999994 | 1.391990e+09 | False | . 1 B | 965 | N | 13 | 22 | 13 | 23 | T | D | 1 | 5 | 5 | Q | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 6 | 1 | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | O | 0 | 0 | 1 | -1 | 21 | 4 | 2 | C | 4 | 1 | 3 | 0 | 0 | 2 | 4 | 10 | B | N | N | E | 1 | N | -1 | 13 | -1 | 25 | -1 | 8 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 24 | N | CA | 2013 | 11 | 3 | False | False | False | False | False | False | 23 | 0.9403 | 0.0006 | 1.02 | 24 | 25 | 24 | 25 | 21 | 24 | 23 | 25 | 1 | 1 | 12 | 22 | 4 | 13 | 7 | 1 | 19 | 23 | 16 | 21 | 4 | 24 | 17 | 22 | 4 | 9 | 18 | 24 | 25 | 17 | 22 | 1 | 25 | 25 | 3 | 2 | 11 | 21 | 12 | 11 | 4 | 3 | 11 | 10 | 2 | 10 | 2 | 3 | 2 | 4 | 2 | 7 | 2 | 10 | 2 | 4 | 2 | 7 | 2 | 2 | 2 | 2 | 21 | 25 | 24 | 2 | 4 | 3 | 7 | 4 | 5 | 13 | 8 | 16 | 15 | 16 | 20 | 15 | 11 | 13 | 20 | 20 | 24 | 11 | 20 | 16 | 23 | 7 | 11 | 20 | 24 | 13 | 17 | 17 | 24 | 14 | 22 | 10 | 21 | 10 | 15 | 8 | 15 | 3 | 8 | 1 | 1 | 7 | 8 | 7 | 6 | 15 | 16 | 2 | 2 | 9 | 4 | 7 | 14 | 8 | 5 | 12 | 5 | 12 | 8 | 4 | 3 | 4 | 3 | 9 | 4 | 12 | 10 | 18 | 23 | 21 | 16 | 19 | 16 | 21 | 5 | 3 | 6 | 17 | 46 | 14 | 59943.999147 | 317.999998 | 1.384387e+09 | False | . 2 B | 965 | N | 13 | 22 | 13 | 23 | X | D | 1 | 5 | 5 | Q | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | 1 | 2 | 0 | 3 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | O | 1 | 0 | 2 | -1 | 21 | 4 | 2 | C | 1 | 1 | 2 | 0 | 0 | 2 | 4 | 10 | B | N | N | H | 1 | N | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 12 | N | CA | 2013 | 11 | 1 | False | False | False | False | False | False | 25 | 0.9403 | 0.0006 | 1.02 | 14 | 21 | 14 | 21 | 12 | 19 | 13 | 20 | 14 | 22 | 4 | 11 | 2 | 3 | 20 | 0 | 6 | 8 | 5 | 4 | 5 | 24 | 6 | 9 | 22 | 4 | 9 | 14 | 21 | 9 | 14 | 1 | 16 | 22 | 4 | 2 | 7 | 14 | 8 | 5 | 8 | 8 | 20 | 22 | 2 | 4 | 2 | 2 | 2 | 2 | 2 | 3 | 2 | 7 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 18 | 24 | 21 | 2 | 2 | 2 | 3 | 11 | 5 | 15 | 11 | 19 | 20 | 20 | 22 | 17 | 16 | 6 | 10 | 11 | 17 | 23 | 25 | 3 | 2 | 13 | 21 | 19 | 24 | 6 | 5 | 24 | 25 | 6 | 6 | 7 | 17 | 18 | 24 | 8 | 16 | 2 | 3 | 3 | 4 | 4 | 3 | 8 | 8 | 10 | 9 | 3 | 5 | 10 | 8 | 25 | 25 | 8 | 5 | 14 | 8 | 12 | 7 | 12 | 11 | 11 | 10 | 7 | 3 | 20 | 22 | 16 | 23 | 19 | 9 | 6 | 22 | 24 | 6 | 5 | 7 | 17 | 45 | 5 | 3553.999261 | 308.999998 | 1.383610e+09 | True | . 3 C | 1,487 | Y | 13 | 22 | 25 | 25 | V | A | 1 | 5 | 5 | P | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | 1 | 2 | 0 | 1 | 2 | XR | XS | YP | XC | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | A | 1 | 0 | 1 | -1 | 25 | 1 | 3 | C | 5 | 1 | 3 | 1 | 0 | 2 | 13 | 7 | B | Y | K | E | 0 | N | -1 | 13 | -1 | 25 | -1 | 20 | -1 | 25 | -1 | 23 | -1 | -1 | -1 | -1 | -1 | 8 | N | IL | 2015 | 5 | 6 | False | False | False | False | False | False | 17 | 0.8746 | 0.0006 | 1.3045 | 25 | 25 | 25 | 25 | 25 | 25 | 25 | 25 | 12 | 21 | 7 | 18 | 1 | 1 | 11 | 0 | 16 | 20 | 8 | 18 | 3 | 4 | 6 | 9 | 23 | 12 | 21 | 25 | 25 | 23 | 25 | 2 | 16 | 21 | 2 | 1 | 3 | 7 | 16 | 16 | 24 | 25 | 19 | 21 | 10 | 18 | 12 | 17 | 10 | 18 | 18 | 21 | 9 | 18 | 13 | 17 | 9 | 18 | 10 | 12 | 9 | 18 | 2 | 9 | 19 | 20 | 19 | 5 | 16 | 25 | 16 | 5 | 2 | 5 | 2 | 3 | 3 | 6 | 2 | 4 | 4 | 7 | 8 | 7 | 10 | 7 | 8 | 10 | 17 | 13 | 20 | 10 | 12 | 14 | 22 | 9 | 13 | 5 | 12 | 7 | 8 | 6 | 11 | 5 | 10 | 16 | 14 | 9 | 10 | 10 | 13 | 19 | 20 | 10 | 13 | 13 | 15 | 6 | 7 | 20 | 20 | 20 | 20 | 17 | 15 | 21 | 23 | 21 | 23 | 20 | 19 | 19 | 21 | 5 | 9 | 9 | 22 | 24 | 14 | 17 | 13 | 16 | 24 | 2 | 19 | 10 | 7220.998942 | 130.000000 | 1.431216e+09 | False | . 4 F | 548 | N | 13 | 22 | 13 | 23 | T | E | 1 | 5 | 5 | Q | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | 1 | 2 | 0 | 1 | 2 | XH | XG | XF | ZT | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | R | 1 | 0 | 1 | -1 | 21 | 2 | 2 | C | 4 | 0 | 1 | 0 | 0 | 2 | 1 | 10 | B | N | O | H | 1 | N | -1 | 22 | -1 | 25 | -1 | 18 | -1 | -1 | -1 | 16 | -1 | -1 | -1 | -1 | -1 | 17 | N | NJ | 2014 | 3 | 3 | False | False | False | False | False | False | 11 | 0.9566 | 0.004 | 1.1886 | 3 | 2 | 3 | 3 | 2 | 2 | 3 | 2 | 7 | 14 | 4 | 11 | 7 | 20 | 11 | 0 | 8 | 10 | 11 | 19 | 5 | 10 | 8 | 12 | 15 | 4 | 6 | 3 | 2 | 3 | 2 | 1 | 7 | 9 | 12 | 13 | 3 | 7 | 8 | 6 | 8 | 8 | 10 | 9 | 5 | 15 | 11 | 16 | 5 | 15 | 6 | 12 | 4 | 10 | 10 | 16 | 4 | 10 | 12 | 19 | 8 | 17 | 2 | 8 | 11 | 24 | 25 | 25 | 25 | 16 | 18 | 25 | 25 | 24 | 25 | 16 | 18 | 25 | 25 | 1 | 1 | 4 | 3 | 5 | 5 | 8 | 12 | 4 | 4 | 5 | 5 | 7 | 6 | 11 | 19 | 5 | 4 | 6 | 15 | 3 | 2 | 7 | 13 | 9 | 14 | 24 | 24 | 12 | 15 | 10 | 13 | 12 | 12 | 8 | 8 | 10 | 11 | 7 | 11 | 10 | 7 | 5 | 2 | 3 | 1 | 12 | 11 | 10 | 9 | 6 | 2 | 20 | 22 | 12 | 20 | 18 | 9 | 5 | 8 | 7 | 13 | 16 | 5 | 11 | 11 | 13 | 64502.998849 | 71.999999 | 1.394669e+09 | False | . 5 B | 965 | N | 13 | 22 | 13 | 23 | T | D | 0 | 5 | 5 | T | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | 1 | 3 | 0 | 5 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | O | 1 | 0 | 1 | -1 | 21 | 2 | 1 | A | 1 | 0 | 1 | 0 | 0 | 2 | 1 | 10 | B | N | O | G | 2 | Y | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 19 | N | CA | 2013 | 9 | 3 | False | False | False | False | False | False | 24 | 0.9403 | 0.0006 | 1.02 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 10 | 20 | 2 | 1 | 5 | 17 | 4 | 1 | 1 | 1 | -1 | -1 | 4 | 24 | 19 | 23 | 15 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 4 | 4 | 12 | 15 | 9 | 18 | 10 | 8 | 2 | 1 | 17 | 19 | 2 | 3 | 7 | 8 | 2 | 8 | 2 | 5 | 2 | 4 | 5 | 8 | 2 | 8 | 9 | 12 | 3 | 8 | 2 | 16 | 9 | 4 | 8 | 6 | 16 | 4 | 9 | 25 | 25 | 25 | 25 | 25 | 25 | 24 | 25 | 9 | 16 | 10 | 14 | 13 | 23 | 7 | 10 | 6 | 9 | 7 | 8 | 16 | 21 | 2 | 2 | 14 | 21 | 7 | 16 | 13 | 21 | 2 | 2 | 9 | 14 | 18 | 17 | 10 | 11 | 6 | 6 | 12 | 12 | 25 | 25 | 10 | 9 | 5 | 3 | 8 | 5 | 12 | 5 | 12 | 8 | 7 | 5 | 7 | 5 | 13 | 7 | 16 | 16 | 11 | 19 | 16 | 15 | 17 | 1 | 1 | 5 | 4 | 3 | 24 | 37 | 12 | 37300.000122 | 255.000001 | 1.378944e+09 | False | . 6 B | 965 | N | 13 | 22 | 13 | 23 | T | D | 1 | 5 | 5 | T | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 3 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 3 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 3 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | O | 0 | 0 | 1 | -1 | 21 | 2 | 2 | C | 1 | 1 | 2 | 0 | 0 | 2 | 1 | 10 | B | N | N | G | 1 | N | -1 | 13 | -1 | 25 | -1 | 8 | -1 | -1 | -1 | 15 | -1 | -1 | 25 | -1 | -1 | 9 | N | CA | 2013 | 3 | 2 | False | False | False | False | False | False | 23 | 0.9403 | 0.0006 | 1.02 | 9 | 15 | 9 | 15 | 8 | 13 | 8 | 14 | 3 | 5 | 6 | 17 | 5 | 17 | 23 | 0 | 11 | 14 | 8 | 17 | 1 | 24 | 15 | 20 | 8 | 15 | 23 | 9 | 15 | 9 | 14 | 1 | 16 | 22 | 10 | 11 | 3 | 6 | 23 | 24 | 12 | 14 | 11 | 9 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 3 | 2 | 4 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 14 | 23 | 22 | 2 | 4 | 3 | 8 | 11 | 3 | 15 | 11 | 19 | 20 | 20 | 22 | 17 | 16 | 18 | 23 | 25 | 25 | 7 | 9 | 21 | 24 | 19 | 24 | 25 | 25 | 23 | 25 | 25 | 25 | 13 | 21 | 11 | 22 | 13 | 20 | 22 | 25 | 2 | 3 | 3 | 5 | 8 | 9 | 14 | 22 | 14 | 15 | 2 | 3 | 8 | 3 | 24 | 25 | 23 | 23 | 22 | 23 | 23 | 24 | 17 | 18 | 18 | 19 | 22 | 21 | 14 | 13 | 3 | 5 | 4 | 10 | 7 | 12 | 14 | 17 | 21 | 25 | 4 | 11 | 13 | 48178.000355 | 71.999999 | 1.363133e+09 | False | . 7 J | 1,113 | N | 13 | 22 | 13 | 23 | Y | K | 1 | 5 | 5 | Q | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 6 | 1 | 1 | 2 | 0 | 1 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | J | 0 | 0 | 1 | -1 | 21 | 4 | 2 | C | 2 | 0 | 2 | 0 | 0 | 2 | 1 | 10 | B | N | N | G | 0 | N | -1 | 23 | -1 | 25 | -1 | 10 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 16 | N | TX | 2014 | 7 | 3 | False | False | False | False | False | False | 23 | 0.8928 | 0.0004 | 1.2665 | 6 | 8 | 6 | 8 | 5 | 7 | 5 | 8 | 10 | 19 | 19 | 24 | 23 | 25 | 11 | 1 | 7 | 9 | 25 | 25 | 4 | 24 | 6 | 8 | 3 | 3 | 3 | 6 | 8 | 8 | 12 | 1 | 14 | 20 | 15 | 19 | 4 | 7 | 6 | 3 | 10 | 11 | 14 | 15 | 3 | 10 | 23 | 23 | 6 | 16 | 6 | 14 | 3 | 10 | 23 | 22 | 5 | 16 | 24 | 23 | 7 | 15 | 2 | 9 | 5 | 6 | 9 | 7 | 18 | 5 | 10 | 21 | 23 | 19 | 21 | 10 | 13 | 24 | 25 | 6 | 10 | 3 | 2 | 3 | 2 | 2 | 2 | 11 | 18 | 6 | 7 | 6 | 4 | 4 | 3 | 3 | 2 | 6 | 13 | 2 | 1 | 2 | 1 | 16 | 23 | 19 | 18 | 6 | 5 | 15 | 22 | 9 | 6 | 1 | 1 | 20 | 22 | 6 | 5 | 9 | 6 | 21 | 21 | 19 | 19 | 10 | 9 | 11 | 10 | 24 | 25 | 5 | 3 | 6 | 12 | 20 | 17 | 20 | 18 | 22 | 7 | 7 | 9 | 4 | 29 | 17 | 60277.000961 | 198.000000 | 1.405555e+09 | False | . 8 B | 935 | N | 13 | 22 | 1 | 6 | T | J | 0 | 4 | 3 | P | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 6 | 1 | 1 | 2 | 0 | 3 | 2 | ZA | ZE | XR | XD | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | O | 1 | 0 | 1 | -1 | 21 | 2 | 2 | A | 4 | 1 | 3 | 0 | 0 | 2 | 1 | 10 | B | N | N | G | 1 | N | -1 | 13 | -1 | 25 | -1 | 7 | -1 | -1 | -1 | 15 | -1 | -1 | -1 | -1 | -1 | 16 | N | CA | 2014 | 1 | 1 | False | False | False | False | False | False | 25 | 0.9403 | 0.0007 | 1.02 | 7 | 11 | 7 | 11 | 6 | 9 | 6 | 10 | 4 | 7 | 5 | 16 | 6 | 18 | 20 | 0 | 16 | 20 | 4 | 3 | 1 | 24 | 16 | 21 | 20 | 9 | 18 | 7 | 11 | 7 | 10 | 1 | 13 | 19 | 10 | 12 | 12 | 22 | 12 | 11 | 4 | 3 | 24 | 25 | 2 | 3 | 4 | 5 | 2 | 5 | 2 | 2 | 2 | 6 | 3 | 6 | 2 | 6 | 5 | 5 | 2 | 5 | 15 | 23 | 24 | 2 | 2 | 2 | 4 | 7 | 2 | 13 | 8 | 14 | 11 | 11 | 16 | 14 | 9 | 11 | 19 | 10 | 15 | 10 | 18 | 21 | 24 | 8 | 13 | 12 | 19 | 14 | 18 | 15 | 23 | 17 | 23 | 5 | 12 | 12 | 18 | 4 | 6 | 3 | 7 | 2 | 3 | 6 | 6 | 8 | 8 | 14 | 16 | 2 | 4 | 10 | 10 | 21 | 23 | 13 | 10 | 16 | 11 | 16 | 14 | 9 | 7 | 9 | 7 | 14 | 9 | 17 | 18 | 13 | 21 | 21 | 13 | 12 | 20 | 24 | 1 | 1 | 6 | 6 | 3 | 14 | 16438.000261 | 14.000005 | 1.389658e+09 | False | . 9 E | 1,480 | N | 13 | 22 | 13 | 23 | T | F | 1 | 5 | 5 | P | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 7 | 0 | 1 | 2 | 0 | 1 | 2 | ZG | ZF | ZN | ZQ | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | -1 | Y | 0 | R | 1 | 0 | 1 | -1 | 23 | 3 | 2 | C | 4 | 0 | 2 | 1 | 0 | 2 | 1 | 14 | A | N | K | E | 0 | N | -1 | 13 | -1 | 25 | -1 | 18 | -1 | -1 | 25 | 25 | -1 | -1 | -1 | -1 | -1 | 18 | N | IL | 2013 | 8 | 1 | False | False | False | False | False | False | 14 | 0.9487 | 0.0006 | 1.3045 | 8 | 14 | 2 | 2 | 2 | 2 | 8 | 13 | 4 | 6 | 6 | 17 | 6 | 18 | 4 | 1 | 7 | 9 | 5 | 6 | 4 | 11 | 6 | 8 | 3 | 3 | 5 | 8 | 14 | 7 | 9 | 2 | 4 | 4 | 8 | 7 | 5 | 10 | 14 | 13 | 10 | 11 | 21 | 23 | 9 | 17 | 11 | 17 | 9 | 17 | 14 | 18 | 9 | 17 | 13 | 17 | 9 | 17 | 9 | 12 | 7 | 16 | 2 | 9 | 16 | 20 | 19 | 4 | 9 | 24 | 17 | 1 | 1 | 1 | 1 | 1 | 1 | 2 | 1 | 6 | 8 | 4 | 3 | 9 | 15 | 9 | 13 | 9 | 16 | 11 | 17 | 15 | 20 | 9 | 16 | 11 | 17 | 8 | 18 | 9 | 13 | 4 | 7 | 12 | 22 | 18 | 16 | 19 | 23 | 14 | 22 | 22 | 23 | 8 | 8 | 16 | 18 | 6 | 8 | 17 | 16 | 17 | 13 | 12 | 8 | 20 | 22 | 19 | 21 | 17 | 13 | 10 | 7 | 6 | 13 | 6 | 17 | 19 | 15 | 19 | 9 | 9 | 14 | 23 | 33 | 13 | 49660.999846 | 225.000001 | 1.376352e+09 | False | . len(dls.train)*bs, len(dls.valid)*val_bs . (194560, 65280) . roc_auc_binary = RocAucBinary() learn = tabular_learner(dls, metrics=roc_auc_binary) . type(roc_auc_binary) . fastai.metrics.AccumMetric . learn.lr_find() . SuggestedLRs(valley=tensor(0.0006)) . Reference to why we use fit_one_cycle . Note I ran fit_one_cycle with a value of 10 when prepping this notebook for publishing, but the test results came out suspiciously high on 1 outputs, given that the test submission I ran before was heavily weighted with 0 outputs and got a 0.83 score when I ran with 5 but I didn&#39;t set the random seed value then. What happened I think was changing the splitter, I got a different tensor output and I was looking at the alternate value column instead column with the prediction value. . learn.fit_one_cycle(epochs, lr, wd=wd) . epoch train_loss valid_loss roc_auc_score time . 0 | 0.279747 | 0.202921 | 0.954094 | 01:45 | . 1 | 0.188010 | 0.182800 | 0.960526 | 01:46 | . 2 | 0.170447 | 0.182175 | 0.960804 | 01:51 | . 3 | 0.147315 | 0.189143 | 0.958946 | 01:48 | . 4 | 0.112068 | 0.201419 | 0.956670 | 01:46 | . Referenced another Kaggle notebook for this, we don&#39;t need it but it&#39;s good to see what fastai metrics is actually packaging up for you . preds, targs = learn.get_preds() . preds[0:1][0][0], preds[0:1][0][1] . (tensor(0.9987), tensor(0.0013)) . Here was my mistake, I was looking at the wrong classifier value . len(preds) . 65189 . (preds[:][:][:,1] &gt;= 0.5).sum(), (preds[:][:][:,1] &lt; 0.5).sum() . (tensor(10698), tensor(54491)) . from sklearn.metrics import roc_auc_score valid_score = roc_auc_score(to_np(targs), to_np(preds[:][:][:,1])) valid_score . 0.9566703126953837 . Doing inferences based on this blog post from Walk With Fastai initially, but then experimenting to get this . dl_test = dls.test_dl(df_test) . preds, _ = learn.get_preds(dl=dl_test) . (preds[:][:][:,1] &gt;= 0.5).sum(), (preds[:][:][:,1] &lt; 0.5).sum() . (tensor(28902), tensor(144934)) . Submission To Kaggle . path.ls() . (#20) [Path(&#39;homesite-quote-conversion.zip&#39;),Path(&#39;models&#39;),Path(&#39;submission.csv&#39;),Path(&#39;submission2.csv&#39;),Path(&#39;submission3.csv&#39;),Path(&#39;submission4.csv&#39;),Path(&#39;submission5.csv&#39;),Path(&#39;submission6.csv&#39;),Path(&#39;submission7.csv&#39;),Path(&#39;submission8.csv&#39;)...] . len(df_test.index), len(preds[:][:][:,1]) . (173836, 173836) . preds[:1][:1] . tensor([[0.9976, 0.0024]]) . We want the 2nd value, this is what gives us our prediction value of how likely our model thinks this is going to be a sale . preds[:1][:1][:,1] . tensor([0.0024]) . submission = pd.DataFrame({&#39;QuoteNumber&#39;: df_test.index, &#39;QuoteConversion_Flag&#39;: preds[:][:][:,1].tolist()}, columns=[&#39;QuoteNumber&#39;, &#39;QuoteConversion_Flag&#39;]) . Played around with the example on list comprehension here to get it to work with what I had to work with . submission.QuoteConversion_Flag = [float(qcf) for qcf in submission.QuoteConversion_Flag] . submission.head() . QuoteNumber QuoteConversion_Flag . 0 3 | 0.002424 | . 1 5 | 0.012709 | . 2 7 | 0.005221 | . 3 9 | 0.001084 | . 4 10 | 0.571628 | . submission.QuoteConversion_Flag = round(submission.QuoteConversion_Flag).astype(&#39;Int64&#39;) . submission.head() . QuoteNumber QuoteConversion_Flag . 0 3 | 0 | . 1 5 | 0 | . 2 7 | 0 | . 3 9 | 0 | . 4 10 | 1 | . len(submission[submission.QuoteConversion_Flag==1]) . 28902 . len(submission[submission.QuoteConversion_Flag==0]) . 144934 . submission.to_csv(path/&#39;submission13.csv&#39;, index=False) . api.competition_submit(path/&#39;submission13.csv&#39;,message=&quot;Thirteenth pass&quot;, competition=&#39;homesite-quote-conversion&#39;) . 100%|██████████| 1.45M/1.45M [00:00&lt;00:00, 7.07MB/s] . Successfully submitted to Homesite Quote Conversion . learn.save(&#39;homesite_fastai_nn13&#39;) . Path(&#39;models/homesite_fastai_nn13.pth&#39;) .",
            "url": "https://redditech.github.io/team-fast-tabulous/kaggle/fastai/2021/06/27/Improving-Fastai-split-choices-Copy1.html",
            "relUrl": "/kaggle/fastai/2021/06/27/Improving-Fastai-split-choices-Copy1.html",
            "date": " • Jun 27, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Random Forest and Data Exploration",
            "content": "Load Kaggle API Key and Libraries . !pip install -Uqq ipywidgets #This is already included in fastai but ah well import ipywidgets as widgets from pathlib import Path def on_file_upload(change): cred_path = Path(&#39;~/.kaggle/kaggle.json&#39;).expanduser() if not cred_path.exists(): cred_path.parent.mkdir(exist_ok=True) cred_path.write_text(str(btn_upload.data[-1])[2:-1]) cred_path.chmod(0o600) btn_upload = widgets.FileUpload(accept=&#39;.json&#39;, multiple=False) btn_upload.observe(on_file_upload, names=&#39;data&#39;) display(widgets.Label(&#39;Upload &#39;kaggle.json &#39;&#39;), btn_upload) . !pip install -Uqq fastai !pip install -Uqq kaggle import pandas as pd import numpy as np from fastai.tabular.all import * from kaggle import api from sklearn.model_selection import train_test_split from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import OneHotEncoder from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import roc_auc_score from sklearn.metrics import plot_roc_curve from sklearn.metrics import plot_confusion_matrix . |████████████████████████████████| 194kB 6.8MB/s |████████████████████████████████| 61kB 7.0MB/s . Load and Clean Data . api.competition_download_cli(&quot;homesite-quote-conversion&quot;) file_extract(&quot;homesite-quote-conversion.zip&quot;) #Extract all sub-directories and delete .zip files for item in os.listdir(): if item.endswith(&quot;.zip&quot;): file_extract(item) for item in os.listdir(): if item.endswith(&quot;.zip&quot;): os.remove(item) . 0%| | 0.00/62.0M [00:00&lt;?, ?B/s] . Downloading homesite-quote-conversion.zip to /content . 100%|██████████| 62.0M/62.0M [00:00&lt;00:00, 74.2MB/s] . . X_full = pd.read_csv(&quot;train.csv&quot;, parse_dates=[&#39;Original_Quote_Date&#39;]) X_test_full = pd.read_csv(&quot;test.csv&quot;, parse_dates=[&#39;Original_Quote_Date&#39;]) #Remove rows with missing target data (QuoteConversion_Flag) X_full.dropna(axis=0, subset=[&#39;QuoteConversion_Flag&#39;], inplace=True) #Format dates #This model actually performs better if dates are just dropped #but I&#39;ve included them just to demonstrate how this would be done X_full[&quot;Year&quot;] = X_full[&quot;Original_Quote_Date&quot;].dt.year X_full[&quot;Month&quot;] = X_full[&quot;Original_Quote_Date&quot;].dt.month X_full[&quot;Day&quot;] = X_full[&quot;Original_Quote_Date&quot;].dt.day X_test_full[&quot;Year&quot;] = X_test_full[&quot;Original_Quote_Date&quot;].dt.year X_test_full[&quot;Month&quot;] = X_test_full[&quot;Original_Quote_Date&quot;].dt.month X_test_full[&quot;Day&quot;] = X_test_full[&quot;Original_Quote_Date&quot;].dt.day X_full = X_full.drop(&quot;Original_Quote_Date&quot;, axis=1) X_test_full = X_test_full.drop(&quot;Original_Quote_Date&quot;, axis=1) #Seperate target column from the rest of the data (seperate X from Y) y = X_full.QuoteConversion_Flag X_full.drop([&#39;QuoteConversion_Flag&#39;], axis=1, inplace=True) #Split data for training X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, train_size=0.8, test_size=0.2, random_state=42) #Select numrical columns numerical_cols = [col for col in X_train_full.columns if X_train_full[col].dtype in [&#39;int64&#39;, &#39;float64&#39;]] #Select categorical columns (must have cardinality &lt; 10) categorical_cols = [col for col in X_train_full.columns if X_train_full[col].dtype == &#39;object&#39; and X_train_full[col].nunique() &lt; 10] #Combine together to get all columns selected_cols = numerical_cols + categorical_cols X_train = X_train_full[selected_cols].copy() X_valid = X_valid_full[selected_cols].copy() X_test = X_test_full[selected_cols].copy() . numerical_transformer = SimpleImputer(strategy=&#39;mean&#39;) #Preprocessing for categorical data categorical_transformer = Pipeline(steps=[ (&#39;imputer&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;)), (&#39;onehot&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;)) ]) #Bundle preprocessing for numerical and categorical data preprocessor = ColumnTransformer( transformers=[ (&#39;num&#39;, numerical_transformer, numerical_cols), (&#39;cat&#39;, categorical_transformer, categorical_cols) ]) #Define model model = RandomForestClassifier(n_estimators=100, random_state=0) #Bundle preprocessing and modeling code in a pipeline clf = Pipeline(steps=[ (&#39;preprocessor&#39;, preprocessor), (&#39;model&#39;, model) ]) . Fit and Predict . clf.fit(X_train, y_train) #Preprocessing of validation data, get predictions preds = clf.predict(X_valid) . preds_final = clf.predict(X_test) #Format and output to csv file df_final = pd.DataFrame({ &quot;QuoteNumber&quot;: X_test.QuoteNumber, &quot;QuoteConversion_Flag&quot;: preds_final }) df_final.to_csv(&quot;submission.csv&quot;, index=False) . Visualisations . print(&quot;ROC AUC Score: &quot; + str(roc_auc_score(y_valid, preds))) . ROC AUC Score: 0.80549992883288 . plot_roc_curve(clf, X_valid, y_valid) . &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7f9a14bcd9d0&gt; . plot_confusion_matrix(clf, X_valid, y_valid, values_format=&#39;d&#39;) . &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f9a12017d10&gt; . from sklearn.feature_selection._base import SelectorMixin from sklearn.feature_extraction.text import _VectorizerMixin def get_feature_out(estimator, feature_in): if hasattr(estimator,&#39;get_feature_names&#39;): if isinstance(estimator, _VectorizerMixin): # handling all vectorizers return [f&#39;vec_{f}&#39; for f in estimator.get_feature_names()] else: return estimator.get_feature_names(feature_in) elif isinstance(estimator, SelectorMixin): return np.array(feature_in)[estimator.get_support()] else: return feature_in def get_feature_names(ct): &quot;&quot;&quot; handles all estimators, pipelines inside ColumnTransfomer doesn&#39;t work when remainder ==&#39;passthrough&#39; which requires the input column names. &quot;&quot;&quot; output_features = [] for name, estimator, features in ct.transformers_: if name!=&#39;remainder&#39;: if isinstance(estimator, Pipeline): current_features = features for step in estimator: current_features = get_feature_out(step, current_features) features_out = current_features else: features_out = get_feature_out(estimator, features) output_features.extend(features_out) elif estimator==&#39;passthrough&#39;: output_features.extend(ct._feature_names_in[features]) return output_features . #but a simple concat would probably also work... #np.concatenate((np.array(numerical_cols), # clf[0].transformers_[1][1][1].get_feature_names())) df_importance = pd.DataFrame({&#39;cols&#39;:get_feature_names(clf[0]), &#39;imp&#39;:clf.steps[1][1].feature_importances_}).sort_values(&#39;imp&#39;, ascending=False) . def plot_fi(fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) plot_fi(df_importance[:30]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9a117ce450&gt; . Unimportant features may be removed from the model to make the data more manageable .",
            "url": "https://redditech.github.io/team-fast-tabulous/jupyter/collab/2021/06/26/Random-Forest-and-Data-Exploration.html",
            "relUrl": "/jupyter/collab/2021/06/26/Random-Forest-and-Data-Exploration.html",
            "date": " • Jun 26, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Python EDA Librares for Large Dataset (Google Colab)",
            "content": "Download Data . Store your data in a google drive folder and then mount drive to connect to your google drive . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . Import relevant libraries . import pandas as pd import numpy as np from pathlib import Path . Copy Path of the folder where you store your data . path = Path(&#39;/content/drive/MyDrive/Kaggle/data/homesite-quote&#39;) path.mkdir(parents=True, exist_ok=True) path . PosixPath(&#39;/content/drive/MyDrive/Kaggle/data/homesite-quote&#39;) . Import data and store it as a dataframe . df = pd.read_csv(path/&#39;train_df.csv&#39;, low_memory=False) test_df=pd.read_csv(path/&#39;test.csv&#39;, low_memory=False) . EDA . There are 4 EDA libraries that we are exploring today. Each has its own advantages and disadvantages.This page references https://towardsdatascience.com/4-libraries-that-can-perform-eda-in-one-line-of-python-code-b13938a06ae . Pandas Profiling . Please refer to https://github.com/pandas-profiling/pandas-profiling for the full instruction and examples . Run the below code to install Data Profiling straight from Github . ! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip . Import the library . from pandas_profiling import ProfileReport . Creating the a report. For this report, I set minimal=True to disable expensive computation to save runtime. This mode only gives you basic analysis with no correlation matrix (Because we have more than 300 columns, the correlation matrix is too big to be displayed). I tried running this on Full mode and it took forever to load. . profile=ProfileReport(df, title=&quot;Homesite Quote Conversion&quot;,minimal=True) . Run this code in Google Colab to show the report . %%time profile.to_notebook_iframe() . CPU times: user 2min 38s, sys: 1min 2s, total: 3min 41s Wall time: 2min 32s . Save an output file. This will create a html page in your Google Colab temporary folder. Download or move to it Google Drive if you want to save it . profile.to_file(output_file=&quot;Homesite_Quote_EDA_Data_Profilling.html&quot;) . A sidenote for people are working on the dataset. Looking at the descriptive analysis, it seems like a lot of columns have data with values from 1 - 25. These fields might have been encoded and transformed from categorical fields. But we don&#39;t know whether these codes are ordered according to their level of or not. . . We can also view the Warnings tab to view the issues that each column might experience . . DTale . D-Tale is the combination of a Flask back-end and a React front-end to bring you an easy way to view &amp; analyze Pandas data structures. Please refer to https://pypi.org/project/dtale/ for full instruction and examples . Import the library to Google Colab . !pip install -U dtale . Set up server for the app. You can use either USE_NGROK or USE_COLAB . import dtale import dtale.app as dtale_app #dtale_app.USE_NGROK=True dtale_app.USE_COLAB = True . /usr/local/lib/python3.7/dist-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details. defaults = yaml.load(f) . Show the report . %%time dtale.show(df) . https://vq30jy6l36k-496ff2e9c6d22116-40000-colab.googleusercontent.com/dtale/main/1 . CPU times: user 10.5 s, sys: 1.6 s, total: 12.1 s Wall time: 12.7 s . Dtale is very interactive app and contains rich information and a lot of different graphs. . . . This is also the only report that can generate correlation matrix in a very short period of time - 12.6s. The layout is also flexible enough to show all the values of the correlation matric. We can see that fields with similar name (for example CoverageFields1,2,3 etc) have quite high pearson correlation score . . Sweetviz . Sweetviz is an open-source Python library that generates beautiful, high-density visualizations to kickstart EDA (Exploratory Data Analysis) with just two lines of code. Output is a fully self-contained HTML application. Please refer to https://pypi.org/project/sweetviz/ for the full instruction and examples. The additional feature of Sweetviz is it allows user to compare 2 datasets (for example: train set and test set) . Install the library . !pip install sweetviz . Reimport pandas, np and import sweetviz. The layout of the app might be affected if you don&#39;t reimport pandas and numpy . import pandas as pd import numpy as np import sweetviz as sv . Once we run the code, sweetviz also give us clear instruction of how to handle errors. The below 3 columns caused some issues so I just handle them before running the model . df[&#39;PropertyField29&#39;]=df[&#39;PropertyField29&#39;].fillna(-1) test_df[&#39;PropertyField29&#39;]=test_df[&#39;PropertyField29&#39;].fillna(-1) df[&#39;PersonalField84&#39;]=df[&#39;PersonalField84&#39;].fillna(-1) test_df[&#39;PersonalField84&#39;]=test_df[&#39;PersonalField84&#39;].fillna(-1) . df[&#39;PropertyField37&#39;]=df[&#39;PropertyField37&#39;].astype(&#39;bool&#39;) test_df[&#39;PropertyField37&#39;]=test_df[&#39;PropertyField37&#39;].astype(&#39;bool&#39;) . Generate the report. We are giving names to each dataset (optional), and specifying a target feature (optional also). Specifying a target feature is extremely valuable as it will show how &quot;Survived&quot; is affected by each variable . %%time comparison_report = sv.compare([df,&#39;Train&#39;], [test_df,&#39;Test&#39;], target_feat=&#39;QuoteConversion_Flag&#39;,pairwise_analysis=&#39;off&#39;) . CPU times: user 3min 44s, sys: 37.7 s, total: 4min 22s Wall time: 3min 57s . Show the report. The report can be output as a standalone HTML file, OR embedded in this notebook. For notebooks, we can specify the width, height of the window, as well as scaling of the report itself. This line of code uses the default values (w=&quot;100%&quot;, h=750, layout=&quot;vertical&quot;) . comparison_report.show_notebook() . Save an output file. This will create a html page in your Google Colab temporary folder. Download or move to it Google Drive if you want to save it . comparison_report.show_html(filepath=&#39;SWEETVIZ_REPORT_Full.html&#39;, open_browser=True, layout=&#39;widescreen&#39;) . Report SWEETVIZ_REPORT_Full.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files. . A sidenote for people are working on the dataset. Looking at the descriptive analysis comparing train set and test set, it seems like they are strikingly similar. The distribution of values in each column is almost identical for most columns. This may mean that the test set just resemble the train set perfectly . . Autoviz . Automatically Visualize any dataset, any size with a single line of code. https://autoviz.io/ . Install library . !pip install autoviz . Import . from autoviz.AutoViz_Class import AutoViz_Class AV = AutoViz_Class() . Download the dataset to your google colab temporary folder. This is because Autoviz seems to only able to load dataset in the direct folder . !wget https://media.githubusercontent.com/media/redditech/team-fast-tabulous/master/dataset/train_df.csv . Run the report. This library doesn&#39;t seem to support large dataset with a lot of variables. It threw some error messages after a few minutes . %%time df_Au = AV.AutoViz(&#39;train_df.csv&#39;) . Shape of your Data Set: (208602, 299) ############## C L A S S I F Y I N G V A R I A B L E S #################### Classifying variables in data set... Number of Numeric Columns = 5 Number of Integer-Categorical Columns = 242 Number of String-Categorical Columns = 12 Number of Factor-Categorical Columns = 0 Number of String-Boolean Columns = 11 Number of Numeric-Boolean Columns = 21 Number of Discrete String Columns = 5 Number of NLP String Columns = 0 Number of Date Time Columns = 0 Number of ID Columns = 1 Number of Columns to Delete = 2 299 Predictors classified... This does not include the Target column(s) 8 variables removed since they were ID or low-information variables Since Number of Rows in data 208602 exceeds maximum, randomly sampling 150000 rows for EDA... 5 numeric variables in data exceeds limit, taking top 30 variables Number of All Scatter Plots = 15 . Could not draw Violin Plot . ValueError Traceback (most recent call last) /usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py in __call__(self, obj) 332 pass 333 else: --&gt; 334 return printer(obj) 335 # Finally look for special method names 336 method = get_real_method(obj, self.print_method) /usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py in &lt;lambda&gt;(fig) 239 240 if &#39;png&#39; in formats: --&gt; 241 png_formatter.for_type(Figure, lambda fig: print_figure(fig, &#39;png&#39;, **kwargs)) 242 if &#39;retina&#39; in formats or &#39;png2x&#39; in formats: 243 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)) /usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs) 123 124 bytes_io = BytesIO() --&gt; 125 fig.canvas.print_figure(bytes_io, **kw) 126 data = bytes_io.getvalue() 127 if fmt == &#39;svg&#39;: /usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs) 2092 self.figure, 2093 functools.partial( -&gt; 2094 print_method, dpi=dpi, orientation=orientation) 2095 ) 2096 ctx = (renderer._draw_disabled() /usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py in _get_renderer(figure, print_method) 1558 with cbook._setattr_cm(figure, draw=_draw): 1559 try: -&gt; 1560 print_method(io.BytesIO()) 1561 except Done as exc: 1562 renderer, = figure._cachedRenderer, = exc.args /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs) 512 } 513 --&gt; 514 FigureCanvasAgg.draw(self) 515 if pil_kwargs is not None: 516 from PIL import Image /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py in draw(self) 386 Draw the figure using the renderer. 387 &#34;&#34;&#34; --&gt; 388 self.renderer = self.get_renderer(cleared=True) 389 # Acquire a lock on the shared font cache. 390 with RendererAgg.lock, /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py in get_renderer(self, cleared) 402 and getattr(self, &#34;_lastKey&#34;, None) == key) 403 if not reuse_renderer: --&gt; 404 self.renderer = RendererAgg(w, h, self.figure.dpi) 405 self._lastKey = key 406 elif cleared: /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py in __init__(self, width, height, dpi) 90 self.width = width 91 self.height = height &gt; 92 self._renderer = _RendererAgg(int(width), int(height), dpi) 93 self._filter_renderers = [] 94 ValueError: Image size of 1440x256680 pixels is too large. It must be less than 2^16 in each direction. . &lt;Figure size 1440x256680 with 1425 Axes&gt; . Time to run AutoViz (in seconds) = 496.620 ###################### VISUALIZATION Completed ######################## CPU times: user 7min 55s, sys: 26.4 s, total: 8min 21s Wall time: 8min 16s . Conclusion . All of these EDA libraries really help to speed up your EDA process, especially when you are a beginner and want to spend too much time on data visualisation and analysis. . In my opinion, Dtale has the best performance on large dataset as it can report detailed complex data analyses in the shortest time without breaking the code. It is also quite interactive and allows you to export code. . Below is the runtime of each library: . Pandas Profiling (minimal version): 2mins 32s | Dtale: 12.7 s | Sweetviz (comparing dataset): 3min 57s | Autoviz: more than 8 mins (doens&#39;t seem to work with large dataset) | .",
            "url": "https://redditech.github.io/team-fast-tabulous/kaggle/2021/06/26/EDA_Packages_for_Large_Dataset.html",
            "relUrl": "/kaggle/2021/06/26/EDA_Packages_for_Large_Dataset.html",
            "date": " • Jun 26, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Synthentic data",
            "content": "import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas_profiling import random as rd from sklearn.ensemble import RandomForestClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.metrics import mean_squared_error from sklearn.metrics import plot_roc_curve from sklearn.metrics import plot_confusion_matrix from sklearn.metrics import roc_auc_score from dtreeviz.trees import * from IPython.display import Image, display_svg, SVG pd.options.display.max_rows = 20 pd.options.display.max_columns = 8 path = &#39;C:/Users/ML/PycharmProjects/fastai/data/&#39; . train_df = pd.read_csv(path + &#39;train.csv&#39;, low_memory=False) test_df = pd.read_csv(path + &#39;test.csv&#39;, low_memory=False) . train_df . QuoteNumber Original_Quote_Date QuoteConversion_Flag Field6 ... GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 1 | 2013-08-16 | 0 | B | ... | -1 | 10 | N | CA | . 1 2 | 2014-04-22 | 0 | F | ... | -1 | 20 | N | NJ | . 2 4 | 2014-08-25 | 0 | F | ... | -1 | 8 | N | NJ | . 3 6 | 2013-04-15 | 0 | J | ... | -1 | 21 | N | TX | . 4 8 | 2014-01-25 | 0 | E | ... | -1 | 12 | N | IL | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 260748 434584 | 2013-05-16 | 0 | J | ... | -1 | 8 | N | TX | . 260749 434585 | 2014-12-07 | 0 | J | ... | -1 | 20 | N | TX | . 260750 434586 | 2014-02-18 | 0 | F | ... | -1 | 8 | N | NJ | . 260751 434587 | 2014-04-08 | 0 | F | ... | -1 | 16 | N | NJ | . 260752 434588 | 2013-03-19 | 0 | F | ... | -1 | 8 | N | NJ | . 260753 rows × 299 columns . train_df.head() . QuoteNumber Original_Quote_Date QuoteConversion_Flag Field6 ... GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 1 | 2013-08-16 | 0 | B | ... | -1 | 10 | N | CA | . 1 2 | 2014-04-22 | 0 | F | ... | -1 | 20 | N | NJ | . 2 4 | 2014-08-25 | 0 | F | ... | -1 | 8 | N | NJ | . 3 6 | 2013-04-15 | 0 | J | ... | -1 | 21 | N | TX | . 4 8 | 2014-01-25 | 0 | E | ... | -1 | 12 | N | IL | . 5 rows × 299 columns . Check if there are NaN values . nan_validation = train_df.isna().sum()&gt;0 nan_validation . QuoteNumber False Original_Quote_Date False QuoteConversion_Flag False Field6 False Field7 False ... GeographicField61B False GeographicField62A False GeographicField62B False GeographicField63 False GeographicField64 False Length: 299, dtype: bool . nan_validation.unique() . array([False, True]) . train_df.isna().sum()[train_df.isna().sum() &gt; 0].index . Index([&#39;PersonalField7&#39;, &#39;PersonalField84&#39;, &#39;PropertyField3&#39;, &#39;PropertyField4&#39;, &#39;PropertyField29&#39;, &#39;PropertyField32&#39;, &#39;PropertyField34&#39;, &#39;PropertyField36&#39;, &#39;PropertyField38&#39;], dtype=&#39;object&#39;) . train_df.PersonalField7.unique() . array([&#39;N&#39;, &#39;Y&#39;, nan], dtype=object) . train_df.PersonalField7.isna() . 0 False 1 False 2 False 3 False 4 False ... 260748 False 260749 False 260750 False 260751 False 260752 False Name: PersonalField7, Length: 260753, dtype: bool . train_df.PersonalField7.isna().unique() . array([False, True]) . train_df.dropna(subset=[&#39;PersonalField7&#39;, &#39;PersonalField84&#39;, &#39;PropertyField3&#39;, &#39;PropertyField4&#39;, &#39;PropertyField29&#39;, &#39;PropertyField32&#39;, &#39;PropertyField34&#39;, &#39;PropertyField36&#39;, &#39;PropertyField38&#39;], inplace=False) . QuoteNumber Original_Quote_Date QuoteConversion_Flag Field6 ... GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 1 | 2013-08-16 | 0 | B | ... | -1 | 10 | N | CA | . 4 8 | 2014-01-25 | 0 | E | ... | -1 | 12 | N | IL | . 6 13 | 2013-11-01 | 0 | J | ... | -1 | 8 | N | TX | . 11 22 | 2013-06-11 | 0 | B | ... | -1 | 8 | N | CA | . 19 37 | 2013-06-18 | 0 | E | ... | -1 | 8 | N | IL | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 260737 434569 | 2014-03-10 | 0 | E | ... | -1 | 25 | N | IL | . 260738 434571 | 2014-04-11 | 0 | E | ... | -1 | 10 | N | IL | . 260743 434579 | 2013-05-02 | 0 | B | ... | -1 | 17 | N | CA | . 260744 434580 | 2013-10-28 | 0 | F | ... | -1 | 19 | N | NJ | . 260748 434584 | 2013-05-16 | 0 | J | ... | -1 | 8 | N | TX | . 55242 rows × 299 columns . cols_to_delete = train_df.isna().sum()[train_df.isna().sum() &gt; 0].index def drop_cols_from_list(df,cols_to_delete): df.drop(cols_to_delete,axis=1,inplace=True) return df train_df1 = drop_cols_from_list(train_df,cols_to_delete) test_df1 = drop_cols_from_list(test_df,cols_to_delete) . train_df1 . QuoteNumber Original_Quote_Date QuoteConversion_Flag Field6 ... GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 1 | 2013-08-16 | 0 | B | ... | -1 | 10 | N | CA | . 1 2 | 2014-04-22 | 0 | F | ... | -1 | 20 | N | NJ | . 2 4 | 2014-08-25 | 0 | F | ... | -1 | 8 | N | NJ | . 3 6 | 2013-04-15 | 0 | J | ... | -1 | 21 | N | TX | . 4 8 | 2014-01-25 | 0 | E | ... | -1 | 12 | N | IL | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 260748 434584 | 2013-05-16 | 0 | J | ... | -1 | 8 | N | TX | . 260749 434585 | 2014-12-07 | 0 | J | ... | -1 | 20 | N | TX | . 260750 434586 | 2014-02-18 | 0 | F | ... | -1 | 8 | N | NJ | . 260751 434587 | 2014-04-08 | 0 | F | ... | -1 | 16 | N | NJ | . 260752 434588 | 2013-03-19 | 0 | F | ... | -1 | 8 | N | NJ | . 260753 rows × 290 columns . train_df1.head(1) . QuoteNumber Original_Quote_Date QuoteConversion_Flag Field6 ... GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 1 | 2013-08-16 | 0 | B | ... | -1 | 10 | N | CA | . 1 rows × 290 columns . train_df1.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 260753 entries, 0 to 260752 Columns: 290 entries, QuoteNumber to GeographicField64 dtypes: float64(4), int64(265), object(21) memory usage: 576.9+ MB . test_df1.head(1) . QuoteNumber Original_Quote_Date Field6 Field7 ... GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 3 | 2014-08-12 | E | 16 | ... | -1 | 25 | Y | IL | . 1 rows × 289 columns . vld = train_df1.isna().sum()&gt;0 vld.unique() . array([False]) . train_df._get_numeric_data().columns . Index([&#39;QuoteNumber&#39;, &#39;QuoteConversion_Flag&#39;, &#39;Field7&#39;, &#39;Field8&#39;, &#39;Field9&#39;, &#39;Field11&#39;, &#39;CoverageField1A&#39;, &#39;CoverageField1B&#39;, &#39;CoverageField2A&#39;, &#39;CoverageField2B&#39;, ... &#39;GeographicField58A&#39;, &#39;GeographicField58B&#39;, &#39;GeographicField59A&#39;, &#39;GeographicField59B&#39;, &#39;GeographicField60A&#39;, &#39;GeographicField60B&#39;, &#39;GeographicField61A&#39;, &#39;GeographicField61B&#39;, &#39;GeographicField62A&#39;, &#39;GeographicField62B&#39;], dtype=&#39;object&#39;, length=269) . cols_to_drop = [] for i in set(train_df.columns) - set(train_df._get_numeric_data().columns): if (train_df.loc[:,i].nunique() &gt;= 3): cols_to_drop.append(i) train_df = drop_cols_from_list(train_df,cols_to_drop) test_df = drop_cols_from_list(test_df,cols_to_drop) . cls_to_encode = set(train_df.columns) - set(train_df._get_numeric_data().columns) def ohe(df,cls_to_encode): df = pd.get_dummies(df,columns=cls_to_encode,drop_first=True) return df train_df = ohe(train_df,cls_to_encode) test_df = ohe(test_df,cls_to_encode) . test_df.drop(list(set(test_df.columns) - set(train_df.columns)),axis=1,inplace=True) . train_df . QuoteNumber QuoteConversion_Flag Field7 Field8 ... PropertyField30_Y PropertyField37_Y Field12_Y PropertyField5_Y . 0 1 | 0 | 23 | 0.9403 | ... | 0 | 0 | 0 | 1 | . 1 2 | 0 | 7 | 1.0006 | ... | 0 | 0 | 0 | 1 | . 2 4 | 0 | 7 | 1.0006 | ... | 0 | 0 | 0 | 1 | . 3 6 | 0 | 10 | 0.9769 | ... | 0 | 1 | 0 | 1 | . 4 8 | 0 | 23 | 0.9472 | ... | 0 | 0 | 0 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 260748 434584 | 0 | 23 | 0.9691 | ... | 0 | 0 | 0 | 1 | . 260749 434585 | 0 | 26 | 0.8870 | ... | 0 | 1 | 0 | 1 | . 260750 434586 | 0 | 11 | 0.9685 | ... | 0 | 1 | 0 | 1 | . 260751 434587 | 0 | 7 | 1.0006 | ... | 0 | 0 | 0 | 1 | . 260752 434588 | 0 | 15 | 0.8945 | ... | 0 | 0 | 0 | 1 | . 260753 rows × 273 columns . labels = train_df.QuoteConversion_Flag.value_counts() labels . 0 211859 1 48894 Name: QuoteConversion_Flag, dtype: int64 . fig5, ax5 = plt.subplots(figsize=(16,12)) sns.countplot(data = train_df, x=&#39;QuoteConversion_Flag&#39;).set_title(&quot;2-class label distribution&quot;) . Text(0.5, 1.0, &#39;2-class label distribution&#39;) . Train data profile . . Test data profile . . Data augmentation with CTGAN . # the cathegorical columns from ctgan import CTGANSynthesizer ctgan = CTGANSynthesizer() # Initiate the CTGANSynthesizer and call its fit method to pass in the table ctgan = CTGANSynthesizer(epochs=80) ctgan.fit(train_df) #generate synthetic data, 1000 rows of data synthetic_data = ctgan.sample(200000) . c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:269: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data. % (init + 1), ConvergenceWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ c: users ml .conda envs ctgan lib site-packages sklearn utils validation.py:72: FutureWarning: Pass n_components=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error &#34;will result in an error&#34;, FutureWarning) c: users ml .conda envs ctgan lib site-packages sklearn mixture _base.py:148: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X. random_state=random_state).fit(X).labels_ . synthetic_data . QuoteNumber QuoteConversion_Flag Field7 Field8 ... PropertyField30_Y PropertyField37_Y Field12_Y PropertyField5_Y . 0 58475 | 0 | 14 | 0.919014 | ... | 0 | 0 | 1 | 1 | . 1 283439 | 0 | 16 | 0.979751 | ... | 0 | 0 | 0 | 1 | . 2 329449 | 0 | 6 | 0.887974 | ... | 0 | 1 | 0 | 0 | . 3 102468 | 0 | 0 | 0.914928 | ... | 0 | 0 | 0 | 0 | . 4 179776 | 0 | 2 | 1.009825 | ... | 0 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 199995 58471 | 1 | 24 | 0.888249 | ... | 0 | 0 | 1 | 1 | . 199996 122615 | 1 | 7 | 0.884400 | ... | 0 | 0 | 0 | 1 | . 199997 99632 | 0 | 26 | 1.001284 | ... | 0 | 0 | 0 | 0 | . 199998 387518 | 1 | 14 | 0.940138 | ... | 1 | 0 | 0 | 0 | . 199999 67670 | 0 | 15 | 0.938704 | ... | 0 | 1 | 1 | 0 | . 200000 rows × 273 columns . ctgan.save(&#39;HQC_ctgan_model.pkl&#39;) . synthetic_data.to_pickle(&quot;synthetic_data.pkl&quot;) . synthetic_data.to_csv(&quot;synthetic_data.csv&quot;, index=False) . from table_evaluator import load_data, TableEvaluator table_evaluator = TableEvaluator(train_df, synthetic_data) table_evaluator.visual_evaluation() . c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:306: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) c: users ml .conda envs ctgan lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . Label distribution . labels = synthetic_data.QuoteConversion_Flag.value_counts() labels . 0 165326 1 34674 Name: QuoteConversion_Flag, dtype: int64 . fig5, ax5 = plt.subplots(figsize=(16,12)) sns.countplot(data = synthetic_data, x=&#39;QuoteConversion_Flag&#39;).set_title(&quot;2-class label distribution&quot;) . Text(0.5, 1.0, &#39;2-class label distribution&#39;) . Synthetic data evaluation . Statistical evaluation . . Likelihood metrics . .",
            "url": "https://redditech.github.io/team-fast-tabulous/jupyter/collab/2021/06/25/HQC_synthetic_data_prototype.html",
            "relUrl": "/jupyter/collab/2021/06/25/HQC_synthetic_data_prototype.html",
            "date": " • Jun 25, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Exploring Homesite Data (WORK IN PROGRESS)",
            "content": "Introduction . Here I will do some exploratory data analysis of Homesite Data from the Kaggle Competition using Google Colab . Setup fastai and Google drive . !pip install -Uqq fastai . from fastai.tabular.all import * . The snippet below is only useful in Colab for accessing my Google Drive and is straight out the fastbook source code in Github . global gdrive gdrive = Path(&#39;/content/gdrive/My Drive&#39;) from google.colab import drive if not gdrive.exists(): drive.mount(str(gdrive.parent)) . Add the Kaggle bits . !pip install kaggle . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) . !ls /content/gdrive/MyDrive/Kaggle/kaggle.json . /content/gdrive/MyDrive/Kaggle/kaggle.json . Useful links here: . Documentation on Path library | Documentation on fastai extensions to Path library | . Path.cwd() . Path(&#39;/content&#39;) . Setup kaggle environment . !mkdir -p ~/.kaggle !cp /content/gdrive/MyDrive/Kaggle/kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . from kaggle import api . path = Path.cwd() path.ls() . (#3) [Path(&#39;/content/.config&#39;),Path(&#39;/content/sample_data&#39;),Path(&#39;/content/gdrive&#39;)] . path = path/&quot;gdrive/MyDrive/Kaggle/homesite_data&quot; path.mkdir(exist_ok=True) Path.BASE_PATH = path api.competition_download_cli(&#39;homesite-quote-conversion&#39;, path=path) file_extract(path/&quot;homesite-quote-conversion.zip&quot;) file_extract(path/&quot;train.csv.zip&quot;) file_extract(path/&quot;test.csv.zip&quot;) . homesite-quote-conversion.zip: Skipping, found more recently modified local copy (use --force to force download) . path . Path(&#39;.&#39;) . path.ls() . (#6) [Path(&#39;homesite-quote-conversion.zip&#39;),Path(&#39;sample_submission.csv.zip&#39;),Path(&#39;test.csv.zip&#39;),Path(&#39;train.csv.zip&#39;),Path(&#39;train.csv&#39;),Path(&#39;test.csv&#39;)] . Exploring the Homesite data . df_train = pd.read_csv(path/&quot;train.csv&quot;, low_memory=False) df_train.head() . QuoteNumber Original_Quote_Date QuoteConversion_Flag Field6 Field7 Field8 Field9 Field10 Field11 Field12 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField3 SalesField4 SalesField5 SalesField6 SalesField7 SalesField8 SalesField9 SalesField10 SalesField11 SalesField12 ... GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56A GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60A GeographicField60B GeographicField61A GeographicField61B GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 1 | 2013-08-16 | 0 | B | 23 | 0.9403 | 0.0006 | 965 | 1.0200 | N | 17 | 23 | 17 | 23 | 15 | 22 | 16 | 22 | 13 | 22 | 13 | 23 | T | D | 2 | 1 | 7 | 18 | 3 | 8 | 0 | 5 | 5 | 24 | V | 48649 | 0 | 0 | 0 | 0 | ... | 8 | 4 | 20 | 22 | 10 | 8 | 6 | 5 | 15 | 13 | 19 | 18 | 16 | 14 | 21 | 23 | 21 | 23 | 16 | 11 | 22 | 24 | 7 | 14 | -1 | 17 | 15 | 17 | 14 | 18 | 9 | 9 | -1 | 8 | -1 | 18 | -1 | 10 | N | CA | . 1 2 | 2014-04-22 | 0 | F | 7 | 1.0006 | 0.0040 | 548 | 1.2433 | N | 6 | 8 | 6 | 8 | 5 | 7 | 5 | 8 | 13 | 22 | 13 | 23 | T | E | 5 | 9 | 5 | 14 | 6 | 18 | 1 | 5 | 5 | 11 | P | 26778 | 0 | 0 | 1 | 1 | ... | 23 | 24 | 11 | 15 | 21 | 24 | 6 | 11 | 21 | 21 | 18 | 15 | 20 | 20 | 13 | 12 | 12 | 12 | 15 | 9 | 13 | 11 | 11 | 20 | -1 | 9 | 18 | 21 | 8 | 7 | 10 | 10 | -1 | 11 | -1 | 17 | -1 | 20 | N | NJ | . 2 4 | 2014-08-25 | 0 | F | 7 | 1.0006 | 0.0040 | 548 | 1.2433 | N | 7 | 12 | 7 | 12 | 6 | 10 | 7 | 11 | 25 | 25 | 13 | 23 | T | J | 4 | 6 | 3 | 10 | 4 | 11 | 1 | 5 | 5 | 11 | K | 8751 | 0 | 0 | 2 | 2 | ... | 21 | 22 | 24 | 25 | 20 | 22 | 7 | 13 | 23 | 23 | 20 | 19 | 20 | 20 | 18 | 20 | 19 | 21 | 20 | 19 | 11 | 8 | 3 | 3 | -1 | 5 | 21 | 24 | 12 | 15 | 15 | 18 | -1 | 21 | -1 | 11 | -1 | 8 | N | NJ | . 3 6 | 2013-04-15 | 0 | J | 10 | 0.9769 | 0.0004 | 1,165 | 1.2665 | N | 3 | 2 | 3 | 2 | 2 | 2 | 3 | 2 | 13 | 22 | 13 | 23 | Y | F | 15 | 23 | 8 | 19 | 14 | 24 | 0 | 5 | 5 | 23 | V | 43854 | 0 | 0 | 0 | 0 | ... | 3 | 1 | 14 | 22 | 6 | 2 | 7 | 14 | 11 | 8 | 19 | 18 | 18 | 16 | 13 | 12 | 13 | 12 | 17 | 13 | 5 | 2 | 3 | 4 | -1 | 7 | 14 | 14 | 14 | 18 | 6 | 5 | -1 | 10 | -1 | 9 | -1 | 21 | N | TX | . 4 8 | 2014-01-25 | 0 | E | 23 | 0.9472 | 0.0006 | 1,487 | 1.3045 | N | 8 | 13 | 8 | 13 | 7 | 11 | 7 | 13 | 13 | 22 | 13 | 23 | T | F | 4 | 6 | 3 | 6 | 3 | 6 | 1 | 5 | 5 | 7 | R | 12505 | 1 | 0 | 0 | 0 | ... | 24 | 25 | 9 | 11 | 25 | 25 | 5 | 3 | 22 | 22 | 21 | 21 | 17 | 15 | 25 | 25 | 25 | 25 | 17 | 13 | 13 | 11 | 3 | 4 | -1 | 7 | 11 | 9 | 10 | 10 | 18 | 22 | -1 | 10 | -1 | 11 | -1 | 12 | N | IL | . 5 rows × 299 columns . df_test = pd.read_csv(path/&quot;test.csv&quot;, low_memory=False) df_test.head() . QuoteNumber Original_Quote_Date Field6 Field7 Field8 Field9 Field10 Field11 Field12 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField3 SalesField4 SalesField5 SalesField6 SalesField7 SalesField8 SalesField9 SalesField10 SalesField11 SalesField12 SalesField13 ... GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56A GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60A GeographicField60B GeographicField61A GeographicField61B GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 3 | 2014-08-12 | E | 16 | 0.9364 | 0.0006 | 1,487 | 1.3045 | N | 4 | 4 | 4 | 4 | 3 | 3 | 3 | 4 | 13 | 22 | 13 | 23 | Y | K | 13 | 22 | 6 | 16 | 9 | 21 | 0 | 5 | 5 | 11 | P | 67052 | 0 | 0 | 0 | 0 | 0 | ... | 22 | 23 | 9 | 12 | 25 | 25 | 6 | 9 | 4 | 2 | 16 | 12 | 20 | 20 | 2 | 2 | 2 | 1 | 1 | 1 | 10 | 7 | 25 | 25 | -1 | 19 | 19 | 22 | 12 | 15 | 1 | 1 | -1 | 1 | -1 | 20 | -1 | 25 | Y | IL | . 1 5 | 2013-09-07 | F | 11 | 0.9919 | 0.0038 | 564 | 1.1886 | N | 8 | 14 | 8 | 14 | 7 | 12 | 8 | 13 | 13 | 22 | 13 | 23 | T | E | 4 | 5 | 3 | 6 | 3 | 6 | 1 | 5 | 5 | 4 | R | 27288 | 1 | 0 | 0 | 0 | 0 | ... | 23 | 24 | 12 | 21 | 23 | 25 | 7 | 11 | 16 | 14 | 13 | 6 | 17 | 15 | 7 | 5 | 7 | 5 | 13 | 7 | 14 | 14 | 7 | 14 | -1 | 4 | 1 | 1 | 5 | 3 | 10 | 10 | -1 | 5 | -1 | 5 | -1 | 21 | N | NJ | . 2 7 | 2013-03-29 | F | 15 | 0.8945 | 0.0038 | 564 | 1.0670 | N | 11 | 18 | 11 | 18 | 10 | 16 | 10 | 18 | 13 | 22 | 13 | 23 | T | E | 3 | 3 | 5 | 14 | 3 | 9 | 1 | 5 | 5 | 23 | V | 65264 | 0 | 1 | 2 | 2 | 0 | ... | 16 | 18 | 9 | 10 | 14 | 16 | 6 | 8 | 20 | 19 | 17 | 14 | 16 | 13 | 20 | 22 | 20 | 22 | 20 | 18 | 10 | 7 | 4 | 7 | -1 | 11 | 13 | 12 | 18 | 22 | 10 | 11 | -1 | 20 | -1 | 22 | -1 | 11 | N | NJ | . 3 9 | 2015-03-21 | K | 21 | 0.8870 | 0.0004 | 1,113 | 1.2665 | Y | 14 | 22 | 15 | 22 | 13 | 20 | 22 | 25 | 13 | 22 | 13 | 23 | Y | F | 5 | 9 | 9 | 20 | 5 | 16 | 1 | 5 | 5 | 11 | R | 32725 | 1 | 1 | 1 | 1 | 0 | ... | 11 | 11 | 9 | 10 | 11 | 13 | 15 | 21 | 14 | 12 | 17 | 13 | 10 | 6 | 20 | 22 | 20 | 22 | 19 | 16 | 12 | 11 | 4 | 6 | -1 | 13 | 10 | 8 | 5 | 3 | 8 | 8 | -1 | 13 | -1 | 8 | -1 | 21 | N | TX | . 4 10 | 2014-12-10 | B | 25 | 0.9153 | 0.0007 | 935 | 1.0200 | N | 4 | 5 | 4 | 5 | 4 | 4 | 4 | 5 | 13 | 22 | 13 | 23 | Y | D | 12 | 21 | 1 | 1 | 3 | 6 | 0 | 5 | 5 | 11 | T | 56025 | 0 | 1 | 1 | 1 | 0 | ... | 9 | 8 | 25 | 25 | 9 | 3 | 9 | 18 | 7 | 4 | 16 | 12 | 13 | 9 | 8 | 6 | 8 | 6 | 11 | 5 | 19 | 21 | 13 | 21 | -1 | 23 | 11 | 8 | 5 | 3 | 7 | 7 | -1 | 3 | -1 | 22 | -1 | 21 | N | CA | . 5 rows × 298 columns . y_column = df_train.columns.difference(df_test.columns) . y_column . Index([&#39;QuoteConversion_Flag&#39;], dtype=&#39;object&#39;) . From this it looks like QuoteConversion_Flag is the value we want to predict. Let&#39;s take a look at this . type(df_train.QuoteConversion_Flag) . pandas.core.series.Series . df_train.QuoteConversion_Flag.unique() . array([0, 1]) . type(df_train.QuoteConversion_Flag.unique()[0]) . numpy.int64 . What&#39;s interesting here is that the training data outcomes that we want to generate predictions on seems to just be a binary classification, either there&#39;s a sale (1) or there&#39;s no sale (0). With maths and rounding up and down, predictions above 50% will be rounded up to a sale and predictions less than 50% will be classified as predicting no sale if we keep this field as an integer. . In reality, 50% might not be a good threshold to keep and we may want to vary this rounding value of confidence. If we convert it to a floating point, we can have more control over this threshold, and thus the final prediction, so we can predict a sale if there&#39;s say, more than 90% confidence in the prediction. . df_train.QuoteConversion_Flag = df_train.QuoteConversion_Flag.astype(dtype=&#39;float64&#39;) . First things first . Learning from my collague Tim&#39;s work already we know: . Quotenumber is unique so make it the index | Original_Quote_Date should be made into a date | . Additionally, we should make sure to apply any changes to data types to both train and test data so predictions don&#39;t fail later on . df_train = df_train.set_index(&#39;QuoteNumber&#39;) df_test = df_test.set_index(&#39;QuoteNumber&#39;) . df_train[&#39;Original_Quote_Date&#39;] = pd.to_datetime(df_train[&#39;Original_Quote_Date&#39;]) df_test[&#39;Original_Quote_Date&#39;] = pd.to_datetime(df_train[&#39;Original_Quote_Date&#39;]) . Goal: Identifying our categorical and continuous data columns . categorical=L() continuous=L() notused =L() . Dividing up the data into manageable chunks for exploration . Let&#39;s now see how many columns we are working with . df_train.columns . Index([&#39;Original_Quote_Date&#39;, &#39;QuoteConversion_Flag&#39;, &#39;Field6&#39;, &#39;Field7&#39;, &#39;Field8&#39;, &#39;Field9&#39;, &#39;Field10&#39;, &#39;Field11&#39;, &#39;Field12&#39;, &#39;CoverageField1A&#39;, ... &#39;GeographicField59A&#39;, &#39;GeographicField59B&#39;, &#39;GeographicField60A&#39;, &#39;GeographicField60B&#39;, &#39;GeographicField61A&#39;, &#39;GeographicField61B&#39;, &#39;GeographicField62A&#39;, &#39;GeographicField62B&#39;, &#39;GeographicField63&#39;, &#39;GeographicField64&#39;], dtype=&#39;object&#39;, length=298) . df_train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 260753 entries, 1 to 434588 Columns: 298 entries, Original_Quote_Date to GeographicField64 dtypes: datetime64[ns](1), float64(7), int64(263), object(27) memory usage: 604.8+ MB . 298 columns is alot, let&#39;s split these by type . df_train.dtypes.unique() . array([dtype(&#39;&lt;M8[ns]&#39;), dtype(&#39;float64&#39;), dtype(&#39;O&#39;), dtype(&#39;int64&#39;)], dtype=object) . Exploring the float64 data . Let&#39;s take a look at the smallest subset, the floating point columns, to see if there&#39;s anything interesting about them . (df_train[df_train.columns[df_train.dtypes==&#39;float64&#39;]].drop(columns=[&#39;QuoteConversion_Flag&#39;])).head() . Field8 Field9 Field11 PersonalField84 PropertyField25 PropertyField29 . QuoteNumber . 1 0.9403 | 0.0006 | 1.0200 | 2.0 | 3.0 | 0.0 | . 2 1.0006 | 0.0040 | 1.2433 | NaN | 2.0 | NaN | . 4 1.0006 | 0.0040 | 1.2433 | NaN | 1.5 | NaN | . 6 0.9769 | 0.0004 | 1.2665 | NaN | 1.0 | NaN | . 8 0.9472 | 0.0006 | 1.3045 | 2.0 | 3.0 | 0.0 | . Referencing this article for some good tips on EDA (Exploratory Data Analysis) . (df_train[df_train.columns[df_train.dtypes==&#39;float64&#39;]].drop(columns=[&#39;QuoteConversion_Flag&#39;])).describe() . Field8 Field9 Field11 PersonalField84 PropertyField25 PropertyField29 . count 260753.000000 | 260753.000000 | 260753.000000 | 136545.000000 | 260753.000000 | 60068.000000 | . mean 0.938346 | 0.001451 | 1.162718 | 1.990142 | 1.632422 | 0.000200 | . std 0.037086 | 0.001486 | 0.116833 | 0.127931 | 0.717064 | 0.014133 | . min 0.874600 | 0.000400 | 1.000000 | 1.000000 | 0.000000 | 0.000000 | . 25% 0.915300 | 0.000600 | 1.020000 | 2.000000 | 1.000000 | 0.000000 | . 50% 0.940300 | 0.000600 | 1.188600 | 2.000000 | 1.500000 | 0.000000 | . 75% 0.968500 | 0.003800 | 1.266500 | 2.000000 | 2.000000 | 0.000000 | . max 1.010100 | 0.004000 | 1.304500 | 8.000000 | 7.000000 | 1.000000 | . Field8 analysis . df_train[&#39;Field8&#39;].value_counts() . 0.9403 48161 0.9153 44198 0.8870 18476 1.0006 18371 0.9919 16845 0.8928 13945 0.8922 10982 0.9691 8237 0.9392 7235 1.0101 6927 0.9838 5178 0.9108 4827 0.9313 4816 1.0005 3436 0.9769 3329 0.9497 3241 0.9472 3175 0.9566 3065 0.9559 2824 0.8793 2697 0.9685 2558 0.9219 2366 0.9368 2238 0.9223 2210 0.8945 2166 0.9023 2036 0.9487 2012 0.9525 1972 0.9482 1835 0.8746 1806 0.9893 1685 0.9258 1280 0.8930 1213 0.9194 1205 0.9364 1189 0.9375 1108 0.9485 969 0.9489 940 Name: Field8, dtype: int64 . df_train[&#39;Field8&#39;].isna().sum() . 0 . df_train[&quot;Field8&quot;].plot.hist(bins=200) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe5588c0510&gt; . Field8 looks like a good candidate as a continuous field. Let&#39;s add it to that list . continuous.append(&#39;Field8&#39;) . continuous . (#1) [&#39;Field8&#39;] . Field 9 analysis . df_train[&quot;Field9&quot;].value_counts() . 0.0006 74096 0.0004 65011 0.0007 52353 0.0040 39065 0.0038 30228 Name: Field9, dtype: int64 . df_train[&quot;Field9&quot;].isna().sum() . 0 . df_train[&quot;Field9&quot;].plot.hist(bins=50) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe558603b50&gt; . Even though it is a floating point field, it looks very categorical in nature rather than continuous. We could peak at the test dataset to confirm this . df_test[~df_test[&quot;Field9&quot;].isin(df_train[&quot;Field9&quot;].unique())].value_counts() . Series([], dtype: int64) . So this hypothesis holds true, so we&#39;re going with the assumption that Field9 is categorical not continuous . categorical.append(&quot;Field9&quot;) . df_train[&quot;Field9&quot;].unique() . array([0.0006, 0.004 , 0.0004, 0.0007, 0.0038]) . Field11 . df_train[&quot;Field11&quot;].value_counts() . 1.0200 92359 1.2665 62187 1.3045 28378 1.1886 27646 1.2433 23492 1.2694 6927 1.0670 6412 1.1161 4816 1.2714 3346 1.2392 2824 1.0000 2366 Name: Field11, dtype: int64 . df_train[&quot;Field11&quot;].isna().sum() . 0 . df_train[&quot;Field11&quot;].plot.hist(bins=200) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe557ffedd0&gt; . Like Field9 this too also looks more categorical than continuous in nature. We can help reassure this hypothesis by again &quot;peeking&quot; to see if any test data values don&#39;t confirm to this theory . df_test[~df_test[&quot;Field11&quot;].isin(df_train[&quot;Field11&quot;].unique())].value_counts() . Series([], dtype: int64) . Once again, the hypothesis holds true, so let&#39;s add Field11 to our list of categorical fields . categorical.append(&quot;Field11&quot;) . PersonalField84 . df_train[&quot;PersonalField84&quot;].value_counts() . 2.0 134906 1.0 1564 5.0 62 3.0 7 7.0 3 4.0 2 8.0 1 Name: PersonalField84, dtype: int64 . df_train[&quot;PersonalField84&quot;].isna().sum() . 124208 . df_train[&quot;PersonalField84&quot;].plot.hist(bins=100) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe557d29990&gt; . This one also seems a bit categorical. But there are quite a few NA values. We have a few choices here. . Given the heavy skewing towards one value, it would arguably be a good assumption to fill this with the mode value and not a mean value. | Given that a significant number of NA is present, we could ignore it totally as part of our model, and revisit it after if we&#39;re looking to fine tune our input dataset | For now, let&#39;s ignore it | . notused.append(&quot;PersonalField84&quot;) . PropertyField25 . df_train[&quot;PropertyField25&quot;].value_counts() . 1.0 121563 2.0 91666 3.0 27427 1.5 16003 2.5 2173 5.0 1045 4.0 531 3.5 156 7.0 119 6.0 53 0.0 17 Name: PropertyField25, dtype: int64 . df_train[&quot;PropertyField25&quot;].isna().sum() . 0 . df_train[&quot;PropertyField25&quot;].plot.hist(bins=100) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe557d2b910&gt; . This also looks very much like a categorical field and not a continuous field. &quot;Peeking&quot; once more at our test data to help the belief we&#39;re not making a bad assumption here . df_test[~df_test[&quot;PropertyField25&quot;].isin(df_train[&quot;PropertyField25&quot;].unique())].value_counts() . Series([], dtype: int64) . Once again, the hypothesis holds true, so let&#39;s add PropertyField25 to our list of categorical fields . categorical.append(&quot;PropertyField25&quot;) . PropertyField29 . PropertyField29 looks like it might be a boolean. Let&#39;s test this . df_train[&#39;PropertyField29&#39;].value_counts() . 0.0 60056 1.0 12 Name: PropertyField29, dtype: int64 . df_train[&#39;PropertyField29&#39;].isna().sum() . 200685 . This one also seems a bit categorical. But there are quite a few NA values. We have a few choices here. . Given the heavy skewing towards one value, it would arguably be a good assumption to fill this with the mode value and not a mean value. | Given that a significant number of NA is present, we could ignore it totally as part of our model, and revisit it after if we&#39;re looking to fine tune our input dataset | For now, let&#39;s ignore it | . notused.append(&quot;PropertyField29&quot;) . Experimenting with correlation bits . What if we decided instead we wanted to include this data, with assumptions for values in NA, as part of our model? . How do we fill the NA values? . Should we use 0 or should we use 1? . Let&#39;s first look at if it correlates with any other field that might give us insight about how to treat with it . Let&#39;s have a look at the ranges in each of these columns, to identify if they&#39;re continuous or categorical in nature. Reference . correlations = df_train.corr()[&#39;PropertyField29&#39;][:-1].sort_values(kind=&quot;quicksort&quot;) print(correlations) . Field7 -0.739375 PersonalField15 -0.041584 PersonalField33 -0.033010 PersonalField32 -0.025723 SalesField4 -0.023916 ... PersonalField72 NaN PersonalField73 NaN PropertyField6 NaN GeographicField10A NaN GeographicField10B NaN Name: PropertyField29, Length: 269, dtype: float64 . df_train[[&#39;PropertyField29&#39;,&#39;Field7&#39;]].head() . PropertyField29 Field7 . QuoteNumber . 1 0.0 | 23 | . 2 NaN | 7 | . 4 NaN | 7 | . 6 NaN | 10 | . 8 0.0 | 23 | . df_train[[&#39;PropertyField29&#39;,&#39;Field7&#39;]].plot.scatter(x=&#39;Field7&#39;, ... y=&#39;PropertyField29&#39;, ... c=&#39;DarkBlue&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f526fa0b990&gt; . Status check . categorical, continuous, notused . ((#3) [&#39;Field9&#39;,&#39;Field11&#39;,&#39;PropertyField25&#39;], (#1) [&#39;Field8&#39;], (#2) [&#39;PersonalField84&#39;,&#39;PropertyField29&#39;]) . Int64 analysis . df_train[df_train.columns[df_train.dtypes==&#39;int64&#39;]].head() . Field7 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField3 SalesField4 SalesField5 SalesField6 SalesField8 SalesField9 SalesField10 SalesField11 SalesField12 SalesField13 SalesField14 SalesField15 PersonalField1 PersonalField2 PersonalField4A PersonalField4B PersonalField5 PersonalField6 PersonalField8 PersonalField9 PersonalField10A ... GeographicField43A GeographicField43B GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56A GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60A GeographicField60B GeographicField61A GeographicField61B GeographicField62A GeographicField62B . QuoteNumber . 1 23 | 17 | 23 | 17 | 23 | 15 | 22 | 16 | 22 | 13 | 22 | 13 | 23 | 2 | 1 | 7 | 18 | 3 | 8 | 0 | 5 | 5 | 24 | 48649 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | -1 | -1 | 7 | 0 | 1 | 2 | 5 | ... | 2 | 2 | 8 | 4 | 20 | 22 | 10 | 8 | 6 | 5 | 15 | 13 | 19 | 18 | 16 | 14 | 21 | 23 | 21 | 23 | 16 | 11 | 22 | 24 | 7 | 14 | -1 | 17 | 15 | 17 | 14 | 18 | 9 | 9 | -1 | 8 | -1 | 18 | -1 | 10 | . 2 7 | 6 | 8 | 6 | 8 | 5 | 7 | 5 | 8 | 13 | 22 | 13 | 23 | 5 | 9 | 5 | 14 | 6 | 18 | 1 | 5 | 5 | 11 | 26778 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 14 | 19 | 7 | 0 | 1 | 2 | 24 | ... | 10 | 13 | 23 | 24 | 11 | 15 | 21 | 24 | 6 | 11 | 21 | 21 | 18 | 15 | 20 | 20 | 13 | 12 | 12 | 12 | 15 | 9 | 13 | 11 | 11 | 20 | -1 | 9 | 18 | 21 | 8 | 7 | 10 | 10 | -1 | 11 | -1 | 17 | -1 | 20 | . 4 7 | 7 | 12 | 7 | 12 | 6 | 10 | 7 | 11 | 25 | 25 | 13 | 23 | 4 | 6 | 3 | 10 | 4 | 11 | 1 | 5 | 5 | 11 | 8751 | 0 | 0 | 2 | 2 | 0 | 0 | 0 | 1 | 1 | 16 | 21 | 7 | 0 | 1 | 2 | 7 | ... | 12 | 20 | 21 | 22 | 24 | 25 | 20 | 22 | 7 | 13 | 23 | 23 | 20 | 19 | 20 | 20 | 18 | 20 | 19 | 21 | 20 | 19 | 11 | 8 | 3 | 3 | -1 | 5 | 21 | 24 | 12 | 15 | 15 | 18 | -1 | 21 | -1 | 11 | -1 | 8 | . 6 10 | 3 | 2 | 3 | 2 | 2 | 2 | 3 | 2 | 13 | 22 | 13 | 23 | 15 | 23 | 8 | 19 | 14 | 24 | 0 | 5 | 5 | 23 | 43854 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 2 | 2 | 6 | 1 | 1 | 3 | -1 | ... | 25 | 25 | 3 | 1 | 14 | 22 | 6 | 2 | 7 | 14 | 11 | 8 | 19 | 18 | 18 | 16 | 13 | 12 | 13 | 12 | 17 | 13 | 5 | 2 | 3 | 4 | -1 | 7 | 14 | 14 | 14 | 18 | 6 | 5 | -1 | 10 | -1 | 9 | -1 | 21 | . 8 23 | 8 | 13 | 8 | 13 | 7 | 11 | 7 | 13 | 13 | 22 | 13 | 23 | 4 | 6 | 3 | 6 | 3 | 6 | 1 | 5 | 5 | 7 | 12505 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 20 | 24 | 7 | 0 | 1 | 2 | 5 | ... | 10 | 12 | 24 | 25 | 9 | 11 | 25 | 25 | 5 | 3 | 22 | 22 | 21 | 21 | 17 | 15 | 25 | 25 | 25 | 25 | 17 | 13 | 13 | 11 | 3 | 4 | -1 | 7 | 11 | 9 | 10 | 10 | 18 | 22 | -1 | 10 | -1 | 11 | -1 | 12 | . 5 rows × 263 columns . Object analysis . df[df.columns[df.dtypes==&#39;object&#39;]].head() . Field6 Field10 Field12 CoverageField8 CoverageField9 SalesField7 PersonalField7 PersonalField16 PersonalField17 PersonalField18 PersonalField19 PropertyField3 PropertyField4 PropertyField5 PropertyField7 PropertyField14 PropertyField28 PropertyField30 PropertyField31 PropertyField32 PropertyField33 PropertyField34 PropertyField36 PropertyField37 PropertyField38 GeographicField63 GeographicField64 . 0 B | 965 | N | T | D | V | N | ZA | ZE | XR | XD | N | N | Y | O | C | B | N | N | Y | G | Y | N | N | N | N | CA | . 1 F | 548 | N | T | E | P | N | XB | YJ | YE | XT | N | N | Y | N | B | B | N | O | N | H | Y | N | N | N | N | NJ | . 2 F | 548 | N | T | J | K | N | ZH | XS | YP | XC | Y | Y | Y | R | C | B | N | K | Y | H | Y | N | N | N | N | NJ | . 3 J | 1,165 | N | Y | F | V | N | XO | XE | YI | XX | N | N | Y | R | C | B | N | O | Y | G | N | N | Y | N | N | TX | . 4 E | 1,487 | N | T | F | R | N | ZA | ZE | XR | XD | Y | Y | Y | D | A | B | N | O | N | H | N | N | N | N | N | IL | . There are a lot of fields, let&#39;s make sure those that look like they should be boolean columns are set as boolean data types . Experiment with correlation (Work In Progress) . Using this article about correlation, let&#39;s see if there&#39;s any obvious correlations in our data columns . correlations = df_train.corr() print(correlations) . QuoteConversion_Flag ... GeographicField62B QuoteConversion_Flag 1.000000 ... -0.017524 Field7 -0.137532 ... 0.077039 Field8 0.127039 ... -0.092827 Field9 0.179569 ... -0.126171 Field11 -0.100376 ... 0.000361 ... ... ... ... GeographicField60B 0.012505 ... -0.185878 GeographicField61A 0.004799 ... 0.083706 GeographicField61B 0.038513 ... 0.112729 GeographicField62A -0.003142 ... 0.300912 GeographicField62B -0.017524 ... 1.000000 [270 rows x 270 columns] . Need to filter this a little better. Borrowing from StackOverflow article to get the highest correlated values . df_train.shape . (260753, 298) . c = correlations.abs() s = c.unstack() so = s.sort_values(kind=&quot;quicksort&quot;) print(so[0:10]) . GeographicField35A PropertyField20 1.279033e-07 PropertyField20 GeographicField35A 1.279033e-07 GeographicField13A PropertyField20 8.859665e-07 PropertyField20 GeographicField13A 8.859665e-07 PersonalField63 GeographicField51A 1.474951e-06 GeographicField51A PersonalField63 1.474951e-06 PersonalField22 GeographicField17B 1.733656e-06 GeographicField17B PersonalField22 1.733656e-06 PersonalField30 PersonalField38 1.945838e-06 PersonalField38 PersonalField30 1.945838e-06 dtype: float64 . Need to filter this a little better. Borrowing from StackOverflow article to get the highest correlated values . . Borrowing from seaborn documentation to make this a little more visual . from string import ascii_letters import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt sns.set_theme(style=&quot;white&quot;) # Generate a mask for the upper triangle mask = np.triu(np.ones_like(so, dtype=bool)) # Set up the matplotlib figure f, ax = plt.subplots(figsize=(11, 9)) # Generate a custom diverging colormap cmap = sns.diverging_palette(230, 20, as_cmap=True) # Draw the heatmap with the mask and correct aspect ratio sns.heatmap(so, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={&quot;shrink&quot;: .5}) .",
            "url": "https://redditech.github.io/team-fast-tabulous/kaggle/2021/06/23/Exploring-Homesite_Data.html",
            "relUrl": "/kaggle/2021/06/23/Exploring-Homesite_Data.html",
            "date": " • Jun 23, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Identify problem type",
            "content": "Install packages recommended in fastbook Ch09 . !pip install -Uqq fastbook kaggle waterfallcharts treeinterpreter dtreeviz . |████████████████████████████████| 727kB 7.6MB/s |████████████████████████████████| 61kB 9.9MB/s |████████████████████████████████| 51kB 8.6MB/s |████████████████████████████████| 194kB 48.7MB/s |████████████████████████████████| 1.2MB 52.9MB/s |████████████████████████████████| 61kB 10.5MB/s |████████████████████████████████| 61kB 10.4MB/s Building wheel for waterfallcharts (setup.py) ... done Building wheel for dtreeviz (setup.py) ... done . import fastbook fastbook.setup_book() . Mounted at /content/gdrive . from fastbook import * from fastai.vision.widgets import * from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype from fastai.tabular.all import * from sklearn.ensemble import RandomForestRegressor from sklearn.tree import DecisionTreeRegressor from dtreeviz.trees import * from IPython.display import Image, display_svg, SVG pd.options.display.max_rows = 20 pd.options.display.max_columns = 8 . Upload your kaggle.json API key . btn_upload = widgets.FileUpload(description=&quot;kaggle.json&quot;) btn_upload . Save credentials . cred_path = Path(&#39;~/.kaggle/kaggle.json&#39;).expanduser() if not cred_path.parent.exists(): cred_path.parent.mkdir() if len(btn_upload.data) &gt; 0: with open(cred_path, mode=&quot;wb&quot;) as cred_file: cred_file.write(btn_upload.data[-1]) cred_path.chmod(0o600) . from kaggle import api . Note that &#39;!pip install kaggle&#39; does not update cli kaggle in Google colab and is only v1.5.4 while kaggle.api is v1.5.12 . !kaggle --version . Kaggle API 1.5.4 . Python&#39;s kaggle.api is using a more recent version . api.__version__ . &#39;1.5.12&#39; . Get data from kaggle, extract and store in _data . path_hqc = (Path.cwd()/&quot;_data&quot;) path_hqc.mkdir(exist_ok=True) Path.BASE_PATH = path_hqc api.competition_download_cli(&#39;homesite-quote-conversion&#39;, path=path_hqc) file_extract(path_hqc/&quot;homesite-quote-conversion.zip&quot;) file_extract(path_hqc/&quot;train.csv.zip&quot;) file_extract(path_hqc/&quot;test.csv.zip&quot;) . 0%| | 0.00/62.0M [00:00&lt;?, ?B/s] . Downloading homesite-quote-conversion.zip to /content/_data . 100%|██████████| 62.0M/62.0M [00:00&lt;00:00, 174MB/s] . . Check what the data looks like . df = pd.read_csv(path_hqc/&quot;train.csv&quot;, low_memory=False) df.head() . QuoteNumber Original_Quote_Date QuoteConversion_Flag Field6 ... GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 1 | 2013-08-16 | 0 | B | ... | -1 | 10 | N | CA | . 1 2 | 2014-04-22 | 0 | F | ... | -1 | 20 | N | NJ | . 2 4 | 2014-08-25 | 0 | F | ... | -1 | 8 | N | NJ | . 3 6 | 2013-04-15 | 0 | J | ... | -1 | 21 | N | TX | . 4 8 | 2014-01-25 | 0 | E | ... | -1 | 12 | N | IL | . 5 rows × 299 columns . Check how much data we have and check if QuoteNumber is unique . df.shape, len(df[&#39;QuoteNumber&#39;].unique()) . ((260753, 299), 260753) . Conclusion: QuoteNumber is unique . We don&#39;t want to use QuoteNumber as a feature but we could use it as the index . df = df.set_index(&#39;QuoteNumber&#39;) . Examine data types in train.csv . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 260753 entries, 1 to 434588 Columns: 298 entries, Original_Quote_Date to GeographicField64 dtypes: float64(6), int64(264), object(28) memory usage: 594.8+ MB . Find the 28 fields which do not have numeric datatypes . from collections import defaultdict dct_fields_by_dtype = defaultdict(list) for i, dt in enumerate(df.dtypes): dct_fields_by_dtype[dt].append(df.dtypes.index[i]) print(&quot;dtypes in train.csv:&quot;, dct_fields_by_dtype.keys()) print(&quot;fields for object dtype:&quot;, dct_fields_by_dtype[np.dtype(&#39;O&#39;)]) print(&quot;number of fields of object dtype:&quot;, len(dct_fields_by_dtype[np.dtype(&#39;O&#39;)])) . dtypes in train.csv: dict_keys([dtype(&#39;O&#39;), dtype(&#39;int64&#39;), dtype(&#39;float64&#39;)]) fields for object dtype: [&#39;Original_Quote_Date&#39;, &#39;Field6&#39;, &#39;Field10&#39;, &#39;Field12&#39;, &#39;CoverageField8&#39;, &#39;CoverageField9&#39;, &#39;SalesField7&#39;, &#39;PersonalField7&#39;, &#39;PersonalField16&#39;, &#39;PersonalField17&#39;, &#39;PersonalField18&#39;, &#39;PersonalField19&#39;, &#39;PropertyField3&#39;, &#39;PropertyField4&#39;, &#39;PropertyField5&#39;, &#39;PropertyField7&#39;, &#39;PropertyField14&#39;, &#39;PropertyField28&#39;, &#39;PropertyField30&#39;, &#39;PropertyField31&#39;, &#39;PropertyField32&#39;, &#39;PropertyField33&#39;, &#39;PropertyField34&#39;, &#39;PropertyField36&#39;, &#39;PropertyField37&#39;, &#39;PropertyField38&#39;, &#39;GeographicField63&#39;, &#39;GeographicField64&#39;] number of fields of object dtype: 28 . Original_Quote_Date can be converted to datetime . df[&#39;Original_Quote_Date&#39;] = pd.to_datetime(df[&#39;Original_Quote_Date&#39;]) . Recalculate breakdown now that we have changed dtype of Original_Quote_Date . dct_fields_by_dtype = defaultdict(list) for i, dt in enumerate(df.dtypes): dct_fields_by_dtype[dt].append(df.dtypes.index[i]) df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 260753 entries, 1 to 434588 Columns: 298 entries, Original_Quote_Date to GeographicField64 dtypes: datetime64[ns](1), float64(6), int64(264), object(27) memory usage: 594.8+ MB . Compare Original_Quote_Date in train.csv and test.csv . df_test = pd.read_csv(path_hqc/&quot;test.csv&quot;, low_memory=False) df_test[&quot;Original_Quote_Date&quot;] = pd.to_datetime(df_test[&quot;Original_Quote_Date&quot;]) print(&quot;train.csv&quot;, df[&#39;Original_Quote_Date&#39;].min(), df[&#39;Original_Quote_Date&#39;].max(), df.shape) print(&quot;test.csv &quot;, df_test[&#39;Original_Quote_Date&#39;].min(), df_test[&#39;Original_Quote_Date&#39;].max(), df_test.shape) . train.csv 2013-01-01 00:00:00 2015-05-18 00:00:00 (260753, 298) test.csv 2013-01-01 00:00:00 2015-05-18 00:00:00 (173836, 298) . Conclusion: overlapping date ranges (in fact identical date ranges) so don&#39;t need to consider as time series problem . Check the non-numeric values in other object fields . for col in dct_fields_by_dtype[np.dtype(&#39;O&#39;)]: print(f&quot;{col:20s} {df[col].unique()}&quot;) . Field6 [&#39;B&#39; &#39;F&#39; &#39;J&#39; &#39;E&#39; &#39;C&#39; &#39;K&#39; &#39;A&#39; &#39;D&#39;] Field10 [&#39;965&#39; &#39;548&#39; &#39;1,165&#39; &#39;1,487&#39; &#39;935&#39; &#39;564&#39; &#39;1,113&#39; &#39;1,480&#39;] Field12 [&#39;N&#39; &#39;Y&#39;] CoverageField8 [&#39;T&#39; &#39;Y&#39; &#39;X&#39; &#39;W&#39; &#39;V&#39; &#39;U&#39; &#39;Z&#39;] CoverageField9 [&#39;D&#39; &#39;E&#39; &#39;J&#39; &#39;F&#39; &#39;A&#39; &#39;G&#39; &#39;K&#39; &#39;C&#39; &#39;L&#39; &#39;B&#39; &#39;I&#39; &#39;H&#39;] SalesField7 [&#39;V&#39; &#39;P&#39; &#39;K&#39; &#39;R&#39; &#39;T&#39; &#39;Q&#39; &#39;M&#39;] PersonalField7 [&#39;N&#39; &#39;Y&#39; nan] PersonalField16 [&#39;ZA&#39; &#39;XB&#39; &#39;ZH&#39; &#39;XO&#39; &#39;YE&#39; &#39;XR&#39; &#39;ZG&#39; &#39;ZF&#39; &#39;XW&#39; &#39;XS&#39; &#39;ZT&#39; &#39;XD&#39; &#39;XH&#39; &#39;XM&#39; &#39;YH&#39; &#39;ZD&#39; &#39;XJ&#39; &#39;ZN&#39; &#39;YF&#39; &#39;XX&#39; &#39;XL&#39; &#39;XQ&#39; &#39;ZJ&#39; &#39;ZR&#39; &#39;ZW&#39; &#39;XE&#39; &#39;XC&#39; &#39;ZK&#39; &#39;XK&#39; &#39;ZC&#39; &#39;XZ&#39; &#39;XI&#39; &#39;ZE&#39; &#39;ZU&#39; &#39;YI&#39; &#39;XP&#39; &#39;ZO&#39; &#39;ZP&#39; &#39;ZB&#39; &#39;XF&#39; &#39;ZS&#39; &#39;XT&#39; &#39;XY&#39; &#39;ZQ&#39; &#39;ZI&#39; &#39;XV&#39; &#39;XU&#39; &#39;XN&#39; &#39;ZV&#39; &#39;ZL&#39;] PersonalField17 [&#39;ZE&#39; &#39;YJ&#39; &#39;XS&#39; &#39;XE&#39; &#39;XU&#39; &#39;ZQ&#39; &#39;YY&#39; &#39;XV&#39; &#39;ZF&#39; &#39;XK&#39; &#39;YS&#39; &#39;ZK&#39; &#39;YF&#39; &#39;YV&#39; &#39;XG&#39; &#39;ZL&#39; &#39;ZH&#39; &#39;ZW&#39; &#39;XH&#39; &#39;ZU&#39; &#39;YH&#39; &#39;XC&#39; &#39;ZV&#39; &#39;XR&#39; &#39;ZI&#39; &#39;XX&#39; &#39;YR&#39; &#39;XW&#39; &#39;ZC&#39; &#39;YZ&#39; &#39;YU&#39; &#39;YX&#39; &#39;ZA&#39; &#39;ZP&#39; &#39;XI&#39; &#39;YN&#39; &#39;YL&#39; &#39;YK&#39; &#39;ZN&#39; &#39;XT&#39; &#39;ZT&#39; &#39;XQ&#39; &#39;XB&#39; &#39;YI&#39; &#39;YM&#39; &#39;XL&#39; &#39;YQ&#39; &#39;ZG&#39; &#39;ZS&#39; &#39;YT&#39; &#39;ZO&#39; &#39;YE&#39; &#39;XN&#39; &#39;ZM&#39; &#39;XM&#39; &#39;YG&#39; &#39;YP&#39; &#39;XD&#39; &#39;ZD&#39; &#39;YW&#39; &#39;XJ&#39; &#39;ZB&#39; &#39;XP&#39; &#39;XO&#39; &#39;ZR&#39; &#39;XY&#39;] PersonalField18 [&#39;XR&#39; &#39;YE&#39; &#39;YP&#39; &#39;YI&#39; &#39;XQ&#39; &#39;ZW&#39; &#39;XT&#39; &#39;XF&#39; &#39;XS&#39; &#39;YG&#39; &#39;ZF&#39; &#39;XZ&#39; &#39;XI&#39; &#39;XK&#39; &#39;YF&#39; &#39;ZE&#39; &#39;YQ&#39; &#39;ZP&#39; &#39;YL&#39; &#39;ZD&#39; &#39;XW&#39; &#39;YN&#39; &#39;YK&#39; &#39;ZJ&#39; &#39;ZK&#39; &#39;ZC&#39; &#39;XU&#39; &#39;ZN&#39; &#39;XP&#39; &#39;XL&#39; &#39;XM&#39; &#39;ZL&#39; &#39;XC&#39; &#39;ZH&#39; &#39;XG&#39; &#39;XN&#39; &#39;XY&#39; &#39;ZQ&#39; &#39;XO&#39; &#39;ZT&#39; &#39;XJ&#39; &#39;ZA&#39; &#39;ZU&#39; &#39;XE&#39; &#39;ZV&#39; &#39;ZS&#39; &#39;YR&#39; &#39;YH&#39; &#39;YJ&#39; &#39;ZR&#39; &#39;ZO&#39; &#39;YO&#39; &#39;ZM&#39; &#39;XD&#39; &#39;YM&#39; &#39;XX&#39; &#39;ZB&#39; &#39;XH&#39; &#39;XV&#39; &#39;ZG&#39; &#39;ZI&#39;] PersonalField19 [&#39;XD&#39; &#39;XT&#39; &#39;XC&#39; &#39;XX&#39; &#39;ZQ&#39; &#39;ZT&#39; &#39;ZO&#39; &#39;YJ&#39; &#39;ZN&#39; &#39;YH&#39; &#39;ZI&#39; &#39;YN&#39; &#39;YF&#39; &#39;YK&#39; &#39;XY&#39; &#39;XI&#39; &#39;ZA&#39; &#39;ZW&#39; &#39;ZV&#39; &#39;XU&#39; &#39;ZL&#39; &#39;XK&#39; &#39;XW&#39; &#39;XF&#39; &#39;ZK&#39; &#39;YE&#39; &#39;XB&#39; &#39;XZ&#39; &#39;XP&#39; &#39;ZJ&#39; &#39;YM&#39; &#39;XO&#39; &#39;YG&#39; &#39;XN&#39; &#39;ZR&#39; &#39;ZE&#39; &#39;ZB&#39; &#39;ZG&#39; &#39;YL&#39; &#39;ZF&#39; &#39;XR&#39; &#39;XJ&#39; &#39;XM&#39; &#39;ZP&#39; &#39;XQ&#39; &#39;XV&#39; &#39;ZH&#39; &#39;XE&#39; &#39;ZU&#39; &#39;ZM&#39; &#39;XG&#39; &#39;ZD&#39; &#39;XH&#39; &#39;XL&#39; &#39;YI&#39; &#39;XS&#39; &#39;ZC&#39;] PropertyField3 [&#39;N&#39; &#39;Y&#39; nan] PropertyField4 [&#39;N&#39; &#39;Y&#39; nan] PropertyField5 [&#39;Y&#39; &#39;N&#39;] PropertyField7 [&#39;O&#39; &#39;N&#39; &#39;R&#39; &#39;D&#39; &#39;S&#39; &#39;J&#39; &#39;I&#39; &#39;Q&#39; &#39;A&#39; &#39;K&#39; &#39;G&#39; &#39;F&#39; &#39;H&#39; &#39;E&#39; &#39;L&#39; &#39;C&#39; &#39;P&#39; &#39;M&#39; &#39;B&#39;] PropertyField14 [&#39;C&#39; &#39;B&#39; &#39;A&#39; &#39;D&#39;] PropertyField28 [&#39;B&#39; &#39;D&#39; &#39;A&#39; &#39;C&#39;] PropertyField30 [&#39;N&#39; &#39;Y&#39;] PropertyField31 [&#39;N&#39; &#39;O&#39; &#39;K&#39; &#39;M&#39;] PropertyField32 [&#39;Y&#39; &#39;N&#39; nan] PropertyField33 [&#39;G&#39; &#39;H&#39; &#39;E&#39; &#39;F&#39;] PropertyField34 [&#39;Y&#39; &#39;N&#39; nan] PropertyField36 [&#39;N&#39; &#39;Y&#39; nan] PropertyField37 [&#39;N&#39; &#39;Y&#39;] PropertyField38 [&#39;N&#39; &#39;Y&#39; nan] GeographicField63 [&#39;N&#39; &#39;Y&#39; &#39; &#39;] GeographicField64 [&#39;CA&#39; &#39;NJ&#39; &#39;TX&#39; &#39;IL&#39;] . Field10 looks like integers stored as strings so convert to ints . df[&#39;Field10&#39;] = df[&#39;Field10&#39;].str.replace(&quot;,&quot;, &quot;&quot;).astype(int) . Recalculate breakdown now that we have changed dtype of Field10 . dct_fields_by_dtype = defaultdict(list) for i, dt in enumerate(df.dtypes): dct_fields_by_dtype[dt].append(df.dtypes.index[i]) df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 260753 entries, 1 to 434588 Columns: 298 entries, Original_Quote_Date to GeographicField64 dtypes: datetime64[ns](1), float64(6), int64(265), object(26) memory usage: 594.8+ MB .",
            "url": "https://redditech.github.io/team-fast-tabulous/jupyter/2021/06/20/Identify-problem-type.html",
            "relUrl": "/jupyter/2021/06/20/Identify-problem-type.html",
            "date": " • Jun 20, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Basic Random Forest Model Kaggle Score 0.953",
            "content": "This is a Colab notebook. Basic Random Forest Model without any hyperparameter tuning and feature engineering. Kaggle Score 0.953. This can be a baseline model . import pandas as pd import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.metrics import mean_squared_error from sklearn.metrics import plot_roc_curve from sklearn.metrics import plot_confusion_matrix from sklearn.metrics import roc_auc_score import matplotlib.pyplot as plt from fastbook import * from fastai.tabular.all import * from dtreeviz.trees import * from IPython.display import Image, display_svg, SVG import random as rd pd.options.display.max_rows = 20 pd.options.display.max_columns = 8 . Download Data From Kaggle . !mkdir -p ~/.kaggle !cp /content/gdrive/MyDrive/Kaggle/kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . path = Path(&#39;/content/gdrive/MyDrive/Kaggle/&#39; + &#39;data/homesite-quote&#39;) path.mkdir(parents=True, exist_ok=True) path . Path(&#39;/content/gdrive/MyDrive/Kaggle/data/homesite-quote&#39;) . !kaggle competitions download -c homesite-quote-conversion -p /content/gdrive/MyDrive/Kaggle/data/homesite-quote . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) Downloading train.csv.zip to /content/gdrive/MyDrive/Kaggle/data/homesite-quote 89% 33.0M/37.1M [00:00&lt;00:00, 63.7MB/s] 100% 37.1M/37.1M [00:00&lt;00:00, 83.8MB/s] Downloading sample_submission.csv.zip to /content/gdrive/MyDrive/Kaggle/data/homesite-quote 0% 0.00/258k [00:00&lt;?, ?B/s] 100% 258k/258k [00:00&lt;00:00, 36.1MB/s] Downloading test.csv.zip to /content/gdrive/MyDrive/Kaggle/data/homesite-quote 53% 13.0M/24.7M [00:00&lt;00:00, 68.0MB/s] 100% 24.7M/24.7M [00:00&lt;00:00, 82.5MB/s] . ! unzip -q -n &#39;{path}/train.csv.zip&#39; -d &#39;{path}&#39; ! unzip -q -n &#39;{path}/test.csv.zip&#39; -d &#39;{path}&#39; . Import Data . df = pd.read_csv(path/&#39;train.csv&#39;, low_memory=False) test_df = pd.read_csv(path/&#39;test.csv&#39;, low_memory=False) . Data Prep . def drop_cols(df): df.drop([&#39;Original_Quote_Date&#39;],axis=1,inplace=True) return df train_df=df train_df = drop_cols(train_df) test_df = drop_cols(test_df) . cols_to_delete = train_df.isna().sum()[train_df.isna().sum() &gt; 0].index def drop_cols_from_list(df,cols_to_delete): df.drop(cols_to_delete,axis=1,inplace=True) return df train_df = drop_cols_from_list(train_df,cols_to_delete) test_df = drop_cols_from_list(test_df,cols_to_delete) . cols_to_drop = [] for i in set(train_df.columns) - set(train_df._get_numeric_data().columns): if (train_df.loc[:,i].nunique() &gt;= 3): cols_to_drop.append(i) train_df = drop_cols_from_list(train_df,cols_to_drop) test_df = drop_cols_from_list(test_df,cols_to_drop) . cls_to_encode = set(train_df.columns) - set(train_df._get_numeric_data().columns) def ohe(df,cls_to_encode): df = pd.get_dummies(df,columns=cls_to_encode,drop_first=True) return df train_df = ohe(train_df,cls_to_encode) test_df = ohe(test_df,cls_to_encode) . test_df.drop(list(set(test_df.columns) - set(train_df.columns)),axis=1,inplace=True) . Model . X = train_df.drop(&#39;QuoteConversion_Flag&#39;,axis=1) y = train_df.QuoteConversion_Flag . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = y, random_state=42) . train_idx=pd.DataFrame(X_train.index) valid_idx=pd.DataFrame(X_test.index) . train_idx.to_csv(path/&#39;train_idx.csv&#39;,index=False) valid_idx.to_csv(path/&#39;valid_idx.csv&#39;,index=False) . rfc = RandomForestClassifier(n_jobs=-1) rfc.fit(X_train,y_train) . RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False) . print(&quot;train accuracy score = &quot;, accuracy_score(y_train,rfc.predict(X_train))) print(&quot;test accuracy score = &quot;, accuracy_score(y_test,rfc.predict(X_test))) . train accuracy score = 0.9999952061821076 test accuracy score = 0.9162623919004429 . roc_auc_score(y_train,rfc.predict(X_train)),roc_auc_score(y_test,rfc.predict(X_test)) . (0.9999872171801099, 0.8078645543138884) . plot_roc_curve(rfc, X_test, y_test) plt.show() . plot_confusion_matrix(rfc, X_test, y_test,values_format=&#39;d&#39;) plt.show() . output_submission = pd.DataFrame(zip(test_df.QuoteNumber,rfc.predict_proba(test_df)[:,1]), columns = [&#39;QuoteNumber&#39;,&#39;QuoteConversion_Flag&#39;]) output_submission.to_csv(path/&#39;output_submission.csv&#39;,index=False) .",
            "url": "https://redditech.github.io/team-fast-tabulous/jupyter/2021/06/20/Basic-Random-Forest-Score-0.953.html",
            "relUrl": "/jupyter/2021/06/20/Basic-Random-Forest-Score-0.953.html",
            "date": " • Jun 20, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "Getting Kaggle Data for Homesite",
            "content": "Introduction . This notebook is a demonstration of what I did to get the Homesite competition data from Kaggle. . Tools . If you haven&#39;t installed the Kaggle tools, you would need to do so like this . !pip install kaggle --upgrade . Requirement already satisfied: kaggle in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (1.5.12) Requirement already satisfied: tqdm in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (4.59.0) Requirement already satisfied: requests in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (2.25.1) Requirement already satisfied: python-slugify in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (5.0.2) Requirement already satisfied: urllib3 in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (1.26.4) Requirement already satisfied: python-dateutil in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (2.8.1) Requirement already satisfied: certifi in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (2021.5.30) Requirement already satisfied: six&gt;=1.10 in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (1.16.0) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from requests-&gt;kaggle) (4.0.0) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from requests-&gt;kaggle) (2.10) . Then you need to download your Kaggle key following the instructions here . You may need to create the ~/.kaggle directory manually before copying the downloaded file kaggle.json file to that folder. . Follow the instructions as coded to download the kaggle data. For me, I use a _data folder that is excluded from checkins in the .gitignore to make sure I do not inadvertently upload it when I check in any changes to the repository, since I&#39;m writing this blog post as I am coding. I&#39;m not using any of the tabular stuff from fastai library yet, but I want to use some of the nice utility functions it gives me to make sure I do the command line stuff correctly and capture it in the notebook. So I need to install it. . !mamba install -c fastchan fastai -y . __ __ __ __ / / / / / / / / ███████████████/ /██/ /██/ /██/ /████████████████████████ / / / / / ____ / / _/ _/ _/ o __, / _/ _____/ ` |/ ███╗ ███╗ █████╗ ███╗ ███╗██████╗ █████╗ ████╗ ████║██╔══██╗████╗ ████║██╔══██╗██╔══██╗ ██╔████╔██║███████║██╔████╔██║██████╔╝███████║ ██║╚██╔╝██║██╔══██║██║╚██╔╝██║██╔══██╗██╔══██║ ██║ ╚═╝ ██║██║ ██║██║ ╚═╝ ██║██████╔╝██║ ██║ ╚═╝ ╚═╝╚═╝ ╚═╝╚═╝ ╚═╝╚═════╝ ╚═╝ ╚═╝ mamba (0.13.0) supported by @QuantStack GitHub: https://github.com/mamba-org/mamba Twitter: https://twitter.com/QuantStack █████████████████████████████████████████████████████████████ Looking for: [&#39;fastai&#39;] fastchan/noarch [=&gt; ] (--:--) No change fastchan/noarch [====================] (00m:00s) No change fastchan/osx-64 [=&gt; ] (--:--) No change fastchan/osx-64 [====================] (00m:00s) No change pkgs/main/osx-64 [=&gt; ] (--:--) No change pkgs/main/osx-64 [====================] (00m:00s) No change pkgs/r/osx-64 [&gt; ] (--:--) No change pkgs/r/osx-64 [====================] (00m:00s) No change pkgs/main/noarch [&gt; ] (--:--) No change pkgs/main/noarch [====================] (00m:00s) No change pkgs/r/noarch [=&gt; ] (--:--) No change pkgs/r/noarch [====================] (00m:00s) No change Transaction Prefix: /usr/local/anaconda3/envs/fastai All requested packages already installed . Now I have to load fastai and I can get the utilities I want. . from fastai.tabular.all import * . Path.cwd() . Path(&#39;/Users/nissan/code/team-fast-tabulous/_notebooks&#39;) . os.chdir(&#39;../_data&#39;) Path.cwd() . Path(&#39;/Users/nissan/code/team-fast-tabulous/_data&#39;) . !kaggle competitions download -c homesite-quote-conversion . homesite-quote-conversion.zip: Skipping, found more recently modified local copy (use --force to force download) . path = Path.cwd() path.ls() . (#7) [Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/test.csv&#39;),Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/homesite-quote-conversion.zip&#39;),Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/train.csv&#39;),Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/test.csv.zip&#39;),Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/train.csv.zip&#39;),Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/sample_submission.csv.zip&#39;),Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/sample_submission.csv&#39;)] . Unfortunately, untar_data doesn&#39;t support zip format, so we need another tool, file_extract for this . file_extract(&#39;homesite-quote-conversion.zip&#39;) . path.ls() . (#7) [Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/test.csv&#39;),Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/homesite-quote-conversion.zip&#39;),Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/train.csv&#39;),Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/test.csv.zip&#39;),Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/train.csv.zip&#39;),Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/sample_submission.csv.zip&#39;),Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/sample_submission.csv&#39;)] . file_extract(&#39;test.csv.zip&#39;) file_extract(&#39;train.csv.zip&#39;) file_extract(&#39;sample_submission.csv.zip&#39;) path.ls() . (#7) [Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/test.csv&#39;),Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/homesite-quote-conversion.zip&#39;),Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/train.csv&#39;),Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/test.csv.zip&#39;),Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/train.csv.zip&#39;),Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/sample_submission.csv.zip&#39;),Path(&#39;/Users/nissan/code/team-fast-tabulous/_data/sample_submission.csv&#39;)] . And there you have it. I have the uncompressed data available, and can now start doing some exploratory data analysis on it. .",
            "url": "https://redditech.github.io/team-fast-tabulous/jupyter/collab/2021/06/19/Get-Kaggle-Data.html",
            "relUrl": "/jupyter/collab/2021/06/19/Get-Kaggle-Data.html",
            "date": " • Jun 19, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "Finding Optimum XGBoost Parameters for Tabular For Homesite Competition",
            "content": "Introduction . Using the code here we will look at the optimal number of estimators, depth, and then combination of the two when using XGBoost to generate a model . Notes: . Possible improvements if manually coding would be to add subsamples as a variable, however with both the increase of permutations and the long running times for GridSearch and a look to explore other options like Optuna and Bayesian Search to automate this, I limited it to just max_depth and learning_rate as the additional variables for now | Because of timeouts for long ranges, broke up the ranges for the various parameters into groupings, will only explore a subsequent grouping if there is an optimal value that is max in a previous grouping. This will also give a more manageable range for the permutations when trying to find the best combination of all parameters | Added to tune learning_rate based on this code | Removed the triage variable, FillMissing will handle NA values in categorical fields appropriately, but just need to filter it out from modifying categories function | Changed the categorize functions from last notebook to exclude any columns in y_names from being evaluated since these shouldn&#39;t be part of the model training as a parameter | Added code to save all models to reuse in another notebook that submits to Kaggle for test evaluation | . Setup . Adding based on tutorial for notebooks . %matplotlib inline %reload_ext autoreload %autoreload 2 !pip install -Uqq fastai !pip install kaggle from fastai.tabular.all import * . |████████████████████████████████| 194kB 14.1MB/s |████████████████████████████████| 61kB 9.0MB/s Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) . global gdrive #colab only code block gdrive = Path(&#39;/content/gdrive/My Drive&#39;) from google.colab import drive if not gdrive.exists(): drive.mount(str(gdrive.parent)) !mkdir -p ~/.kaggle !cp /content/gdrive/MyDrive/Kaggle/kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . Mounted at /content/gdrive . from kaggle import api path = Path.cwd() path.ls() . (#3) [Path(&#39;/content/.config&#39;),Path(&#39;/content/gdrive&#39;),Path(&#39;/content/sample_data&#39;)] . Only run the next three lines the first time if in a local repository. This will prevent large training data files and model files being checked into Github . !touch .gitignore . !echo &quot;_data&quot; &gt; .gitignore . !mkdir _data . os.chdir(&#39;_data&#39;) Path.cwd() . Path(&#39;/content/_data&#39;) . Back to it . os.chdir(path/&quot;gdrive/MyDrive/Kaggle/&quot;) # colab only code Path.cwd() . Path(&#39;/content/gdrive/MyDrive/Kaggle&#39;) . path = Path.cwd()/&quot;homesite_competition_data&quot; path.mkdir(exist_ok=True) Path.BASE_PATH = path api.competition_download_cli(&#39;homesite-quote-conversion&#39;, path=path) file_extract(path/&quot;homesite-quote-conversion.zip&quot;) file_extract(path/&quot;train.csv.zip&quot;) file_extract(path/&quot;test.csv.zip&quot;) path.ls() . homesite-quote-conversion.zip: Skipping, found more recently modified local copy (use --force to force download) . (#6) [Path(&#39;homesite-quote-conversion.zip&#39;),Path(&#39;sample_submission.csv.zip&#39;),Path(&#39;test.csv.zip&#39;),Path(&#39;train.csv.zip&#39;),Path(&#39;train.csv&#39;),Path(&#39;test.csv&#39;)] . Settings . test_size = 0.3 y_block=CategoryBlock() # n_estimators = [50, 100, 150, 200] # max_depth = [2, 4, 6, 8] n_estimators_range = range(50,500,50) n_estimators_range1 = range(500,1050,50) n_estimators_range2 = range(1050,1500,50) n_estimators_range3 = range(1550,2050,50) max_depth_range = range(1, 12, 2) max_depth_range1 = range(13,22,2) max_depth_range2 = range(23,32,2) learning_rate_range = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3] subsample_range = [0.1,0.3,0.5,0.7,1.0] sampling_methods = [&#39;uniform&#39;,&#39;gradient_based&#39;] random_seed =42 n_splits = 10 # 10 folds scoring = &quot;roc_auc&quot; category_threshold = 20 . from sklearn.metrics import roc_auc_score # valid_score = roc_auc_score(to_np(targs), to_np(preds[:,1])) # valid_score . The GridsearchCV functions . Customising functions from this page and this pageto work with our dataset . import xgboost as xgb import matplotlib from sklearn.model_selection import StratifiedKFold from sklearn.model_selection import GridSearchCV from sklearn.preprocessing import LabelEncoder from matplotlib import pyplot . This function will tune for n_estimators only . Notes . Used https://scikit-learn.org/stable/modules/model_evaluation.html#scoring to find correct scoring string | Reading more on StratifiedKFold to understand if I used the n_splits correctly, I chose 10 based on this article | . def xgboost_tune_estimators(X, y, n_estimators_range, n_splits, sampling_methods, random_seed, scoring): matplotlib.use(&#39;Agg&#39;) label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = xgb.XGBClassifier(tree_method=&#39;gpu_hist&#39;, gpu_id=0, verbosity=2, sampling_methods=sampling_methods) param_grid = dict(n_estimators=n_estimators_range) kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed) grid_search = GridSearchCV(model, param_grid, scoring=scoring, n_jobs=-1, cv=kfold, verbose=4) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(&quot;Best: %f using %s&quot; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[&#39;mean_test_score&#39;] stds = grid_result.cv_results_[&#39;std_test_score&#39;] params = grid_result.cv_results_[&#39;params&#39;] for mean, stdev, param in zip(means, stds, params): print(&quot;%f (%f) with: %r&quot; % (mean, stdev, param)) # plot pyplot.errorbar(n_estimators_range, means, yerr=stds) pyplot.title(&quot;XGBoost n_estimators vs Log Loss&quot;) pyplot.xlabel(&#39;n_estimators&#39;) pyplot.ylabel(&#39;Log Loss&#39;) pyplot.savefig(&#39;n_estimators.png&#39;) . This function will tune for max_depth value only . def xgboost_tune_max_depth(X,y, max_depth_range, n_splits, sampling_methods, random_seed, scoring): matplotlib.use(&#39;Agg&#39;) # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = xgb.XGBClassifier(tree_method=&#39;gpu_hist&#39;, gpu_id=0, verbosity=2, sampling_methods=sampling_methods) print(max_depth_range) param_grid = dict(max_depth=max_depth_range) kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed) grid_search = GridSearchCV(model, param_grid, scoring=scoring, n_jobs=-1, cv=kfold, verbose=4) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(&quot;Best: %f using %s&quot; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[&#39;mean_test_score&#39;] stds = grid_result.cv_results_[&#39;std_test_score&#39;] params = grid_result.cv_results_[&#39;params&#39;] for mean, stdev, param in zip(means, stds, params): print(&quot;%f (%f) with: %r&quot; % (mean, stdev, param)) # plot pyplot.errorbar(max_depth_range, means, yerr=stds) pyplot.title(&quot;XGBoost max_depth vs Log Loss&quot;) pyplot.xlabel(&#39;max_depth&#39;) pyplot.ylabel(&#39;Log Loss&#39;) pyplot.savefig(&#39;max_depth.png&#39;) . This function will tune for learning_rate . def xgboost_tune_lr(X, y, lr_range, n_splits, sampling_methods, random_seed, scoring): matplotlib.use(&#39;Agg&#39;) label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = xgb.XGBClassifier(tree_method=&#39;gpu_hist&#39;, gpu_id=0, verbosity=2, sampling_methods=sampling_methods) param_grid = dict(learning_rate=lr_range) kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed) grid_search = GridSearchCV(model, param_grid, scoring=scoring, n_jobs=-1, cv=kfold, verbose=4) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(&quot;Best: %f using %s&quot; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[&#39;mean_test_score&#39;] stds = grid_result.cv_results_[&#39;std_test_score&#39;] params = grid_result.cv_results_[&#39;params&#39;] for mean, stdev, param in zip(means, stds, params): print(&quot;%f (%f) with: %r&quot; % (mean, stdev, param)) # plot pyplot.errorbar(lr_range, means, yerr=stds) pyplot.title(&quot;XGBoost learning_rate vs Log Loss&quot;) pyplot.xlabel(&#39;learning_rate&#39;) pyplot.ylabel(&#39;Log Loss&#39;) pyplot.savefig(&#39;learning_rate.png&#39;) . This function will tune for n_estimators, max_depth, learning_rate in combination (takes really long to run) . def xgboost_tune_n_estimators_and_max_depth_and_lr(X, y, n_estimators_range, max_depth_range, lr_range, n_splits, sampling_methods, random_seed, scoring): matplotlib.use(&#39;Agg&#39;) # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = xgb.XGBClassifier(tree_method=&#39;gpu_hist&#39;, gpu_id=0, verbosity=2, sampling_methods=sampling_methods) print(n_estimators_range) print(max_depth_range) print(lr_range) param_grid = dict(max_depth=max_depth_range, n_estimators=n_estimators_range, learning_rate=lr_range) kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed) grid_search = GridSearchCV(model, param_grid, scoring=scoring, n_jobs=-1, cv=kfold, verbose=4) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(&quot;Best: %f using %s&quot; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[&#39;mean_test_score&#39;] stds = grid_result.cv_results_[&#39;std_test_score&#39;] params = grid_result.cv_results_[&#39;params&#39;] for mean, stdev, param in zip(means, stds, params): print(&quot;%f (%f) with: %r&quot; % (mean, stdev, param)) # plot results # scores = np.array(means).reshape(len(max_depth_range), len(n_estimators_range)) # for i, value in enumerate(max_depth_range): # pyplot.plot(n_estimators_range, scores[i], label=&#39;depth: &#39; + str(value)) # pyplot.legend() # pyplot.xlabel(&#39;n_estimators&#39;) # pyplot.ylabel(&#39;Log Loss&#39;) # pyplot.savefig(&#39;n_estimators_vs_max_depth.png&#39;) . My useful functions . def reassign_to_categorical(field, df, y_names, continuous, categorical): if ((df[field].isna().sum()==0) and (field not in y_names)): field_categories = df[field].unique() df[field] = df[field].astype(&#39;category&#39;) df[field].cat.set_categories(field_categories, inplace=True) if field in continuous: continuous.remove(field) if field not in categorical: categorical.append(field) return df, continuous, categorical . def categorize( df, y_names, cont_names, cat_names, category_threshold=50): for field in df.columns: if ((len(df[field].unique()) &lt;= category_threshold) and (type(df[field].dtype) != pd.core.dtypes.dtypes.CategoricalDtype)): reassign_to_categorical(field, df, y_names, cont_names, cat_names) return df, cont_names, cat_names . def homesite_prep(df_train, df_test, y_names, category_threshold): df_train.QuoteConversion_Flag = df_train.QuoteConversion_Flag.astype(dtype=&#39;boolean&#39;) df_train = df_train.set_index(&#39;QuoteNumber&#39;) df_test = df_test.set_index(&#39;QuoteNumber&#39;) df_train[&#39;Original_Quote_Date&#39;] = pd.to_datetime(df_train[&#39;Original_Quote_Date&#39;]) df_test[&#39;Original_Quote_Date&#39;] = pd.to_datetime(df_test[&#39;Original_Quote_Date&#39;]) df_train = add_datepart(df_train, &#39;Original_Quote_Date&#39;) df_test = add_datepart(df_test, &#39;Original_Quote_Date&#39;) cont_names, cat_names = cont_cat_split(df_train, dep_var=y_names) df_train, cont_names, cat_names = categorize(df_train, y_names, cont_names, cat_names, category_threshold) return df_train, df_test, cont_names, cat_names . def find_y_columns(df_train, df_test): y_columns = df_train.columns.difference(df_test.columns) return y_columns . Load the data . df_train = pd.read_csv(path/&quot;train.csv&quot;, low_memory=False) df_train.head(2) . QuoteNumber Original_Quote_Date QuoteConversion_Flag Field6 Field7 Field8 Field9 Field10 Field11 Field12 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField3 SalesField4 SalesField5 SalesField6 SalesField7 SalesField8 SalesField9 SalesField10 SalesField11 SalesField12 ... GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56A GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60A GeographicField60B GeographicField61A GeographicField61B GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 1 | 2013-08-16 | 0 | B | 23 | 0.9403 | 0.0006 | 965 | 1.0200 | N | 17 | 23 | 17 | 23 | 15 | 22 | 16 | 22 | 13 | 22 | 13 | 23 | T | D | 2 | 1 | 7 | 18 | 3 | 8 | 0 | 5 | 5 | 24 | V | 48649 | 0 | 0 | 0 | 0 | ... | 8 | 4 | 20 | 22 | 10 | 8 | 6 | 5 | 15 | 13 | 19 | 18 | 16 | 14 | 21 | 23 | 21 | 23 | 16 | 11 | 22 | 24 | 7 | 14 | -1 | 17 | 15 | 17 | 14 | 18 | 9 | 9 | -1 | 8 | -1 | 18 | -1 | 10 | N | CA | . 1 2 | 2014-04-22 | 0 | F | 7 | 1.0006 | 0.0040 | 548 | 1.2433 | N | 6 | 8 | 6 | 8 | 5 | 7 | 5 | 8 | 13 | 22 | 13 | 23 | T | E | 5 | 9 | 5 | 14 | 6 | 18 | 1 | 5 | 5 | 11 | P | 26778 | 0 | 0 | 1 | 1 | ... | 23 | 24 | 11 | 15 | 21 | 24 | 6 | 11 | 21 | 21 | 18 | 15 | 20 | 20 | 13 | 12 | 12 | 12 | 15 | 9 | 13 | 11 | 11 | 20 | -1 | 9 | 18 | 21 | 8 | 7 | 10 | 10 | -1 | 11 | -1 | 17 | -1 | 20 | N | NJ | . 2 rows × 299 columns . df_test = pd.read_csv(path/&quot;test.csv&quot;, low_memory=False) df_test.head(2) . QuoteNumber Original_Quote_Date Field6 Field7 Field8 Field9 Field10 Field11 Field12 CoverageField1A CoverageField1B CoverageField2A CoverageField2B CoverageField3A CoverageField3B CoverageField4A CoverageField4B CoverageField5A CoverageField5B CoverageField6A CoverageField6B CoverageField8 CoverageField9 CoverageField11A CoverageField11B SalesField1A SalesField1B SalesField2A SalesField2B SalesField3 SalesField4 SalesField5 SalesField6 SalesField7 SalesField8 SalesField9 SalesField10 SalesField11 SalesField12 SalesField13 ... GeographicField44A GeographicField44B GeographicField45A GeographicField45B GeographicField46A GeographicField46B GeographicField47A GeographicField47B GeographicField48A GeographicField48B GeographicField49A GeographicField49B GeographicField50A GeographicField50B GeographicField51A GeographicField51B GeographicField52A GeographicField52B GeographicField53A GeographicField53B GeographicField54A GeographicField54B GeographicField55A GeographicField55B GeographicField56A GeographicField56B GeographicField57A GeographicField57B GeographicField58A GeographicField58B GeographicField59A GeographicField59B GeographicField60A GeographicField60B GeographicField61A GeographicField61B GeographicField62A GeographicField62B GeographicField63 GeographicField64 . 0 3 | 2014-08-12 | E | 16 | 0.9364 | 0.0006 | 1,487 | 1.3045 | N | 4 | 4 | 4 | 4 | 3 | 3 | 3 | 4 | 13 | 22 | 13 | 23 | Y | K | 13 | 22 | 6 | 16 | 9 | 21 | 0 | 5 | 5 | 11 | P | 67052 | 0 | 0 | 0 | 0 | 0 | ... | 22 | 23 | 9 | 12 | 25 | 25 | 6 | 9 | 4 | 2 | 16 | 12 | 20 | 20 | 2 | 2 | 2 | 1 | 1 | 1 | 10 | 7 | 25 | 25 | -1 | 19 | 19 | 22 | 12 | 15 | 1 | 1 | -1 | 1 | -1 | 20 | -1 | 25 | Y | IL | . 1 5 | 2013-09-07 | F | 11 | 0.9919 | 0.0038 | 564 | 1.1886 | N | 8 | 14 | 8 | 14 | 7 | 12 | 8 | 13 | 13 | 22 | 13 | 23 | T | E | 4 | 5 | 3 | 6 | 3 | 6 | 1 | 5 | 5 | 4 | R | 27288 | 1 | 0 | 0 | 0 | 0 | ... | 23 | 24 | 12 | 21 | 23 | 25 | 7 | 11 | 16 | 14 | 13 | 6 | 17 | 15 | 7 | 5 | 7 | 5 | 13 | 7 | 14 | 14 | 7 | 14 | -1 | 4 | 1 | 1 | 5 | 3 | 10 | 10 | -1 | 5 | -1 | 5 | -1 | 21 | N | NJ | . 2 rows × 298 columns . y_names = find_y_columns(df_train, df_test)[0] df_train, df_test, cont_names, cat_names = homesite_prep(df_train, df_test, y_names, category_threshold) . procs = [Categorify, FillMissing, Normalize] splits = TrainTestSplitter(test_size=test_size, stratify=df_train[y_names])(df_train) . to = TabularPandas(df=df_train, procs=procs, cat_names=cat_names, cont_names=cont_names, y_names=y_names,splits=splits, y_block=y_block) . sampling_methods[0], sampling_methods[1] . (&#39;uniform&#39;, &#39;gradient_based&#39;) . Exploring single parameter tuning performance . n_estimators . %time xgboost_tune_estimators(to.xs, to.ys.values.ravel(), n_estimators_range, n_splits, sampling_methods[1], random_seed, scoring) . Fitting 10 folds for each of 9 candidates, totalling 90 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 17 tasks | elapsed: 30.4s /usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning [Parallel(n_jobs=-1)]: Done 90 out of 90 | elapsed: 5.1min finished . Best: 0.964941 using {&#39;n_estimators&#39;: 450} 0.953037 (0.000664) with: {&#39;n_estimators&#39;: 50} 0.957954 (0.000668) with: {&#39;n_estimators&#39;: 100} 0.960359 (0.000591) with: {&#39;n_estimators&#39;: 150} 0.961898 (0.000540) with: {&#39;n_estimators&#39;: 200} 0.962885 (0.000550) with: {&#39;n_estimators&#39;: 250} 0.963609 (0.000563) with: {&#39;n_estimators&#39;: 300} 0.964180 (0.000594) with: {&#39;n_estimators&#39;: 350} 0.964613 (0.000588) with: {&#39;n_estimators&#39;: 400} 0.964941 (0.000612) with: {&#39;n_estimators&#39;: 450} CPU times: user 9.27 s, sys: 1.42 s, total: 10.7 s Wall time: 5min 15s . %time xgboost_tune_estimators(to.xs, to.ys.values.ravel(), n_estimators_range1, n_splits, sampling_methods[1], random_seed, scoring) . Fitting 10 folds for each of 11 candidates, totalling 110 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers. [Parallel(n_jobs=-1)]: Done 21 tasks | elapsed: 3.8min [Parallel(n_jobs=-1)]: Done 94 tasks | elapsed: 21.1min [Parallel(n_jobs=-1)]: Done 110 out of 110 | elapsed: 25.7min finished . Best: 0.966545 using {&#39;n_estimators&#39;: 1000} 0.965281 (0.000917) with: {&#39;n_estimators&#39;: 500} 0.965500 (0.000936) with: {&#39;n_estimators&#39;: 550} 0.965691 (0.000945) with: {&#39;n_estimators&#39;: 600} 0.965869 (0.000955) with: {&#39;n_estimators&#39;: 650} 0.966008 (0.000901) with: {&#39;n_estimators&#39;: 700} 0.966127 (0.000916) with: {&#39;n_estimators&#39;: 750} 0.966237 (0.000894) with: {&#39;n_estimators&#39;: 800} 0.966322 (0.000882) with: {&#39;n_estimators&#39;: 850} 0.966377 (0.000877) with: {&#39;n_estimators&#39;: 900} 0.966441 (0.000879) with: {&#39;n_estimators&#39;: 950} 0.966545 (0.000882) with: {&#39;n_estimators&#39;: 1000} CPU times: user 26.1 s, sys: 1.63 s, total: 27.8 s Wall time: 25min 59s . %time xgboost_tune_estimators(to.xs, to.ys.values.ravel(), n_estimators_range2, n_splits, sampling_methods[1], random_seed, scoring) . Fitting 10 folds for each of 9 candidates, totalling 90 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 17 tasks | elapsed: 4.4min [Parallel(n_jobs=-1)]: Done 90 out of 90 | elapsed: 23.2min finished . Best: 0.966838 using {&#39;n_estimators&#39;: 1400} 0.966547 (0.000732) with: {&#39;n_estimators&#39;: 1050} 0.966608 (0.000750) with: {&#39;n_estimators&#39;: 1100} 0.966677 (0.000760) with: {&#39;n_estimators&#39;: 1150} 0.966705 (0.000746) with: {&#39;n_estimators&#39;: 1200} 0.966746 (0.000737) with: {&#39;n_estimators&#39;: 1250} 0.966796 (0.000750) with: {&#39;n_estimators&#39;: 1300} 0.966814 (0.000762) with: {&#39;n_estimators&#39;: 1350} 0.966838 (0.000758) with: {&#39;n_estimators&#39;: 1400} 0.966838 (0.000732) with: {&#39;n_estimators&#39;: 1450} CPU times: user 25.6 s, sys: 2.16 s, total: 27.8 s Wall time: 23min 31s . %time xgboost_tune_estimators(to.xs, to.ys.values.ravel(), n_estimators_range3, n_splits, sampling_methods[1], random_seed, scoring) . Fitting 10 folds for each of 10 candidates, totalling 100 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 17 tasks | elapsed: 6.2min /usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning [Parallel(n_jobs=-1)]: Done 90 tasks | elapsed: 32.6min [Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 36.4min finished . Best: 0.966902 using {&#39;n_estimators&#39;: 1950} 0.966850 (0.000720) with: {&#39;n_estimators&#39;: 1550} 0.966867 (0.000728) with: {&#39;n_estimators&#39;: 1600} 0.966880 (0.000725) with: {&#39;n_estimators&#39;: 1650} 0.966870 (0.000727) with: {&#39;n_estimators&#39;: 1700} 0.966897 (0.000728) with: {&#39;n_estimators&#39;: 1750} 0.966884 (0.000728) with: {&#39;n_estimators&#39;: 1800} 0.966889 (0.000721) with: {&#39;n_estimators&#39;: 1850} 0.966898 (0.000732) with: {&#39;n_estimators&#39;: 1900} 0.966902 (0.000735) with: {&#39;n_estimators&#39;: 1950} 0.966889 (0.000737) with: {&#39;n_estimators&#39;: 2000} CPU times: user 33.2 s, sys: 2.44 s, total: 35.6 s Wall time: 36min 46s . From the batches we got from 0.964941 (0.000612) with: {&#39;n_estimators&#39;: 450} to 0.966902 (0.000735) with: {&#39;n_estimators&#39;: 1950} just tuning n_estimators . max_depth . %time xgboost_tune_max_depth(to.xs,to.ys.values.ravel(), max_depth_range, n_splits, sampling_methods[1], random_seed, scoring) . range(1, 12, 2) Fitting 10 folds for each of 6 candidates, totalling 60 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 17 tasks | elapsed: 31.8s /usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning [Parallel(n_jobs=-1)]: Done 60 out of 60 | elapsed: 3.2min finished . Best: 0.965565 using {&#39;max_depth&#39;: 11} 0.940100 (0.001024) with: {&#39;max_depth&#39;: 1} 0.957954 (0.000668) with: {&#39;max_depth&#39;: 3} 0.962320 (0.000596) with: {&#39;max_depth&#39;: 5} 0.964306 (0.000669) with: {&#39;max_depth&#39;: 7} 0.965231 (0.000838) with: {&#39;max_depth&#39;: 9} 0.965565 (0.000880) with: {&#39;max_depth&#39;: 11} CPU times: user 6.8 s, sys: 1.42 s, total: 8.22 s Wall time: 3min 18s . %time xgboost_tune_max_depth(to.xs,to.ys.values.ravel(), max_depth_range1, n_splits, sampling_methods[1], random_seed, scoring) . range(13, 22, 2) Fitting 10 folds for each of 5 candidates, totalling 50 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. /usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning [Parallel(n_jobs=-1)]: Done 17 tasks | elapsed: 4.8min [Parallel(n_jobs=-1)]: Done 50 out of 50 | elapsed: 26.6min finished . Best: 0.965149 using {&#39;max_depth&#39;: 13} 0.965149 (0.000994) with: {&#39;max_depth&#39;: 13} 0.964474 (0.000903) with: {&#39;max_depth&#39;: 15} 0.964005 (0.000966) with: {&#39;max_depth&#39;: 17} 0.963855 (0.000978) with: {&#39;max_depth&#39;: 19} 0.963873 (0.000889) with: {&#39;max_depth&#39;: 21} CPU times: user 14.4 s, sys: 1.82 s, total: 16.3 s Wall time: 26min 44s . With our two range checks, it seems like our best results come with 0.965565 (0.000880) with: {&#39;max_depth&#39;: 11} and seems to get worse after that, since 0.965149 (0.000994) with: {&#39;max_depth&#39;: 13} in the 2nd range checked is less than the best in the initial range chechked, and progressively gets worse. So we should do our final fine tuning of permutations of variables only with the first range for max_depth and skip tuning with the final range of max_depth . . learning_rate . %time xgboost_tune_lr(to.xs,to.ys.values.ravel(), learning_rate_range, n_splits, sampling_methods[1], random_seed, scoring) . Fitting 10 folds for each of 6 candidates, totalling 60 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 17 tasks | elapsed: 41.0s /usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning [Parallel(n_jobs=-1)]: Done 60 out of 60 | elapsed: 1.8min finished . Best: 0.963524 using {&#39;learning_rate&#39;: 0.3} 0.870361 (0.002025) with: {&#39;learning_rate&#39;: 0.0001} 0.872141 (0.003595) with: {&#39;learning_rate&#39;: 0.001} 0.906749 (0.001726) with: {&#39;learning_rate&#39;: 0.01} 0.957872 (0.001386) with: {&#39;learning_rate&#39;: 0.1} 0.962143 (0.001049) with: {&#39;learning_rate&#39;: 0.2} 0.963524 (0.001038) with: {&#39;learning_rate&#39;: 0.3} CPU times: user 4.03 s, sys: 1.58 s, total: 5.61 s Wall time: 1min 54s . The best learning rate, if just tuning that alone is Best: 0.963524 using {&#39;learning_rate&#39;: 0.3} . n_estimators_range[0:1], n_estimators_range[1:4], n_estimators_range[4:8] . (range(50, 100, 50), range(100, 250, 50), range(250, 450, 50)) . n_estimators_agg_range = (n_estimators_range[0], n_estimators_range3[-1], 50) n_estimators_agg_range . (50, 2000, 50) . Hypothesis: Multi-parameter vs Single parameter tuning . Test if tuning can lead to lower values and as good as or better metrics than single-parameter tuning alone . Hypothesis: We don&#39;t need as large a number of n_estimators to get at least as good as a validation metric as when we had a large value for n_estimtors Need to split up the ranges more to not timeout the run in Kaggle. Will stop 1 range after we get a validation metric equal to the n_estimators=1950 value . %time xgboost_tune_n_estimators_and_max_depth_and_lr(to.train.xs, to.train.ys.values.ravel(), n_estimators_range[0:1], max_depth_range, learning_rate_range, n_splits, sampling_methods[1], random_seed, scoring) . range(50, 100, 50) range(1, 12, 2) [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3] Fitting 10 folds for each of 36 candidates, totalling 360 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. /usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning [Parallel(n_jobs=-1)]: Done 17 tasks | elapsed: 22.5s [Parallel(n_jobs=-1)]: Done 90 tasks | elapsed: 2.3min [Parallel(n_jobs=-1)]: Done 213 tasks | elapsed: 5.6min [Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 10.0min finished . Best: 0.964010 using {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 50} 0.751979 (0.003046) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 50} 0.869145 (0.002477) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 50} 0.922202 (0.006186) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 50} 0.942591 (0.001473) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 50} 0.950671 (0.001595) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 50} 0.953800 (0.001254) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 50} 0.751979 (0.003046) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 50} 0.869156 (0.002472) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 50} 0.923856 (0.005189) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 50} 0.943740 (0.001201) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 50} 0.951880 (0.001238) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 50} 0.954835 (0.001105) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 50} 0.751979 (0.003046) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 50} 0.901847 (0.002123) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 50} 0.937845 (0.001550) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 50} 0.948545 (0.001346) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 50} 0.954102 (0.001232) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 50} 0.957146 (0.001086) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 50} 0.910937 (0.002325) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 50} 0.952633 (0.001445) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 50} 0.957359 (0.001118) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 50} 0.960241 (0.000898) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 50} 0.962544 (0.000995) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 50} 0.963453 (0.000948) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 50} 0.938624 (0.001576) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 50} 0.957635 (0.001107) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 50} 0.961949 (0.000842) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 50} 0.963619 (0.000812) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 50} 0.964010 (0.000800) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 50} 0.963526 (0.001098) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 50} 0.944601 (0.001402) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 50} 0.960327 (0.001075) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 50} 0.963439 (0.001099) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 50} 0.963698 (0.001077) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 50} 0.962492 (0.000850) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 50} 0.961501 (0.001166) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 50} CPU times: user 10 s, sys: 8.4 s, total: 18.4 s Wall time: 10min 5s . Best: 0.964010 using {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 50} to our comparitor score for this range of 0.964941 (0.000612) with: {&#39;n_estimators&#39;: 450} is already pretty close, but still isn&#39;t close enough to 0.966902 (0.000735) with: {&#39;n_estimators&#39;: 1950} . %time xgboost_tune_n_estimators_and_max_depth_and_lr(to.xs, to.ys.values.ravel(), n_estimators_range[1:2], max_depth_range, learning_rate_range, n_splits, sampling_methods[1], random_seed, scoring) . range(100, 150, 50) range(1, 12, 2) [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3] Fitting 10 folds for each of 36 candidates, totalling 360 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 17 tasks | elapsed: 37.0s /usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning [Parallel(n_jobs=-1)]: Done 90 tasks | elapsed: 4.4min [Parallel(n_jobs=-1)]: Done 213 tasks | elapsed: 10.9min [Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 21.0min finished . Best: 0.965491 using {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 100} 0.752885 (0.002114) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 100} 0.870359 (0.001553) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 100} 0.926093 (0.002292) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 100} 0.942819 (0.001769) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 100} 0.951621 (0.001413) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 100} 0.954988 (0.001182) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 100} 0.752885 (0.002114) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 100} 0.873178 (0.003716) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 100} 0.926634 (0.002313) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 100} 0.945112 (0.001885) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 100} 0.952877 (0.001527) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 100} 0.956281 (0.001381) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 100} 0.832564 (0.001216) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 100} 0.907136 (0.002798) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 100} 0.942553 (0.001768) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 100} 0.951659 (0.001568) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 100} 0.956508 (0.001315) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 100} 0.959096 (0.001204) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 100} 0.940177 (0.001355) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 100} 0.957930 (0.001262) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 100} 0.962430 (0.001106) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 100} 0.964368 (0.001159) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 100} 0.965283 (0.001170) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 100} 0.965390 (0.001145) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 100} 0.947208 (0.001327) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 100} 0.962071 (0.001203) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 100} 0.965159 (0.001172) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 100} 0.965491 (0.001213) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 100} 0.964716 (0.001151) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 100} 0.963535 (0.001113) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 100} 0.951080 (0.001339) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 100} 0.963735 (0.001179) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 100} 0.965322 (0.001224) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 100} 0.964548 (0.001434) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 100} 0.962934 (0.001425) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 100} 0.961770 (0.001188) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 100} CPU times: user 14.9 s, sys: 3.2 s, total: 18.1 s Wall time: 21min 3s . Best: 0.965491 using {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 100} . to our running best of : Best: 0.964010 using {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 50} is a good step up . to our comparitor score for this range of 0.964941 (0.000612) with: {&#39;n_estimators&#39;: 450} is already better . to our individual parameter tuning best of 0.966902 (0.000735) with: {&#39;n_estimators&#39;: 1950} is not as good yet . %time xgboost_tune_n_estimators_and_max_depth_and_lr(to.xs, to.ys.values.ravel(), n_estimators_range[2:3], max_depth_range, learning_rate_range, n_splits, sampling_methods[1], random_seed, scoring) . range(150, 200, 50) range(1, 12, 2) [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3] Fitting 10 folds for each of 36 candidates, totalling 360 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 17 tasks | elapsed: 47.6s [Parallel(n_jobs=-1)]: Done 90 tasks | elapsed: 6.2min [Parallel(n_jobs=-1)]: Done 213 tasks | elapsed: 15.7min [Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 30.9min finished . Best: 0.965929 using {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 150} 0.752885 (0.002114) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 150} 0.870359 (0.001553) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 150} 0.926181 (0.002251) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 150} 0.943161 (0.002037) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 150} 0.951675 (0.001447) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 150} 0.955047 (0.001206) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 150} 0.752885 (0.002114) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 150} 0.876144 (0.001286) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 150} 0.929654 (0.002302) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 150} 0.946549 (0.001667) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 150} 0.953203 (0.001327) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 150} 0.956743 (0.001438) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 150} 0.832564 (0.001216) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 150} 0.921982 (0.002077) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 150} 0.944877 (0.002080) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 150} 0.953576 (0.001472) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 150} 0.957661 (0.001325) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 150} 0.959992 (0.001179) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 150} 0.944823 (0.001435) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 150} 0.960446 (0.001184) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 150} 0.964292 (0.001155) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 150} 0.965639 (0.001191) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 150} 0.965751 (0.001192) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 150} 0.965469 (0.001118) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 150} 0.950747 (0.001370) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 150} 0.963729 (0.001213) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 150} 0.965929 (0.001217) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 150} 0.965486 (0.001286) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 150} 0.964271 (0.001047) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 150} 0.963260 (0.001186) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 150} 0.953139 (0.001382) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 150} 0.964877 (0.001168) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 150} 0.965604 (0.001328) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 150} 0.963946 (0.001444) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 150} 0.962267 (0.001424) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 150} 0.962203 (0.001134) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 150} CPU times: user 15.5 s, sys: 2.32 s, total: 17.8 s Wall time: 31min . Best: 0.965929 using {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 150} . to our running best of : 0.965491 using {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 100} is slightly better . to our comparitor score for this range of 0.964941 (0.000612) with: {&#39;n_estimators&#39;: 450} is better . to our individual parameter tuning best of 0.966902 (0.000735) with: {&#39;n_estimators&#39;: 1950} is not as good yet . %time xgboost_tune_n_estimators_and_max_depth_and_lr(to.xs, to.ys.values.ravel(), n_estimators_range[3:4], max_depth_range, learning_rate_range, n_splits, sampling_methods[1], random_seed, scoring) . range(200, 250, 50) range(1, 12, 2) [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3] Fitting 10 folds for each of 36 candidates, totalling 360 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 17 tasks | elapsed: 1.0min [Parallel(n_jobs=-1)]: Done 90 tasks | elapsed: 8.1min [Parallel(n_jobs=-1)]: Done 213 tasks | elapsed: 20.9min [Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 41.2min finished . Best: 0.966138 using {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 200} 0.752885 (0.002114) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 200} 0.870359 (0.001553) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 200} 0.926250 (0.002253) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 200} 0.943344 (0.002040) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 200} 0.951746 (0.001453) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 200} 0.955191 (0.001309) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 200} 0.752885 (0.002114) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 200} 0.896939 (0.001870) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 200} 0.933709 (0.003101) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 200} 0.947491 (0.001635) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 200} 0.953470 (0.001314) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 200} 0.957012 (0.001366) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 200} 0.862201 (0.001466) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 200} 0.933365 (0.001476) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 200} 0.948352 (0.001533) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 200} 0.955065 (0.001480) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 200} 0.958781 (0.001261) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 200} 0.960895 (0.001197) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 200} 0.946916 (0.001392) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 200} 0.961904 (0.001176) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 200} 0.965339 (0.001210) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 200} 0.966082 (0.001181) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 200} 0.965795 (0.001199) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 200} 0.965349 (0.001140) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 200} 0.952529 (0.001363) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 200} 0.964721 (0.001225) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 200} 0.966138 (0.001321) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 200} 0.965302 (0.001219) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 200} 0.963786 (0.001006) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 200} 0.963365 (0.001264) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 200} 0.954374 (0.001307) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 200} 0.965573 (0.001305) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 200} 0.965499 (0.001258) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 200} 0.963438 (0.001267) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 200} 0.962110 (0.001268) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 200} 0.962387 (0.001110) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 200} CPU times: user 20.9 s, sys: 2.99 s, total: 23.8 s Wall time: 41min 17s . Best: 0.966138 using {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 200} . to our running best of : 0.965929 using {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 150} is better . to our comparitor score for this range of 0.964941 (0.000612) with: {&#39;n_estimators&#39;: 450} is better . to our individual parameter tuning best of 0.966902 (0.000735) with: {&#39;n_estimators&#39;: 1950} is just slightly worse off . %time xgboost_tune_n_estimators_and_max_depth_and_lr(to.xs, to.ys.values.ravel(), n_estimators_range[4:5], max_depth_range, learning_rate_range, n_splits, sampling_methods[1], random_seed, scoring) . range(250, 300, 50) range(1, 12, 2) [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3] Fitting 10 folds for each of 36 candidates, totalling 360 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 17 tasks | elapsed: 1.0min /usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning [Parallel(n_jobs=-1)]: Done 90 tasks | elapsed: 8.5min [Parallel(n_jobs=-1)]: Done 213 tasks | elapsed: 22.2min [Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 45.8min finished . Best: 0.966389 using {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 250} 0.752885 (0.002099) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 250} 0.870360 (0.001705) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 250} 0.926215 (0.001915) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 250} 0.943307 (0.001374) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 250} 0.951765 (0.001050) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 250} 0.954981 (0.001106) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 250} 0.752885 (0.002099) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 250} 0.901628 (0.002865) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 250} 0.935802 (0.001789) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 250} 0.947850 (0.001155) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 250} 0.953761 (0.001109) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 250} 0.957034 (0.000965) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 250} 0.896926 (0.001422) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 250} 0.943373 (0.001304) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 250} 0.951410 (0.001134) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 250} 0.956411 (0.001098) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 250} 0.959704 (0.001012) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 250} 0.961554 (0.001031) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 250} 0.948739 (0.001135) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 250} 0.962983 (0.001039) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 250} 0.965813 (0.000917) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 250} 0.966327 (0.000819) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 250} 0.965787 (0.000926) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 250} 0.964972 (0.000899) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 250} 0.953481 (0.001111) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 250} 0.965296 (0.000997) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 250} 0.966389 (0.001085) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 250} 0.965036 (0.000773) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 250} 0.963644 (0.000851) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 250} 0.963652 (0.000819) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 250} 0.955416 (0.001211) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 250} 0.965968 (0.001096) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 250} 0.965435 (0.000856) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 250} 0.962857 (0.001111) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 250} 0.962074 (0.000712) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 250} 0.962557 (0.000907) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 250} CPU times: user 19 s, sys: 2.96 s, total: 21.9 s Wall time: 45min 54s . Best: 0.966389 using {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 250} . to our running best of : 0.966138 using {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 200} is slightly better . to our comparitor score for this range of 0.964941 (0.000612) with: {&#39;n_estimators&#39;: 450} is much better . to our individual parameter tuning best of 0.966902 (0.000735) with: {&#39;n_estimators&#39;: 1950} is not yet better . %time xgboost_tune_n_estimators_and_max_depth_and_lr(to.xs, to.ys.values.ravel(), n_estimators_range[5:6], max_depth_range, learning_rate_range, n_splits, sampling_methods[1], random_seed, scoring) . range(300, 350, 50) range(1, 12, 2) [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3] Fitting 10 folds for each of 36 candidates, totalling 360 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 17 tasks | elapsed: 1.2min [Parallel(n_jobs=-1)]: Done 90 tasks | elapsed: 10.1min [Parallel(n_jobs=-1)]: Done 213 tasks | elapsed: 26.5min [Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 54.5min finished . Best: 0.966422 using {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 300} 0.752885 (0.002099) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 300} 0.870359 (0.001705) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 300} 0.926303 (0.001948) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 300} 0.943667 (0.001428) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 300} 0.951787 (0.001082) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 300} 0.955087 (0.001107) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 300} 0.752885 (0.002099) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 300} 0.903583 (0.001910) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 300} 0.936403 (0.001723) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 300} 0.948150 (0.001116) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 300} 0.953947 (0.001114) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 300} 0.957204 (0.000949) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 300} 0.898274 (0.001634) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 300} 0.947948 (0.001315) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 300} 0.953170 (0.001111) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 300} 0.957402 (0.001064) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 300} 0.960462 (0.001027) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 300} 0.962134 (0.001052) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 300} 0.950505 (0.001150) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 300} 0.963692 (0.001049) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 300} 0.966168 (0.000944) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 300} 0.966366 (0.000865) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 300} 0.965605 (0.000961) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 300} 0.964851 (0.000900) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 300} 0.954250 (0.001165) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 300} 0.965670 (0.000978) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 300} 0.966422 (0.001029) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 300} 0.964775 (0.000825) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 300} 0.963536 (0.000807) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 300} 0.963716 (0.000844) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 300} 0.956206 (0.001207) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 300} 0.966172 (0.001072) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 300} 0.965214 (0.000907) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 300} 0.962569 (0.001178) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 300} 0.962200 (0.000804) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 300} 0.962703 (0.000914) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 300} CPU times: user 24.1 s, sys: 3.24 s, total: 27.3 s Wall time: 54min 39s . Best: 0.966422 using {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 300} . to our running best of : 0.966389 using {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 250} is slightly better . to our comparitor score for this range of 0.964941 (0.000612) with: {&#39;n_estimators&#39;: 450} is much better . to our individual parameter tuning best of 0.966902 (0.000735) with: {&#39;n_estimators&#39;: 1950} is not quite better . %time xgboost_tune_n_estimators_and_max_depth_and_lr(to.xs, to.ys.values.ravel(), n_estimators_range[6:7], max_depth_range, learning_rate_range, n_splits, sampling_methods[1], random_seed, scoring) . range(350, 400, 50) range(1, 12, 2) [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3] Fitting 10 folds for each of 36 candidates, totalling 360 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 17 tasks | elapsed: 1.3min /usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning [Parallel(n_jobs=-1)]: Done 90 tasks | elapsed: 11.7min [Parallel(n_jobs=-1)]: Done 213 tasks | elapsed: 30.7min [Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 63.3min finished . Best: 0.966364 using {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 350} 0.752885 (0.002099) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 350} 0.870359 (0.001705) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 350} 0.926368 (0.001834) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 350} 0.943933 (0.001292) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 350} 0.951854 (0.001094) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 350} 0.955131 (0.001136) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 350} 0.752885 (0.002099) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 350} 0.904082 (0.001852) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 350} 0.937111 (0.001732) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 350} 0.948413 (0.001086) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 350} 0.954211 (0.001119) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 350} 0.957343 (0.000912) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 350} 0.904170 (0.001252) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 350} 0.949571 (0.001256) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 350} 0.954626 (0.001114) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 350} 0.958239 (0.001046) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 350} 0.961222 (0.001022) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 350} 0.962727 (0.001037) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 350} 0.951543 (0.001098) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 350} 0.964213 (0.001006) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 350} 0.966355 (0.000945) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 350} 0.966364 (0.000883) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 350} 0.965457 (0.000911) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 350} 0.964757 (0.000847) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 350} 0.954959 (0.001214) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 350} 0.965975 (0.000986) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 350} 0.966346 (0.000999) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 350} 0.964533 (0.000798) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 350} 0.963490 (0.000848) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 350} 0.963839 (0.000856) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 350} 0.956761 (0.001212) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 350} 0.966328 (0.001051) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 350} 0.964919 (0.000865) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 350} 0.962367 (0.001163) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 350} 0.962323 (0.000792) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 350} 0.962775 (0.000912) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 350} CPU times: user 28.9 s, sys: 4.1 s, total: 33 s Wall time: 1h 3min 25s . Best: 0.966364 using {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 350} . to our running best of : 0.966422 using {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 300} is getting worse . to our comparitor score for this range of 0.964941 (0.000612) with: {&#39;n_estimators&#39;: 450} is much better . to our individual parameter tuning best of 0.966902 (0.000735) with: {&#39;n_estimators&#39;: 1950} is not quite better . %time xgboost_tune_n_estimators_and_max_depth_and_lr(to.xs, to.ys.values.ravel(), n_estimators_range[7:8], max_depth_range, learning_rate_range, n_splits, sampling_methods[1], random_seed, scoring) . range(400, 450, 50) range(1, 12, 2) [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3] Fitting 10 folds for each of 36 candidates, totalling 360 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 17 tasks | elapsed: 1.5min /usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning [Parallel(n_jobs=-1)]: Done 90 tasks | elapsed: 13.4min [Parallel(n_jobs=-1)]: Done 213 tasks | elapsed: 35.0min [Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 71.9min finished . Best: 0.966493 using {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 400} 0.752885 (0.002099) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 400} 0.870359 (0.001705) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 400} 0.926426 (0.001848) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 400} 0.944073 (0.001318) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 400} 0.951963 (0.001093) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 400} 0.955188 (0.001187) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 400} 0.752885 (0.002099) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 400} 0.904183 (0.001830) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 400} 0.937920 (0.001857) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 400} 0.948551 (0.001133) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 400} 0.954613 (0.001061) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 400} 0.957465 (0.000926) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 400} 0.904391 (0.001228) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 400} 0.950882 (0.001214) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 400} 0.955793 (0.001092) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 400} 0.959033 (0.001034) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 400} 0.961818 (0.001002) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 400} 0.963299 (0.001046) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 400} 0.952305 (0.001118) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 400} 0.964645 (0.001052) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 400} 0.966493 (0.000933) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 400} 0.966339 (0.000897) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 400} 0.965312 (0.000938) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 400} 0.964762 (0.000891) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 400} 0.955576 (0.001221) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 400} 0.966138 (0.000939) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 400} 0.966203 (0.001033) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 400} 0.964334 (0.000765) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 400} 0.963454 (0.000863) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 400} 0.963840 (0.000869) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 400} 0.957199 (0.001185) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 400} 0.966369 (0.000997) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 400} 0.964662 (0.000825) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 400} 0.962201 (0.001143) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 400} 0.962400 (0.000791) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 400} 0.962836 (0.000900) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 400} CPU times: user 28.3 s, sys: 4.08 s, total: 32.3 s Wall time: 1h 11min 59s . Best: 0.966493 using {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 400} . to our running best of : 0.966422 using {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 300} is a very slight improvement . to our comparitor score for this range of 0.964941 (0.000612) with: {&#39;n_estimators&#39;: 450} is better . to our individual parameter tuning best of 0.966902 (0.000735) with: {&#39;n_estimators&#39;: 1950} is not as good . As we can see the most optimal parameters using XGBoost would be to do n_estimators=300, max_depth=5, learning_rate=0.2. Using default values of XGBoost from it&#39;s source code, Let&#39;s run both and compare results side by side . %time xgboost_tune_n_estimators_and_max_depth_and_lr(to.xs, to.ys.values.ravel(), n_estimators_range[8:9], max_depth_range, learning_rate_range, n_splits, sampling_methods[1], random_seed, scoring) . range(450, 500, 50) range(1, 12, 2) [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3] Fitting 10 folds for each of 36 candidates, totalling 360 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. /usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning [Parallel(n_jobs=-1)]: Done 17 tasks | elapsed: 1.7min [Parallel(n_jobs=-1)]: Done 90 tasks | elapsed: 15.1min [Parallel(n_jobs=-1)]: Done 213 tasks | elapsed: 39.3min [Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 80.1min finished . Best: 0.966590 using {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 450} 0.752885 (0.002099) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 450} 0.870359 (0.001705) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 450} 0.926472 (0.001865) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 450} 0.944154 (0.001364) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 450} 0.952004 (0.001115) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 450} 0.955253 (0.001215) with: {&#39;learning_rate&#39;: 0.0001, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 450} 0.752885 (0.002099) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 450} 0.904500 (0.001786) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 450} 0.938420 (0.001760) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 450} 0.948791 (0.001086) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 450} 0.954779 (0.001036) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 450} 0.957578 (0.000942) with: {&#39;learning_rate&#39;: 0.001, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 450} 0.904378 (0.001223) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 450} 0.952087 (0.001192) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 450} 0.956820 (0.001051) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 450} 0.959711 (0.001031) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 450} 0.962359 (0.000997) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 450} 0.963765 (0.001024) with: {&#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 450} 0.952845 (0.001125) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 450} 0.964971 (0.001040) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 450} 0.966590 (0.000928) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 450} 0.966272 (0.000936) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 450} 0.965170 (0.000914) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 450} 0.964778 (0.000910) with: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 450} 0.956056 (0.001241) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 450} 0.966378 (0.000941) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 450} 0.966093 (0.000996) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 450} 0.964063 (0.000775) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 450} 0.963464 (0.000851) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 450} 0.963881 (0.000905) with: {&#39;learning_rate&#39;: 0.2, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 450} 0.957612 (0.001219) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 450} 0.966507 (0.000966) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 450} 0.964433 (0.000775) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 450} 0.962041 (0.001139) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 7, &#39;n_estimators&#39;: 450} 0.962471 (0.000799) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 9, &#39;n_estimators&#39;: 450} 0.962890 (0.000927) with: {&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 11, &#39;n_estimators&#39;: 450} CPU times: user 36.8 s, sys: 5.42 s, total: 42.2 s Wall time: 1h 20min 12s . Best: 0.966590 using {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 450} . to our running best of : 0.966493 using {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 400}is slightly better . to our comparitor score for this range of 0.964941 (0.000612) with: {&#39;n_estimators&#39;: 450} is better . to our individual parameter tuning best of 0.966902 (0.000735) with: {&#39;n_estimators&#39;: 1950} not quite as good . Ending here for now, as the run times were just getting too long, and we&#39;re really really close to comparative performance with the best single-parameter tuned result . Head to Head Comparisons: Default, vs Recommended vs Max parameter values . n_estimators_original = 100 max_depth_original = 6 learning_rate_original = 0.2 n_estimators_recommended = 450 max_depth_recommended = 5 learning_rate_recommended = 0.1 n_estimators_max = 1950 subsample = 1 enable_categorical=True . X_train, y_train = to.train.xs, to.train.ys.values.ravel() X_valid, y_valid = to.valid.xs, to.valid.ys.values.ravel() . model_original = xgb.XGBClassifier(n_estimators = n_estimators_original, max_depth=max_depth_original, learning_rate=learning_rate_original, subsample=subsample, tree_method=&#39;gpu_hist&#39;, gpu_id=0, verbosity=2, enable_categorical=enable_categorical, sampling_methods=sampling_methods[1]) %time xgb_model_original = model_original.fit(X_train, y_train) xgb_preds_original = xgb_model_original.predict_proba(X_valid) xgb_preds_original . CPU times: user 2.24 s, sys: 246 ms, total: 2.49 s Wall time: 2.47 s . array([[9.9646646e-01, 3.5335247e-03], [9.9989974e-01, 1.0023018e-04], [7.9425681e-01, 2.0574318e-01], ..., [9.8356736e-01, 1.6432654e-02], [4.6183008e-01, 5.3816992e-01], [9.6087682e-01, 3.9123163e-02]], dtype=float32) . model_recommended = xgb.XGBClassifier(n_estimators = n_estimators_recommended, max_depth=max_depth_recommended, learning_rate=learning_rate_recommended, subsample=subsample, tree_method=&#39;gpu_hist&#39;, gpu_id=0, verbosity=3, enable_categorical=enable_categorical, sampling_methods=sampling_methods[1]) %time xgb_model_recommended = model_recommended.fit(X_train, y_train) xgb_preds_recommended = xgb_model_recommended.predict_proba(X_valid) xgb_preds_recommended . CPU times: user 5.78 s, sys: 224 ms, total: 6.01 s Wall time: 5.96 s . array([[9.9579751e-01, 4.2025056e-03], [9.9995327e-01, 4.6750749e-05], [7.4739861e-01, 2.5260136e-01], ..., [9.8437619e-01, 1.5623793e-02], [5.1231825e-01, 4.8768175e-01], [9.5093983e-01, 4.9060162e-02]], dtype=float32) . model_max_e = xgb.XGBClassifier(n_estimators = n_estimators_max, max_depth=max_depth_recommended, learning_rate=learning_rate_recommended, subsample=subsample, tree_method=&#39;gpu_hist&#39;, gpu_id=0, verbosity=2, enable_categorical=enable_categorical, sampling_methods=sampling_methods[1]) %time xgb_model_max_e = model_max_e.fit(X_train, y_train) xgb_preds_max_e = xgb_model_max_e.predict_proba(X_valid) xgb_preds_max_e . CPU times: user 21.6 s, sys: 315 ms, total: 22 s Wall time: 21.8 s . array([[9.9950659e-01, 4.9340865e-04], [9.9998891e-01, 1.1102343e-05], [8.0084145e-01, 1.9915855e-01], ..., [9.9154103e-01, 8.4589710e-03], [5.5457193e-01, 4.4542807e-01], [9.7199428e-01, 2.8005721e-02]], dtype=float32) . accuracy(tensor(xgb_preds_original), tensor(y_valid)), accuracy(tensor(xgb_preds_recommended), tensor(y_valid)), accuracy(tensor(xgb_preds_max_e), tensor(y_valid)) . (TensorBase(0.9254), TensorBase(0.9271), TensorBase(0.9255)) . roc_auc_score(y_score=tensor(xgb_preds_original[:,1:2]), y_true=tensor(y_valid)), roc_auc_score(y_score=tensor(xgb_preds_recommended[:,1:2]), y_true=tensor(y_valid)), roc_auc_score(y_score=tensor(xgb_preds_max_e[:,1:2]), y_true=tensor(y_valid)) . (0.9651362820976437, 0.9657186586961237, 0.9643268132563287) . So as we can see, we get a slightly better performance both in accuracy and the roc_auc_score (used in Kaggle competition) when we use a lower n_estimators and tune together with max_depth and learning_rate parameters based on the recommendation, than either when we use just the defaults or the maximum value of n_estimators .",
            "url": "https://redditech.github.io/team-fast-tabulous/kaggle/fastai/2020/07/02/Finding-Optimum-XGBoost-Parameters.html",
            "relUrl": "/kaggle/fastai/2020/07/02/Finding-Optimum-XGBoost-Parameters.html",
            "date": " • Jul 2, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Us",
          "content": "Team Fan-tabulous was formed for the Queensland AI’s Fast.ai course project. Our focus is on tabular data using the Homesite Competition data from Kaggle. We’re made up of enthusiastic learners comprising . Tim Cummings | Tracy Dinh | Jorge | Nissan Dookeran | Owen Powell | Michael Senescall | . We each will be contributing blog posts and notebooks as our individual learning journeys converge in producing our solution using FastAI and Deep Learning techniques we learn in our course. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://redditech.github.io/team-fast-tabulous/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://redditech.github.io/team-fast-tabulous/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}