<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Finding Optimum XGBoost Parameters for Tabular For Homesite Competition | Team Fast-Tabulous!</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Finding Optimum XGBoost Parameters for Tabular For Homesite Competition" />
<meta name="author" content="Nissan Dookeran" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Building on the work of the previous notebook, which referenced Zach’s notebook to apply his techniques for permutation importance and ensemble learning to the Homesite Competition problem set, we will deep dive into optimizing the XGBoost parameters to see if an improved model can be generated, and what effect this has for our resulting ensemble model’s predictions" />
<meta property="og:description" content="Building on the work of the previous notebook, which referenced Zach’s notebook to apply his techniques for permutation importance and ensemble learning to the Homesite Competition problem set, we will deep dive into optimizing the XGBoost parameters to see if an improved model can be generated, and what effect this has for our resulting ensemble model’s predictions" />
<link rel="canonical" href="https://redditech.github.io/team-fast-tabulous/kaggle/fastai/2020/07/02/Finding-Optimum-XGBoost-Parameters.html" />
<meta property="og:url" content="https://redditech.github.io/team-fast-tabulous/kaggle/fastai/2020/07/02/Finding-Optimum-XGBoost-Parameters.html" />
<meta property="og:site_name" content="Team Fast-Tabulous!" />
<meta property="og:image" content="https://redditech.github.io/team-fast-tabulous/images/chart-preview.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-02T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://redditech.github.io/team-fast-tabulous/kaggle/fastai/2020/07/02/Finding-Optimum-XGBoost-Parameters.html","@type":"BlogPosting","headline":"Finding Optimum XGBoost Parameters for Tabular For Homesite Competition","dateModified":"2020-07-02T00:00:00-05:00","datePublished":"2020-07-02T00:00:00-05:00","image":"https://redditech.github.io/team-fast-tabulous/images/chart-preview.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://redditech.github.io/team-fast-tabulous/kaggle/fastai/2020/07/02/Finding-Optimum-XGBoost-Parameters.html"},"author":{"@type":"Person","name":"Nissan Dookeran"},"description":"Building on the work of the previous notebook, which referenced Zach’s notebook to apply his techniques for permutation importance and ensemble learning to the Homesite Competition problem set, we will deep dive into optimizing the XGBoost parameters to see if an improved model can be generated, and what effect this has for our resulting ensemble model’s predictions","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/team-fast-tabulous/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://redditech.github.io/team-fast-tabulous/feed.xml" title="Team Fast-Tabulous!" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-MHGYW30ZCF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-MHGYW30ZCF');
</script>


<link rel="shortcut icon" type="image/x-icon" href="/team-fast-tabulous/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/team-fast-tabulous/">Team Fast-Tabulous!</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/team-fast-tabulous/about/">About Us</a><a class="page-link" href="/team-fast-tabulous/search/">Search</a><a class="page-link" href="/team-fast-tabulous/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Finding Optimum XGBoost Parameters for Tabular For Homesite Competition</h1><p class="page-description">Building on the work of the previous notebook, which referenced <a href='https://github.com/muellerzr/Practical-Deep-Learning-for-Coders-2.0/blob/master/Tabular%20Notebooks/02_Ensembling.ipynb'>Zach's notebook</a> to apply his techniques for permutation importance and ensemble learning to the Homesite Competition problem set, we will deep dive into optimizing the XGBoost parameters to see if an improved model can be generated, and what effect this has for our resulting ensemble model's predictions</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-07-02T00:00:00-05:00" itemprop="datePublished">
        Jul 2, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Nissan Dookeran</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      42 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/team-fast-tabulous/categories/#kaggle">kaggle</a>
        &nbsp;
      
        <a class="category-tags-link" href="/team-fast-tabulous/categories/#fastai">fastai</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/redditech/team-fast-tabulous/tree/master/_notebooks/2020-07-02-Finding-Optimum-XGBoost-Parameters.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/team-fast-tabulous/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/redditech/team-fast-tabulous/master?filepath=_notebooks%2F2020-07-02-Finding-Optimum-XGBoost-Parameters.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/team-fast-tabulous/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/redditech/team-fast-tabulous/blob/master/_notebooks/2020-07-02-Finding-Optimum-XGBoost-Parameters.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/team-fast-tabulous/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Introduction">Introduction </a></li>
<li class="toc-entry toc-h2"><a href="#Setup">Setup </a></li>
<li class="toc-entry toc-h2"><a href="#The-GridsearchCV-functions">The GridsearchCV functions </a>
<ul>
<li class="toc-entry toc-h3"><a href="#My-useful-functions">My useful functions </a></li>
<li class="toc-entry toc-h3"><a href="#Load-the-data">Load the data </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Exploring-single-parameter-tuning-performance">Exploring single parameter tuning performance </a>
<ul>
<li class="toc-entry toc-h3"><a href="#n_estimators">n_estimators </a></li>
<li class="toc-entry toc-h3"><a href="#max_depth">max_depth </a></li>
<li class="toc-entry toc-h3"><a href="#learning_rate">learning_rate </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Hypothesis:-Multi-parameter-vs-Single-parameter-tuning">Hypothesis: Multi-parameter vs Single parameter tuning </a></li>
<li class="toc-entry toc-h2"><a href="#Head-to-Head-Comparisons:-Default,-vs-Recommended-vs-Max-parameter-values">Head to Head Comparisons: Default, vs Recommended vs Max parameter values </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-02-Finding-Optimum-XGBoost-Parameters.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction">
<a class="anchor" href="#Introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction<a class="anchor-link" href="#Introduction"> </a>
</h2>
<p>Using the <a href="https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/">code here</a> we will look at the optimal number of estimators, depth, and then combination of the two when using XGBoost to generate a model</p>
<p>Notes:</p>
<ul>
<li>Possible improvements if manually coding would be to add <code>subsamples</code> as a variable, however with both the increase of permutations and the long running times for GridSearch and a look to explore other options like Optuna and Bayesian Search to automate this, I limited it to just <code>max_depth</code> and <code>learning_rate</code> as the additional variables for now</li>
<li>Because of timeouts for long ranges, broke up the ranges for the various parameters into groupings, will only explore a subsequent grouping if there is an optimal value that is max in a previous grouping. This will also give a more manageable range for the permutations when trying to find the best combination of all parameters </li>
<li>Added to tune learning_rate based on <a href="https://machinelearningmastery.com/tune-learning-rate-for-gradient-boosting-with-xgboost-in-python/">this code</a>
</li>
<li>Removed the <code>triage</code> variable, <code>FillMissing</code> will handle NA values in categorical fields appropriately, but just need to filter it out from modifying categories function</li>
<li>Changed the categorize functions from <a href="https://redditech.github.io/team-fast-tabulous/kaggle/fastai/2021/06/27/Improving-Fastai-split-choices.html">last notebook</a> to exclude any columns in y_names from being evaluated since these shouldn't be part of the model training as a parameter</li>
<li>Added code to save all models to reuse in another notebook that submits to Kaggle for test evaluation</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Setup">
<a class="anchor" href="#Setup" aria-hidden="true"><span class="octicon octicon-link"></span></a>Setup<a class="anchor-link" href="#Setup"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<blockquote>
<p>Adding based on <a href="https://github.com/fastai/course-v3/blob/master/nbs/dl1/00_notebook_tutorial.ipynb">tutorial</a> for notebooks</p>
</blockquote>
</blockquote>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">reload_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2
<span class="o">!</span>pip install -Uqq fastai
<span class="o">!</span>pip install kaggle
<span class="kn">from</span> <span class="nn">fastai.tabular.all</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>     |████████████████████████████████| 194kB 14.1MB/s 
     |████████████████████████████████| 61kB 9.0MB/s 
Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)
Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)
Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)
Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)
Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)
Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)
Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)
Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)
Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3)
Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10)
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">global</span> <span class="n">gdrive</span> <span class="c1">#colab only code block</span>
<span class="n">gdrive</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s1">'/content/gdrive/My Drive'</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">gdrive</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span> <span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">gdrive</span><span class="o">.</span><span class="n">parent</span><span class="p">))</span>
<span class="o">!</span>mkdir -p ~/.kaggle
<span class="o">!</span>cp /content/gdrive/MyDrive/Kaggle/kaggle.json ~/.kaggle/
<span class="o">!</span>chmod <span class="m">600</span> ~/.kaggle/kaggle.json
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Mounted at /content/gdrive
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">kaggle</span> <span class="kn">import</span> <span class="n">api</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="o">.</span><span class="n">cwd</span><span class="p">()</span>
<span class="n">path</span><span class="o">.</span><span class="n">ls</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#3) [Path('/content/.config'),Path('/content/gdrive'),Path('/content/sample_data')]</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>Only run the next three lines the first time if in a local repository. This will prevent large training data files and model files being checked into Github</p>
</blockquote>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>touch .gitignore
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span><span class="nb">echo</span> <span class="s2">"_data"</span> &gt; .gitignore
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>mkdir _data
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="s1">'_data'</span><span class="p">)</span>
<span class="n">Path</span><span class="o">.</span><span class="n">cwd</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Path('/content/_data')</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>Back to it</p>
</blockquote>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s2">"gdrive/MyDrive/Kaggle/"</span><span class="p">)</span> <span class="c1"># colab only code</span>
<span class="n">Path</span><span class="o">.</span><span class="n">cwd</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Path('/content/gdrive/MyDrive/Kaggle')</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="o">.</span><span class="n">cwd</span><span class="p">()</span><span class="o">/</span><span class="s2">"homesite_competition_data"</span>
<span class="n">path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Path</span><span class="o">.</span><span class="n">BASE_PATH</span> <span class="o">=</span> <span class="n">path</span>
<span class="n">api</span><span class="o">.</span><span class="n">competition_download_cli</span><span class="p">(</span><span class="s1">'homesite-quote-conversion'</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="n">path</span><span class="p">)</span>
<span class="n">file_extract</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s2">"homesite-quote-conversion.zip"</span><span class="p">)</span>
<span class="n">file_extract</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s2">"train.csv.zip"</span><span class="p">)</span>
<span class="n">file_extract</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s2">"test.csv.zip"</span><span class="p">)</span>
<span class="n">path</span><span class="o">.</span><span class="n">ls</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>homesite-quote-conversion.zip: Skipping, found more recently modified local copy (use --force to force download)
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#6) [Path('homesite-quote-conversion.zip'),Path('sample_submission.csv.zip'),Path('test.csv.zip'),Path('train.csv.zip'),Path('train.csv'),Path('test.csv')]</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Settings</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">y_block</span><span class="o">=</span><span class="n">CategoryBlock</span><span class="p">()</span>
<span class="c1"># n_estimators = [50, 100, 150, 200]</span>
<span class="c1"># max_depth = [2, 4, 6, 8]</span>
<span class="n">n_estimators_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">50</span><span class="p">)</span>
<span class="n">n_estimators_range1</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span><span class="mi">1050</span><span class="p">,</span><span class="mi">50</span><span class="p">)</span>
<span class="n">n_estimators_range2</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1050</span><span class="p">,</span><span class="mi">1500</span><span class="p">,</span><span class="mi">50</span><span class="p">)</span>
<span class="n">n_estimators_range3</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1550</span><span class="p">,</span><span class="mi">2050</span><span class="p">,</span><span class="mi">50</span><span class="p">)</span>
<span class="n">max_depth_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">max_depth_range1</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span><span class="mi">22</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">max_depth_range2</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">23</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">learning_rate_range</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]</span>
<span class="n">subsample_range</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.3</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.7</span><span class="p">,</span><span class="mf">1.0</span><span class="p">]</span>
<span class="n">sampling_methods</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'uniform'</span><span class="p">,</span><span class="s1">'gradient_based'</span><span class="p">]</span>
<span class="n">random_seed</span> <span class="o">=</span><span class="mi">42</span>
<span class="n">n_splits</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># 10 folds</span>
<span class="n">scoring</span> <span class="o">=</span> <span class="s2">"roc_auc"</span>
<span class="n">category_threshold</span> <span class="o">=</span> <span class="mi">20</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>
<span class="c1"># valid_score = roc_auc_score(to_np(targs), to_np(preds[:,1]))</span>
<span class="c1"># valid_score</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-GridsearchCV-functions">
<a class="anchor" href="#The-GridsearchCV-functions" aria-hidden="true"><span class="octicon octicon-link"></span></a>The <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">GridsearchCV</a> functions<a class="anchor-link" href="#The-GridsearchCV-functions"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Customising functions from <a href="https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/">this page</a> and <a href="https://machinelearningmastery.com/tune-learning-rate-for-gradient-boosting-with-xgboost-in-python/">this page</a>to work with our dataset</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This function will tune for <code>n_estimators</code> only</p>
<p><strong>Notes</strong></p>
<ul>
<li>Used <a href="https://scikit-learn.org/stable/modules/model_evaluation.html#scoring">https://scikit-learn.org/stable/modules/model_evaluation.html#scoring</a> to find correct <code>scoring</code> string</li>
<li>Reading more on <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html">StratifiedKFold</a> to understand if I used the <code>n_splits</code> correctly, I chose 10 based on <a href="https://machinelearningmastery.com/k-fold-cross-validation/">this article</a>
</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">xgboost_tune_estimators</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n_estimators_range</span><span class="p">,</span> <span class="n">n_splits</span><span class="p">,</span> <span class="n">sampling_methods</span><span class="p">,</span> <span class="n">random_seed</span><span class="p">,</span> <span class="n">scoring</span><span class="p">):</span>
    <span class="n">matplotlib</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">'Agg'</span><span class="p">)</span>
    <span class="n">label_encoded_y</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="c1"># grid search</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span><span class="n">tree_method</span><span class="o">=</span><span class="s1">'gpu_hist'</span><span class="p">,</span> <span class="n">gpu_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">verbosity</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">sampling_methods</span><span class="o">=</span><span class="n">sampling_methods</span><span class="p">)</span>
    <span class="n">param_grid</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators_range</span><span class="p">)</span>
    <span class="n">kfold</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_seed</span><span class="p">)</span>
    <span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kfold</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">grid_result</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">label_encoded_y</span><span class="p">)</span>
    <span class="c1"># summarize results</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Best: </span><span class="si">%f</span><span class="s2"> using </span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">grid_result</span><span class="o">.</span><span class="n">best_score_</span><span class="p">,</span> <span class="n">grid_result</span><span class="o">.</span><span class="n">best_params_</span><span class="p">))</span>
    <span class="n">means</span> <span class="o">=</span> <span class="n">grid_result</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">'mean_test_score'</span><span class="p">]</span>
    <span class="n">stds</span> <span class="o">=</span> <span class="n">grid_result</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">'std_test_score'</span><span class="p">]</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">grid_result</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">'params'</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">mean</span><span class="p">,</span> <span class="n">stdev</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">stds</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="si">%f</span><span class="s2"> (</span><span class="si">%f</span><span class="s2">) with: </span><span class="si">%r</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">stdev</span><span class="p">,</span> <span class="n">param</span><span class="p">))</span>
    <span class="c1"># plot</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">n_estimators_range</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">stds</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"XGBoost n_estimators vs Log Loss"</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'n_estimators'</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Log Loss'</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">'n_estimators.png'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This function will tune for <code>max_depth</code> value only</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">xgboost_tune_max_depth</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">max_depth_range</span><span class="p">,</span> <span class="n">n_splits</span><span class="p">,</span> <span class="n">sampling_methods</span><span class="p">,</span> <span class="n">random_seed</span><span class="p">,</span> <span class="n">scoring</span><span class="p">):</span>
    <span class="n">matplotlib</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">'Agg'</span><span class="p">)</span>
    <span class="c1"># encode string class values as integers</span>
    <span class="n">label_encoded_y</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="c1"># grid search</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span><span class="n">tree_method</span><span class="o">=</span><span class="s1">'gpu_hist'</span><span class="p">,</span> <span class="n">gpu_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">verbosity</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">sampling_methods</span><span class="o">=</span><span class="n">sampling_methods</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">max_depth_range</span><span class="p">)</span>
    <span class="n">param_grid</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth_range</span><span class="p">)</span>
    <span class="n">kfold</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_seed</span><span class="p">)</span>
    <span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kfold</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">grid_result</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">label_encoded_y</span><span class="p">)</span>
    <span class="c1"># summarize results</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Best: </span><span class="si">%f</span><span class="s2"> using </span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">grid_result</span><span class="o">.</span><span class="n">best_score_</span><span class="p">,</span> <span class="n">grid_result</span><span class="o">.</span><span class="n">best_params_</span><span class="p">))</span>
    <span class="n">means</span> <span class="o">=</span> <span class="n">grid_result</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">'mean_test_score'</span><span class="p">]</span>
    <span class="n">stds</span> <span class="o">=</span> <span class="n">grid_result</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">'std_test_score'</span><span class="p">]</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">grid_result</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">'params'</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">mean</span><span class="p">,</span> <span class="n">stdev</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">stds</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="si">%f</span><span class="s2"> (</span><span class="si">%f</span><span class="s2">) with: </span><span class="si">%r</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">stdev</span><span class="p">,</span> <span class="n">param</span><span class="p">))</span>
    <span class="c1"># plot</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">max_depth_range</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">stds</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"XGBoost max_depth vs Log Loss"</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'max_depth'</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Log Loss'</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">'max_depth.png'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This function will tune for <code>learning_rate</code></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">xgboost_tune_lr</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lr_range</span><span class="p">,</span> <span class="n">n_splits</span><span class="p">,</span> <span class="n">sampling_methods</span><span class="p">,</span> <span class="n">random_seed</span><span class="p">,</span> <span class="n">scoring</span><span class="p">):</span>
    <span class="n">matplotlib</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">'Agg'</span><span class="p">)</span>
    <span class="n">label_encoded_y</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="c1"># grid search</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span><span class="n">tree_method</span><span class="o">=</span><span class="s1">'gpu_hist'</span><span class="p">,</span> <span class="n">gpu_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">verbosity</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">sampling_methods</span><span class="o">=</span><span class="n">sampling_methods</span><span class="p">)</span>
    <span class="n">param_grid</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">lr_range</span><span class="p">)</span>
    <span class="n">kfold</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_seed</span><span class="p">)</span>
    <span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kfold</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">grid_result</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">label_encoded_y</span><span class="p">)</span>
    <span class="c1"># summarize results</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Best: </span><span class="si">%f</span><span class="s2"> using </span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">grid_result</span><span class="o">.</span><span class="n">best_score_</span><span class="p">,</span> <span class="n">grid_result</span><span class="o">.</span><span class="n">best_params_</span><span class="p">))</span>
    <span class="n">means</span> <span class="o">=</span> <span class="n">grid_result</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">'mean_test_score'</span><span class="p">]</span>
    <span class="n">stds</span> <span class="o">=</span> <span class="n">grid_result</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">'std_test_score'</span><span class="p">]</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">grid_result</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">'params'</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">mean</span><span class="p">,</span> <span class="n">stdev</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">stds</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="si">%f</span><span class="s2"> (</span><span class="si">%f</span><span class="s2">) with: </span><span class="si">%r</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">stdev</span><span class="p">,</span> <span class="n">param</span><span class="p">))</span>
    <span class="c1"># plot</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">lr_range</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">stds</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"XGBoost learning_rate vs Log Loss"</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'learning_rate'</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Log Loss'</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">'learning_rate.png'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This function will tune for <code>n_estimators</code>, <code>max_depth</code>, <code>learning_rate</code> in combination (takes really long to run)</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">xgboost_tune_n_estimators_and_max_depth_and_lr</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n_estimators_range</span><span class="p">,</span> <span class="n">max_depth_range</span><span class="p">,</span> <span class="n">lr_range</span><span class="p">,</span> <span class="n">n_splits</span><span class="p">,</span> <span class="n">sampling_methods</span><span class="p">,</span> <span class="n">random_seed</span><span class="p">,</span> <span class="n">scoring</span><span class="p">):</span>
    <span class="n">matplotlib</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">'Agg'</span><span class="p">)</span>
    <span class="c1"># encode string class values as integers</span>
    <span class="n">label_encoded_y</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="c1"># grid search</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span><span class="n">tree_method</span><span class="o">=</span><span class="s1">'gpu_hist'</span><span class="p">,</span> <span class="n">gpu_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">verbosity</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">sampling_methods</span><span class="o">=</span><span class="n">sampling_methods</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">n_estimators_range</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">max_depth_range</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">lr_range</span><span class="p">)</span>
    <span class="n">param_grid</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth_range</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators_range</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr_range</span><span class="p">)</span>
    <span class="n">kfold</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_seed</span><span class="p">)</span>
    <span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kfold</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">grid_result</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">label_encoded_y</span><span class="p">)</span>
    <span class="c1"># summarize results</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Best: </span><span class="si">%f</span><span class="s2"> using </span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">grid_result</span><span class="o">.</span><span class="n">best_score_</span><span class="p">,</span> <span class="n">grid_result</span><span class="o">.</span><span class="n">best_params_</span><span class="p">))</span>
    <span class="n">means</span> <span class="o">=</span> <span class="n">grid_result</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">'mean_test_score'</span><span class="p">]</span>
    <span class="n">stds</span> <span class="o">=</span> <span class="n">grid_result</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">'std_test_score'</span><span class="p">]</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">grid_result</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">'params'</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">mean</span><span class="p">,</span> <span class="n">stdev</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">stds</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="si">%f</span><span class="s2"> (</span><span class="si">%f</span><span class="s2">) with: </span><span class="si">%r</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">stdev</span><span class="p">,</span> <span class="n">param</span><span class="p">))</span>
    <span class="c1"># plot results</span>
    <span class="c1"># scores = np.array(means).reshape(len(max_depth_range), len(n_estimators_range))</span>
    <span class="c1"># for i, value in enumerate(max_depth_range):</span>
    <span class="c1">#   pyplot.plot(n_estimators_range, scores[i], label='depth: ' + str(value))</span>
    <span class="c1"># pyplot.legend()</span>
    <span class="c1"># pyplot.xlabel('n_estimators')</span>
    <span class="c1"># pyplot.ylabel('Log Loss')</span>
    <span class="c1"># pyplot.savefig('n_estimators_vs_max_depth.png')</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="My-useful-functions">
<a class="anchor" href="#My-useful-functions" aria-hidden="true"><span class="octicon octicon-link"></span></a>My useful functions<a class="anchor-link" href="#My-useful-functions"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">reassign_to_categorical</span><span class="p">(</span><span class="n">field</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">y_names</span><span class="p">,</span> <span class="n">continuous</span><span class="p">,</span> <span class="n">categorical</span><span class="p">):</span>
  <span class="k">if</span> <span class="p">((</span><span class="n">df</span><span class="p">[</span><span class="n">field</span><span class="p">]</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">field</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">y_names</span><span class="p">)):</span>
    <span class="n">field_categories</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">field</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
    <span class="n">df</span><span class="p">[</span><span class="n">field</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">field</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'category'</span><span class="p">)</span>
    <span class="n">df</span><span class="p">[</span><span class="n">field</span><span class="p">]</span><span class="o">.</span><span class="n">cat</span><span class="o">.</span><span class="n">set_categories</span><span class="p">(</span><span class="n">field_categories</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">continuous</span><span class="p">:</span> <span class="n">continuous</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">field</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">field</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">categorical</span><span class="p">:</span> <span class="n">categorical</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">field</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">df</span><span class="p">,</span> <span class="n">continuous</span><span class="p">,</span> <span class="n">categorical</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">categorize</span><span class="p">(</span> <span class="n">df</span><span class="p">,</span> <span class="n">y_names</span><span class="p">,</span> <span class="n">cont_names</span><span class="p">,</span> <span class="n">cat_names</span><span class="p">,</span> <span class="n">category_threshold</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="k">if</span> <span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">field</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span> <span class="o">&lt;=</span> <span class="n">category_threshold</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">field</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">!=</span> <span class="n">pd</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">dtypes</span><span class="o">.</span><span class="n">dtypes</span><span class="o">.</span><span class="n">CategoricalDtype</span><span class="p">)):</span>
      <span class="n">reassign_to_categorical</span><span class="p">(</span><span class="n">field</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">y_names</span><span class="p">,</span> <span class="n">cont_names</span><span class="p">,</span> <span class="n">cat_names</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">df</span><span class="p">,</span> <span class="n">cont_names</span><span class="p">,</span> <span class="n">cat_names</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">homesite_prep</span><span class="p">(</span><span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span><span class="p">,</span> <span class="n">y_names</span><span class="p">,</span> <span class="n">category_threshold</span><span class="p">):</span>
    <span class="n">df_train</span><span class="o">.</span><span class="n">QuoteConversion_Flag</span> <span class="o">=</span> <span class="n">df_train</span><span class="o">.</span><span class="n">QuoteConversion_Flag</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="s1">'boolean'</span><span class="p">)</span>
    <span class="n">df_train</span> <span class="o">=</span> <span class="n">df_train</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">'QuoteNumber'</span><span class="p">)</span>
    <span class="n">df_test</span> <span class="o">=</span> <span class="n">df_test</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">'QuoteNumber'</span><span class="p">)</span>
    <span class="n">df_train</span><span class="p">[</span><span class="s1">'Original_Quote_Date'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="s1">'Original_Quote_Date'</span><span class="p">])</span>
    <span class="n">df_test</span><span class="p">[</span><span class="s1">'Original_Quote_Date'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="s1">'Original_Quote_Date'</span><span class="p">])</span>
    <span class="n">df_train</span> <span class="o">=</span> <span class="n">add_datepart</span><span class="p">(</span><span class="n">df_train</span><span class="p">,</span> <span class="s1">'Original_Quote_Date'</span><span class="p">)</span>
    <span class="n">df_test</span> <span class="o">=</span> <span class="n">add_datepart</span><span class="p">(</span><span class="n">df_test</span><span class="p">,</span> <span class="s1">'Original_Quote_Date'</span><span class="p">)</span>
    <span class="n">cont_names</span><span class="p">,</span> <span class="n">cat_names</span> <span class="o">=</span> <span class="n">cont_cat_split</span><span class="p">(</span><span class="n">df_train</span><span class="p">,</span> <span class="n">dep_var</span><span class="o">=</span><span class="n">y_names</span><span class="p">)</span>
    <span class="n">df_train</span><span class="p">,</span> <span class="n">cont_names</span><span class="p">,</span> <span class="n">cat_names</span> <span class="o">=</span> <span class="n">categorize</span><span class="p">(</span><span class="n">df_train</span><span class="p">,</span> <span class="n">y_names</span><span class="p">,</span> <span class="n">cont_names</span><span class="p">,</span> <span class="n">cat_names</span><span class="p">,</span> <span class="n">category_threshold</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span><span class="p">,</span> <span class="n">cont_names</span><span class="p">,</span> <span class="n">cat_names</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">find_y_columns</span><span class="p">(</span><span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span><span class="p">):</span>
    <span class="n">y_columns</span> <span class="o">=</span> <span class="n">df_train</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">difference</span><span class="p">(</span><span class="n">df_test</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y_columns</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Load-the-data">
<a class="anchor" href="#Load-the-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Load the data<a class="anchor-link" href="#Load-the-data"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s2">"train.csv"</span><span class="p">,</span> <span class="n">low_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">df_train</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>QuoteNumber</th>
      <th>Original_Quote_Date</th>
      <th>QuoteConversion_Flag</th>
      <th>Field6</th>
      <th>Field7</th>
      <th>Field8</th>
      <th>Field9</th>
      <th>Field10</th>
      <th>Field11</th>
      <th>Field12</th>
      <th>CoverageField1A</th>
      <th>CoverageField1B</th>
      <th>CoverageField2A</th>
      <th>CoverageField2B</th>
      <th>CoverageField3A</th>
      <th>CoverageField3B</th>
      <th>CoverageField4A</th>
      <th>CoverageField4B</th>
      <th>CoverageField5A</th>
      <th>CoverageField5B</th>
      <th>CoverageField6A</th>
      <th>CoverageField6B</th>
      <th>CoverageField8</th>
      <th>CoverageField9</th>
      <th>CoverageField11A</th>
      <th>CoverageField11B</th>
      <th>SalesField1A</th>
      <th>SalesField1B</th>
      <th>SalesField2A</th>
      <th>SalesField2B</th>
      <th>SalesField3</th>
      <th>SalesField4</th>
      <th>SalesField5</th>
      <th>SalesField6</th>
      <th>SalesField7</th>
      <th>SalesField8</th>
      <th>SalesField9</th>
      <th>SalesField10</th>
      <th>SalesField11</th>
      <th>SalesField12</th>
      <th>...</th>
      <th>GeographicField44A</th>
      <th>GeographicField44B</th>
      <th>GeographicField45A</th>
      <th>GeographicField45B</th>
      <th>GeographicField46A</th>
      <th>GeographicField46B</th>
      <th>GeographicField47A</th>
      <th>GeographicField47B</th>
      <th>GeographicField48A</th>
      <th>GeographicField48B</th>
      <th>GeographicField49A</th>
      <th>GeographicField49B</th>
      <th>GeographicField50A</th>
      <th>GeographicField50B</th>
      <th>GeographicField51A</th>
      <th>GeographicField51B</th>
      <th>GeographicField52A</th>
      <th>GeographicField52B</th>
      <th>GeographicField53A</th>
      <th>GeographicField53B</th>
      <th>GeographicField54A</th>
      <th>GeographicField54B</th>
      <th>GeographicField55A</th>
      <th>GeographicField55B</th>
      <th>GeographicField56A</th>
      <th>GeographicField56B</th>
      <th>GeographicField57A</th>
      <th>GeographicField57B</th>
      <th>GeographicField58A</th>
      <th>GeographicField58B</th>
      <th>GeographicField59A</th>
      <th>GeographicField59B</th>
      <th>GeographicField60A</th>
      <th>GeographicField60B</th>
      <th>GeographicField61A</th>
      <th>GeographicField61B</th>
      <th>GeographicField62A</th>
      <th>GeographicField62B</th>
      <th>GeographicField63</th>
      <th>GeographicField64</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2013-08-16</td>
      <td>0</td>
      <td>B</td>
      <td>23</td>
      <td>0.9403</td>
      <td>0.0006</td>
      <td>965</td>
      <td>1.0200</td>
      <td>N</td>
      <td>17</td>
      <td>23</td>
      <td>17</td>
      <td>23</td>
      <td>15</td>
      <td>22</td>
      <td>16</td>
      <td>22</td>
      <td>13</td>
      <td>22</td>
      <td>13</td>
      <td>23</td>
      <td>T</td>
      <td>D</td>
      <td>2</td>
      <td>1</td>
      <td>7</td>
      <td>18</td>
      <td>3</td>
      <td>8</td>
      <td>0</td>
      <td>5</td>
      <td>5</td>
      <td>24</td>
      <td>V</td>
      <td>48649</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>8</td>
      <td>4</td>
      <td>20</td>
      <td>22</td>
      <td>10</td>
      <td>8</td>
      <td>6</td>
      <td>5</td>
      <td>15</td>
      <td>13</td>
      <td>19</td>
      <td>18</td>
      <td>16</td>
      <td>14</td>
      <td>21</td>
      <td>23</td>
      <td>21</td>
      <td>23</td>
      <td>16</td>
      <td>11</td>
      <td>22</td>
      <td>24</td>
      <td>7</td>
      <td>14</td>
      <td>-1</td>
      <td>17</td>
      <td>15</td>
      <td>17</td>
      <td>14</td>
      <td>18</td>
      <td>9</td>
      <td>9</td>
      <td>-1</td>
      <td>8</td>
      <td>-1</td>
      <td>18</td>
      <td>-1</td>
      <td>10</td>
      <td>N</td>
      <td>CA</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>2014-04-22</td>
      <td>0</td>
      <td>F</td>
      <td>7</td>
      <td>1.0006</td>
      <td>0.0040</td>
      <td>548</td>
      <td>1.2433</td>
      <td>N</td>
      <td>6</td>
      <td>8</td>
      <td>6</td>
      <td>8</td>
      <td>5</td>
      <td>7</td>
      <td>5</td>
      <td>8</td>
      <td>13</td>
      <td>22</td>
      <td>13</td>
      <td>23</td>
      <td>T</td>
      <td>E</td>
      <td>5</td>
      <td>9</td>
      <td>5</td>
      <td>14</td>
      <td>6</td>
      <td>18</td>
      <td>1</td>
      <td>5</td>
      <td>5</td>
      <td>11</td>
      <td>P</td>
      <td>26778</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>...</td>
      <td>23</td>
      <td>24</td>
      <td>11</td>
      <td>15</td>
      <td>21</td>
      <td>24</td>
      <td>6</td>
      <td>11</td>
      <td>21</td>
      <td>21</td>
      <td>18</td>
      <td>15</td>
      <td>20</td>
      <td>20</td>
      <td>13</td>
      <td>12</td>
      <td>12</td>
      <td>12</td>
      <td>15</td>
      <td>9</td>
      <td>13</td>
      <td>11</td>
      <td>11</td>
      <td>20</td>
      <td>-1</td>
      <td>9</td>
      <td>18</td>
      <td>21</td>
      <td>8</td>
      <td>7</td>
      <td>10</td>
      <td>10</td>
      <td>-1</td>
      <td>11</td>
      <td>-1</td>
      <td>17</td>
      <td>-1</td>
      <td>20</td>
      <td>N</td>
      <td>NJ</td>
    </tr>
  </tbody>
</table>
<p>2 rows × 299 columns</p>
</div>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s2">"test.csv"</span><span class="p">,</span> <span class="n">low_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">df_test</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>QuoteNumber</th>
      <th>Original_Quote_Date</th>
      <th>Field6</th>
      <th>Field7</th>
      <th>Field8</th>
      <th>Field9</th>
      <th>Field10</th>
      <th>Field11</th>
      <th>Field12</th>
      <th>CoverageField1A</th>
      <th>CoverageField1B</th>
      <th>CoverageField2A</th>
      <th>CoverageField2B</th>
      <th>CoverageField3A</th>
      <th>CoverageField3B</th>
      <th>CoverageField4A</th>
      <th>CoverageField4B</th>
      <th>CoverageField5A</th>
      <th>CoverageField5B</th>
      <th>CoverageField6A</th>
      <th>CoverageField6B</th>
      <th>CoverageField8</th>
      <th>CoverageField9</th>
      <th>CoverageField11A</th>
      <th>CoverageField11B</th>
      <th>SalesField1A</th>
      <th>SalesField1B</th>
      <th>SalesField2A</th>
      <th>SalesField2B</th>
      <th>SalesField3</th>
      <th>SalesField4</th>
      <th>SalesField5</th>
      <th>SalesField6</th>
      <th>SalesField7</th>
      <th>SalesField8</th>
      <th>SalesField9</th>
      <th>SalesField10</th>
      <th>SalesField11</th>
      <th>SalesField12</th>
      <th>SalesField13</th>
      <th>...</th>
      <th>GeographicField44A</th>
      <th>GeographicField44B</th>
      <th>GeographicField45A</th>
      <th>GeographicField45B</th>
      <th>GeographicField46A</th>
      <th>GeographicField46B</th>
      <th>GeographicField47A</th>
      <th>GeographicField47B</th>
      <th>GeographicField48A</th>
      <th>GeographicField48B</th>
      <th>GeographicField49A</th>
      <th>GeographicField49B</th>
      <th>GeographicField50A</th>
      <th>GeographicField50B</th>
      <th>GeographicField51A</th>
      <th>GeographicField51B</th>
      <th>GeographicField52A</th>
      <th>GeographicField52B</th>
      <th>GeographicField53A</th>
      <th>GeographicField53B</th>
      <th>GeographicField54A</th>
      <th>GeographicField54B</th>
      <th>GeographicField55A</th>
      <th>GeographicField55B</th>
      <th>GeographicField56A</th>
      <th>GeographicField56B</th>
      <th>GeographicField57A</th>
      <th>GeographicField57B</th>
      <th>GeographicField58A</th>
      <th>GeographicField58B</th>
      <th>GeographicField59A</th>
      <th>GeographicField59B</th>
      <th>GeographicField60A</th>
      <th>GeographicField60B</th>
      <th>GeographicField61A</th>
      <th>GeographicField61B</th>
      <th>GeographicField62A</th>
      <th>GeographicField62B</th>
      <th>GeographicField63</th>
      <th>GeographicField64</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>2014-08-12</td>
      <td>E</td>
      <td>16</td>
      <td>0.9364</td>
      <td>0.0006</td>
      <td>1,487</td>
      <td>1.3045</td>
      <td>N</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>4</td>
      <td>13</td>
      <td>22</td>
      <td>13</td>
      <td>23</td>
      <td>Y</td>
      <td>K</td>
      <td>13</td>
      <td>22</td>
      <td>6</td>
      <td>16</td>
      <td>9</td>
      <td>21</td>
      <td>0</td>
      <td>5</td>
      <td>5</td>
      <td>11</td>
      <td>P</td>
      <td>67052</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>22</td>
      <td>23</td>
      <td>9</td>
      <td>12</td>
      <td>25</td>
      <td>25</td>
      <td>6</td>
      <td>9</td>
      <td>4</td>
      <td>2</td>
      <td>16</td>
      <td>12</td>
      <td>20</td>
      <td>20</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>10</td>
      <td>7</td>
      <td>25</td>
      <td>25</td>
      <td>-1</td>
      <td>19</td>
      <td>19</td>
      <td>22</td>
      <td>12</td>
      <td>15</td>
      <td>1</td>
      <td>1</td>
      <td>-1</td>
      <td>1</td>
      <td>-1</td>
      <td>20</td>
      <td>-1</td>
      <td>25</td>
      <td>Y</td>
      <td>IL</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5</td>
      <td>2013-09-07</td>
      <td>F</td>
      <td>11</td>
      <td>0.9919</td>
      <td>0.0038</td>
      <td>564</td>
      <td>1.1886</td>
      <td>N</td>
      <td>8</td>
      <td>14</td>
      <td>8</td>
      <td>14</td>
      <td>7</td>
      <td>12</td>
      <td>8</td>
      <td>13</td>
      <td>13</td>
      <td>22</td>
      <td>13</td>
      <td>23</td>
      <td>T</td>
      <td>E</td>
      <td>4</td>
      <td>5</td>
      <td>3</td>
      <td>6</td>
      <td>3</td>
      <td>6</td>
      <td>1</td>
      <td>5</td>
      <td>5</td>
      <td>4</td>
      <td>R</td>
      <td>27288</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>23</td>
      <td>24</td>
      <td>12</td>
      <td>21</td>
      <td>23</td>
      <td>25</td>
      <td>7</td>
      <td>11</td>
      <td>16</td>
      <td>14</td>
      <td>13</td>
      <td>6</td>
      <td>17</td>
      <td>15</td>
      <td>7</td>
      <td>5</td>
      <td>7</td>
      <td>5</td>
      <td>13</td>
      <td>7</td>
      <td>14</td>
      <td>14</td>
      <td>7</td>
      <td>14</td>
      <td>-1</td>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>5</td>
      <td>3</td>
      <td>10</td>
      <td>10</td>
      <td>-1</td>
      <td>5</td>
      <td>-1</td>
      <td>5</td>
      <td>-1</td>
      <td>21</td>
      <td>N</td>
      <td>NJ</td>
    </tr>
  </tbody>
</table>
<p>2 rows × 298 columns</p>
</div>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_names</span> <span class="o">=</span> <span class="n">find_y_columns</span><span class="p">(</span><span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span><span class="p">,</span> <span class="n">cont_names</span><span class="p">,</span> <span class="n">cat_names</span> <span class="o">=</span> <span class="n">homesite_prep</span><span class="p">(</span><span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span><span class="p">,</span> <span class="n">y_names</span><span class="p">,</span> <span class="n">category_threshold</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">procs</span> <span class="o">=</span> <span class="p">[</span><span class="n">Categorify</span><span class="p">,</span> <span class="n">FillMissing</span><span class="p">,</span> <span class="n">Normalize</span><span class="p">]</span>
<span class="n">splits</span> <span class="o">=</span> <span class="n">TrainTestSplitter</span><span class="p">(</span><span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">y_names</span><span class="p">])(</span><span class="n">df_train</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">to</span> <span class="o">=</span> <span class="n">TabularPandas</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">df_train</span><span class="p">,</span> <span class="n">procs</span><span class="o">=</span><span class="n">procs</span><span class="p">,</span> <span class="n">cat_names</span><span class="o">=</span><span class="n">cat_names</span><span class="p">,</span> 
                   <span class="n">cont_names</span><span class="o">=</span><span class="n">cont_names</span><span class="p">,</span> <span class="n">y_names</span><span class="o">=</span><span class="n">y_names</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="n">splits</span><span class="p">,</span>
                  <span class="n">y_block</span><span class="o">=</span><span class="n">y_block</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sampling_methods</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">sampling_methods</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>('uniform', 'gradient_based')</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Exploring-single-parameter-tuning-performance">
<a class="anchor" href="#Exploring-single-parameter-tuning-performance" aria-hidden="true"><span class="octicon octicon-link"></span></a>Exploring single parameter tuning performance<a class="anchor-link" href="#Exploring-single-parameter-tuning-performance"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="n_estimators">
<a class="anchor" href="#n_estimators" aria-hidden="true"><span class="octicon octicon-link"></span></a>n_estimators<a class="anchor-link" href="#n_estimators"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">time</span> xgboost_tune_estimators(to.xs, to.ys.values.ravel(), n_estimators_range, n_splits, sampling_methods[1], random_seed, scoring)
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Fitting 10 folds for each of 9 candidates, totalling 90 fits
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   30.4s
/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  "timeout or by a memory leak.", UserWarning
[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  5.1min finished
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Best: 0.964941 using {'n_estimators': 450}
0.953037 (0.000664) with: {'n_estimators': 50}
0.957954 (0.000668) with: {'n_estimators': 100}
0.960359 (0.000591) with: {'n_estimators': 150}
0.961898 (0.000540) with: {'n_estimators': 200}
0.962885 (0.000550) with: {'n_estimators': 250}
0.963609 (0.000563) with: {'n_estimators': 300}
0.964180 (0.000594) with: {'n_estimators': 350}
0.964613 (0.000588) with: {'n_estimators': 400}
0.964941 (0.000612) with: {'n_estimators': 450}
CPU times: user 9.27 s, sys: 1.42 s, total: 10.7 s
Wall time: 5min 15s
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">time</span> xgboost_tune_estimators(to.xs, to.ys.values.ravel(), n_estimators_range1, n_splits, sampling_methods[1], random_seed, scoring)
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Fitting 10 folds for each of 11 candidates, totalling 110 fits
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.
[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  3.8min
[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed: 21.1min
[Parallel(n_jobs=-1)]: Done 110 out of 110 | elapsed: 25.7min finished
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Best: 0.966545 using {'n_estimators': 1000}
0.965281 (0.000917) with: {'n_estimators': 500}
0.965500 (0.000936) with: {'n_estimators': 550}
0.965691 (0.000945) with: {'n_estimators': 600}
0.965869 (0.000955) with: {'n_estimators': 650}
0.966008 (0.000901) with: {'n_estimators': 700}
0.966127 (0.000916) with: {'n_estimators': 750}
0.966237 (0.000894) with: {'n_estimators': 800}
0.966322 (0.000882) with: {'n_estimators': 850}
0.966377 (0.000877) with: {'n_estimators': 900}
0.966441 (0.000879) with: {'n_estimators': 950}
0.966545 (0.000882) with: {'n_estimators': 1000}
CPU times: user 26.1 s, sys: 1.63 s, total: 27.8 s
Wall time: 25min 59s
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">time</span> xgboost_tune_estimators(to.xs, to.ys.values.ravel(), n_estimators_range2, n_splits, sampling_methods[1], random_seed, scoring)
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Fitting 10 folds for each of 9 candidates, totalling 90 fits
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  4.4min
[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed: 23.2min finished
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Best: 0.966838 using {'n_estimators': 1400}
0.966547 (0.000732) with: {'n_estimators': 1050}
0.966608 (0.000750) with: {'n_estimators': 1100}
0.966677 (0.000760) with: {'n_estimators': 1150}
0.966705 (0.000746) with: {'n_estimators': 1200}
0.966746 (0.000737) with: {'n_estimators': 1250}
0.966796 (0.000750) with: {'n_estimators': 1300}
0.966814 (0.000762) with: {'n_estimators': 1350}
0.966838 (0.000758) with: {'n_estimators': 1400}
0.966838 (0.000732) with: {'n_estimators': 1450}
CPU times: user 25.6 s, sys: 2.16 s, total: 27.8 s
Wall time: 23min 31s
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">time</span> xgboost_tune_estimators(to.xs, to.ys.values.ravel(), n_estimators_range3, n_splits, sampling_methods[1], random_seed, scoring)
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Fitting 10 folds for each of 10 candidates, totalling 100 fits
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  6.2min
/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  "timeout or by a memory leak.", UserWarning
[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 32.6min
[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 36.4min finished
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Best: 0.966902 using {'n_estimators': 1950}
0.966850 (0.000720) with: {'n_estimators': 1550}
0.966867 (0.000728) with: {'n_estimators': 1600}
0.966880 (0.000725) with: {'n_estimators': 1650}
0.966870 (0.000727) with: {'n_estimators': 1700}
0.966897 (0.000728) with: {'n_estimators': 1750}
0.966884 (0.000728) with: {'n_estimators': 1800}
0.966889 (0.000721) with: {'n_estimators': 1850}
0.966898 (0.000732) with: {'n_estimators': 1900}
0.966902 (0.000735) with: {'n_estimators': 1950}
0.966889 (0.000737) with: {'n_estimators': 2000}
CPU times: user 33.2 s, sys: 2.44 s, total: 35.6 s
Wall time: 36min 46s
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>From the batches we got from <code>0.964941 (0.000612) with: {'n_estimators': 450}</code> to <code>0.966902 (0.000735) with: {'n_estimators': 1950}</code> just tuning <code>n_estimators</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="max_depth">
<a class="anchor" href="#max_depth" aria-hidden="true"><span class="octicon octicon-link"></span></a>max_depth<a class="anchor-link" href="#max_depth"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">time</span> xgboost_tune_max_depth(to.xs,to.ys.values.ravel(), max_depth_range, n_splits, sampling_methods[1], random_seed, scoring)
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>range(1, 12, 2)
Fitting 10 folds for each of 6 candidates, totalling 60 fits
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   31.8s
/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  "timeout or by a memory leak.", UserWarning
[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  3.2min finished
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Best: 0.965565 using {'max_depth': 11}
0.940100 (0.001024) with: {'max_depth': 1}
0.957954 (0.000668) with: {'max_depth': 3}
0.962320 (0.000596) with: {'max_depth': 5}
0.964306 (0.000669) with: {'max_depth': 7}
0.965231 (0.000838) with: {'max_depth': 9}
0.965565 (0.000880) with: {'max_depth': 11}
CPU times: user 6.8 s, sys: 1.42 s, total: 8.22 s
Wall time: 3min 18s
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">time</span> xgboost_tune_max_depth(to.xs,to.ys.values.ravel(), max_depth_range1, n_splits, sampling_methods[1], random_seed, scoring)
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>range(13, 22, 2)
Fitting 10 folds for each of 5 candidates, totalling 50 fits
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  "timeout or by a memory leak.", UserWarning
[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  4.8min
[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 26.6min finished
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Best: 0.965149 using {'max_depth': 13}
0.965149 (0.000994) with: {'max_depth': 13}
0.964474 (0.000903) with: {'max_depth': 15}
0.964005 (0.000966) with: {'max_depth': 17}
0.963855 (0.000978) with: {'max_depth': 19}
0.963873 (0.000889) with: {'max_depth': 21}
CPU times: user 14.4 s, sys: 1.82 s, total: 16.3 s
Wall time: 26min 44s
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With our two range checks, it seems like our best results come with <code>0.965565 (0.000880) with: {'max_depth': 11}</code> and seems to get worse after that, since <code>0.965149 (0.000994) with: {'max_depth': 13}</code> in the 2nd range checked is less than the best in the initial range chechked, and progressively gets worse. So we should do our final fine tuning of permutations of variables only with the first range for <code>max_depth</code> and skip tuning with the final range of max_depth</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="learning_rate">
<a class="anchor" href="#learning_rate" aria-hidden="true"><span class="octicon octicon-link"></span></a>learning_rate<a class="anchor-link" href="#learning_rate"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">time</span> xgboost_tune_lr(to.xs,to.ys.values.ravel(), learning_rate_range, n_splits, sampling_methods[1], random_seed, scoring)
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Fitting 10 folds for each of 6 candidates, totalling 60 fits
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   41.0s
/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  "timeout or by a memory leak.", UserWarning
[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  1.8min finished
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Best: 0.963524 using {'learning_rate': 0.3}
0.870361 (0.002025) with: {'learning_rate': 0.0001}
0.872141 (0.003595) with: {'learning_rate': 0.001}
0.906749 (0.001726) with: {'learning_rate': 0.01}
0.957872 (0.001386) with: {'learning_rate': 0.1}
0.962143 (0.001049) with: {'learning_rate': 0.2}
0.963524 (0.001038) with: {'learning_rate': 0.3}
CPU times: user 4.03 s, sys: 1.58 s, total: 5.61 s
Wall time: 1min 54s
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The best learning rate, if just tuning that alone is <code>Best: 0.963524 using {'learning_rate': 0.3}</code></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_estimators_range</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_estimators_range</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">4</span><span class="p">],</span> <span class="n">n_estimators_range</span><span class="p">[</span><span class="mi">4</span><span class="p">:</span><span class="mi">8</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(range(50, 100, 50), range(100, 250, 50), range(250, 450, 50))</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_estimators_agg_range</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_estimators_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_estimators_range3</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">n_estimators_agg_range</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(50, 2000, 50)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Hypothesis:-Multi-parameter-vs-Single-parameter-tuning">
<a class="anchor" href="#Hypothesis:-Multi-parameter-vs-Single-parameter-tuning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hypothesis: Multi-parameter vs Single parameter tuning<a class="anchor-link" href="#Hypothesis:-Multi-parameter-vs-Single-parameter-tuning"> </a>
</h2>
<blockquote>
<p>Test if tuning can lead to lower values and as good as or better metrics than single-parameter tuning alone</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Hypothesis: We don't need as large a number of <code>n_estimators</code> to get at least as good as a validation metric as when we had a large value for <code>n_estimtors</code>
Need to split up the ranges more to not timeout the run in Kaggle. Will stop 1 range after we get a validation metric equal to the <code>n_estimators=1950</code> value</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">time</span> xgboost_tune_n_estimators_and_max_depth_and_lr(to.train.xs, to.train.ys.values.ravel(), n_estimators_range[0:1], max_depth_range, learning_rate_range, n_splits, sampling_methods[1], random_seed, scoring)
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>range(50, 100, 50)
range(1, 12, 2)
[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]
Fitting 10 folds for each of 36 candidates, totalling 360 fits
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  "timeout or by a memory leak.", UserWarning
[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   22.5s
[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  2.3min
[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:  5.6min
[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 10.0min finished
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Best: 0.964010 using {'learning_rate': 0.2, 'max_depth': 9, 'n_estimators': 50}
0.751979 (0.003046) with: {'learning_rate': 0.0001, 'max_depth': 1, 'n_estimators': 50}
0.869145 (0.002477) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 50}
0.922202 (0.006186) with: {'learning_rate': 0.0001, 'max_depth': 5, 'n_estimators': 50}
0.942591 (0.001473) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 50}
0.950671 (0.001595) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 50}
0.953800 (0.001254) with: {'learning_rate': 0.0001, 'max_depth': 11, 'n_estimators': 50}
0.751979 (0.003046) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 50}
0.869156 (0.002472) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 50}
0.923856 (0.005189) with: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 50}
0.943740 (0.001201) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 50}
0.951880 (0.001238) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 50}
0.954835 (0.001105) with: {'learning_rate': 0.001, 'max_depth': 11, 'n_estimators': 50}
0.751979 (0.003046) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 50}
0.901847 (0.002123) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}
0.937845 (0.001550) with: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50}
0.948545 (0.001346) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50}
0.954102 (0.001232) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 50}
0.957146 (0.001086) with: {'learning_rate': 0.01, 'max_depth': 11, 'n_estimators': 50}
0.910937 (0.002325) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 50}
0.952633 (0.001445) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}
0.957359 (0.001118) with: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}
0.960241 (0.000898) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50}
0.962544 (0.000995) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 50}
0.963453 (0.000948) with: {'learning_rate': 0.1, 'max_depth': 11, 'n_estimators': 50}
0.938624 (0.001576) with: {'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 50}
0.957635 (0.001107) with: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 50}
0.961949 (0.000842) with: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 50}
0.963619 (0.000812) with: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 50}
0.964010 (0.000800) with: {'learning_rate': 0.2, 'max_depth': 9, 'n_estimators': 50}
0.963526 (0.001098) with: {'learning_rate': 0.2, 'max_depth': 11, 'n_estimators': 50}
0.944601 (0.001402) with: {'learning_rate': 0.3, 'max_depth': 1, 'n_estimators': 50}
0.960327 (0.001075) with: {'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 50}
0.963439 (0.001099) with: {'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 50}
0.963698 (0.001077) with: {'learning_rate': 0.3, 'max_depth': 7, 'n_estimators': 50}
0.962492 (0.000850) with: {'learning_rate': 0.3, 'max_depth': 9, 'n_estimators': 50}
0.961501 (0.001166) with: {'learning_rate': 0.3, 'max_depth': 11, 'n_estimators': 50}
CPU times: user 10 s, sys: 8.4 s, total: 18.4 s
Wall time: 10min 5s
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>Best: 0.964010 using {'learning_rate': 0.2, 'max_depth': 9, 'n_estimators': 50}</code> to our comparitor score for this range of <code>0.964941 (0.000612) with: {'n_estimators': 450}</code> is already pretty close, but still isn't close enough to <code>0.966902 (0.000735) with: {'n_estimators': 1950}</code></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">time</span> xgboost_tune_n_estimators_and_max_depth_and_lr(to.xs, to.ys.values.ravel(), n_estimators_range[1:2], max_depth_range, learning_rate_range, n_splits, sampling_methods[1], random_seed, scoring)
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>range(100, 150, 50)
range(1, 12, 2)
[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]
Fitting 10 folds for each of 36 candidates, totalling 360 fits
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   37.0s
/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  "timeout or by a memory leak.", UserWarning
[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  4.4min
[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 10.9min
[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 21.0min finished
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Best: 0.965491 using {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 100}
0.752885 (0.002114) with: {'learning_rate': 0.0001, 'max_depth': 1, 'n_estimators': 100}
0.870359 (0.001553) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 100}
0.926093 (0.002292) with: {'learning_rate': 0.0001, 'max_depth': 5, 'n_estimators': 100}
0.942819 (0.001769) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 100}
0.951621 (0.001413) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 100}
0.954988 (0.001182) with: {'learning_rate': 0.0001, 'max_depth': 11, 'n_estimators': 100}
0.752885 (0.002114) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 100}
0.873178 (0.003716) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100}
0.926634 (0.002313) with: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 100}
0.945112 (0.001885) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 100}
0.952877 (0.001527) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 100}
0.956281 (0.001381) with: {'learning_rate': 0.001, 'max_depth': 11, 'n_estimators': 100}
0.832564 (0.001216) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 100}
0.907136 (0.002798) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}
0.942553 (0.001768) with: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100}
0.951659 (0.001568) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100}
0.956508 (0.001315) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 100}
0.959096 (0.001204) with: {'learning_rate': 0.01, 'max_depth': 11, 'n_estimators': 100}
0.940177 (0.001355) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 100}
0.957930 (0.001262) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}
0.962430 (0.001106) with: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}
0.964368 (0.001159) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100}
0.965283 (0.001170) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100}
0.965390 (0.001145) with: {'learning_rate': 0.1, 'max_depth': 11, 'n_estimators': 100}
0.947208 (0.001327) with: {'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 100}
0.962071 (0.001203) with: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 100}
0.965159 (0.001172) with: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 100}
0.965491 (0.001213) with: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 100}
0.964716 (0.001151) with: {'learning_rate': 0.2, 'max_depth': 9, 'n_estimators': 100}
0.963535 (0.001113) with: {'learning_rate': 0.2, 'max_depth': 11, 'n_estimators': 100}
0.951080 (0.001339) with: {'learning_rate': 0.3, 'max_depth': 1, 'n_estimators': 100}
0.963735 (0.001179) with: {'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 100}
0.965322 (0.001224) with: {'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 100}
0.964548 (0.001434) with: {'learning_rate': 0.3, 'max_depth': 7, 'n_estimators': 100}
0.962934 (0.001425) with: {'learning_rate': 0.3, 'max_depth': 9, 'n_estimators': 100}
0.961770 (0.001188) with: {'learning_rate': 0.3, 'max_depth': 11, 'n_estimators': 100}
CPU times: user 14.9 s, sys: 3.2 s, total: 18.1 s
Wall time: 21min 3s
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Best: <code>0.965491 using {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 100}</code></p>
<p>to our running best of : <code>Best: 0.964010 using {'learning_rate': 0.2, 'max_depth': 9, 'n_estimators': 50}</code> is a good step up</p>
<p>to our comparitor score for this range of <code>0.964941 (0.000612) with: {'n_estimators': 450}</code> is already better</p>
<p>to our individual parameter tuning best of <code>0.966902 (0.000735) with: {'n_estimators': 1950}</code> is not as good yet</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">time</span> xgboost_tune_n_estimators_and_max_depth_and_lr(to.xs, to.ys.values.ravel(), n_estimators_range[2:3], max_depth_range, learning_rate_range, n_splits, sampling_methods[1], random_seed, scoring)
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>range(150, 200, 50)
range(1, 12, 2)
[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]
Fitting 10 folds for each of 36 candidates, totalling 360 fits
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   47.6s
[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  6.2min
[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 15.7min
[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 30.9min finished
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Best: 0.965929 using {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 150}
0.752885 (0.002114) with: {'learning_rate': 0.0001, 'max_depth': 1, 'n_estimators': 150}
0.870359 (0.001553) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 150}
0.926181 (0.002251) with: {'learning_rate': 0.0001, 'max_depth': 5, 'n_estimators': 150}
0.943161 (0.002037) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 150}
0.951675 (0.001447) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 150}
0.955047 (0.001206) with: {'learning_rate': 0.0001, 'max_depth': 11, 'n_estimators': 150}
0.752885 (0.002114) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 150}
0.876144 (0.001286) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 150}
0.929654 (0.002302) with: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 150}
0.946549 (0.001667) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 150}
0.953203 (0.001327) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 150}
0.956743 (0.001438) with: {'learning_rate': 0.001, 'max_depth': 11, 'n_estimators': 150}
0.832564 (0.001216) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 150}
0.921982 (0.002077) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 150}
0.944877 (0.002080) with: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 150}
0.953576 (0.001472) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 150}
0.957661 (0.001325) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 150}
0.959992 (0.001179) with: {'learning_rate': 0.01, 'max_depth': 11, 'n_estimators': 150}
0.944823 (0.001435) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 150}
0.960446 (0.001184) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}
0.964292 (0.001155) with: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 150}
0.965639 (0.001191) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 150}
0.965751 (0.001192) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 150}
0.965469 (0.001118) with: {'learning_rate': 0.1, 'max_depth': 11, 'n_estimators': 150}
0.950747 (0.001370) with: {'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 150}
0.963729 (0.001213) with: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}
0.965929 (0.001217) with: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 150}
0.965486 (0.001286) with: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 150}
0.964271 (0.001047) with: {'learning_rate': 0.2, 'max_depth': 9, 'n_estimators': 150}
0.963260 (0.001186) with: {'learning_rate': 0.2, 'max_depth': 11, 'n_estimators': 150}
0.953139 (0.001382) with: {'learning_rate': 0.3, 'max_depth': 1, 'n_estimators': 150}
0.964877 (0.001168) with: {'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 150}
0.965604 (0.001328) with: {'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 150}
0.963946 (0.001444) with: {'learning_rate': 0.3, 'max_depth': 7, 'n_estimators': 150}
0.962267 (0.001424) with: {'learning_rate': 0.3, 'max_depth': 9, 'n_estimators': 150}
0.962203 (0.001134) with: {'learning_rate': 0.3, 'max_depth': 11, 'n_estimators': 150}
CPU times: user 15.5 s, sys: 2.32 s, total: 17.8 s
Wall time: 31min
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Best: <code>0.965929 using {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 150}</code></p>
<p>to our running best of : <code>0.965491 using {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 100}</code> is slightly better</p>
<p>to our comparitor score for this range of <code>0.964941 (0.000612) with: {'n_estimators': 450}</code> is better</p>
<p>to our individual parameter tuning best of <code>0.966902 (0.000735) with: {'n_estimators': 1950}</code> is not as good yet</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">time</span> xgboost_tune_n_estimators_and_max_depth_and_lr(to.xs, to.ys.values.ravel(), n_estimators_range[3:4], max_depth_range, learning_rate_range, n_splits, sampling_methods[1], random_seed, scoring)
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>range(200, 250, 50)
range(1, 12, 2)
[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]
Fitting 10 folds for each of 36 candidates, totalling 360 fits
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.0min
[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  8.1min
[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 20.9min
[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 41.2min finished
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Best: 0.966138 using {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 200}
0.752885 (0.002114) with: {'learning_rate': 0.0001, 'max_depth': 1, 'n_estimators': 200}
0.870359 (0.001553) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 200}
0.926250 (0.002253) with: {'learning_rate': 0.0001, 'max_depth': 5, 'n_estimators': 200}
0.943344 (0.002040) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 200}
0.951746 (0.001453) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 200}
0.955191 (0.001309) with: {'learning_rate': 0.0001, 'max_depth': 11, 'n_estimators': 200}
0.752885 (0.002114) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 200}
0.896939 (0.001870) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 200}
0.933709 (0.003101) with: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 200}
0.947491 (0.001635) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 200}
0.953470 (0.001314) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 200}
0.957012 (0.001366) with: {'learning_rate': 0.001, 'max_depth': 11, 'n_estimators': 200}
0.862201 (0.001466) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 200}
0.933365 (0.001476) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200}
0.948352 (0.001533) with: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200}
0.955065 (0.001480) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200}
0.958781 (0.001261) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 200}
0.960895 (0.001197) with: {'learning_rate': 0.01, 'max_depth': 11, 'n_estimators': 200}
0.946916 (0.001392) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 200}
0.961904 (0.001176) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}
0.965339 (0.001210) with: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}
0.966082 (0.001181) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}
0.965795 (0.001199) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 200}
0.965349 (0.001140) with: {'learning_rate': 0.1, 'max_depth': 11, 'n_estimators': 200}
0.952529 (0.001363) with: {'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 200}
0.964721 (0.001225) with: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 200}
0.966138 (0.001321) with: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 200}
0.965302 (0.001219) with: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200}
0.963786 (0.001006) with: {'learning_rate': 0.2, 'max_depth': 9, 'n_estimators': 200}
0.963365 (0.001264) with: {'learning_rate': 0.2, 'max_depth': 11, 'n_estimators': 200}
0.954374 (0.001307) with: {'learning_rate': 0.3, 'max_depth': 1, 'n_estimators': 200}
0.965573 (0.001305) with: {'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 200}
0.965499 (0.001258) with: {'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 200}
0.963438 (0.001267) with: {'learning_rate': 0.3, 'max_depth': 7, 'n_estimators': 200}
0.962110 (0.001268) with: {'learning_rate': 0.3, 'max_depth': 9, 'n_estimators': 200}
0.962387 (0.001110) with: {'learning_rate': 0.3, 'max_depth': 11, 'n_estimators': 200}
CPU times: user 20.9 s, sys: 2.99 s, total: 23.8 s
Wall time: 41min 17s
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Best: <code>0.966138 using {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 200}</code></p>
<p>to our running best of : <code>0.965929 using {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 150}</code> is better</p>
<p>to our comparitor score for this range of <code>0.964941 (0.000612) with: {'n_estimators': 450}</code> is better</p>
<p>to our individual parameter tuning best of <code>0.966902 (0.000735) with: {'n_estimators': 1950}</code> is just slightly worse off</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">time</span> xgboost_tune_n_estimators_and_max_depth_and_lr(to.xs, to.ys.values.ravel(), n_estimators_range[4:5], max_depth_range, learning_rate_range, n_splits, sampling_methods[1], random_seed, scoring)
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>range(250, 300, 50)
range(1, 12, 2)
[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]
Fitting 10 folds for each of 36 candidates, totalling 360 fits
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.0min
/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  "timeout or by a memory leak.", UserWarning
[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  8.5min
[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 22.2min
[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 45.8min finished
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Best: 0.966389 using {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 250}
0.752885 (0.002099) with: {'learning_rate': 0.0001, 'max_depth': 1, 'n_estimators': 250}
0.870360 (0.001705) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 250}
0.926215 (0.001915) with: {'learning_rate': 0.0001, 'max_depth': 5, 'n_estimators': 250}
0.943307 (0.001374) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 250}
0.951765 (0.001050) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 250}
0.954981 (0.001106) with: {'learning_rate': 0.0001, 'max_depth': 11, 'n_estimators': 250}
0.752885 (0.002099) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 250}
0.901628 (0.002865) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 250}
0.935802 (0.001789) with: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 250}
0.947850 (0.001155) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 250}
0.953761 (0.001109) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 250}
0.957034 (0.000965) with: {'learning_rate': 0.001, 'max_depth': 11, 'n_estimators': 250}
0.896926 (0.001422) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 250}
0.943373 (0.001304) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 250}
0.951410 (0.001134) with: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 250}
0.956411 (0.001098) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 250}
0.959704 (0.001012) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 250}
0.961554 (0.001031) with: {'learning_rate': 0.01, 'max_depth': 11, 'n_estimators': 250}
0.948739 (0.001135) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 250}
0.962983 (0.001039) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 250}
0.965813 (0.000917) with: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250}
0.966327 (0.000819) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250}
0.965787 (0.000926) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250}
0.964972 (0.000899) with: {'learning_rate': 0.1, 'max_depth': 11, 'n_estimators': 250}
0.953481 (0.001111) with: {'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 250}
0.965296 (0.000997) with: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 250}
0.966389 (0.001085) with: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 250}
0.965036 (0.000773) with: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 250}
0.963644 (0.000851) with: {'learning_rate': 0.2, 'max_depth': 9, 'n_estimators': 250}
0.963652 (0.000819) with: {'learning_rate': 0.2, 'max_depth': 11, 'n_estimators': 250}
0.955416 (0.001211) with: {'learning_rate': 0.3, 'max_depth': 1, 'n_estimators': 250}
0.965968 (0.001096) with: {'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 250}
0.965435 (0.000856) with: {'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 250}
0.962857 (0.001111) with: {'learning_rate': 0.3, 'max_depth': 7, 'n_estimators': 250}
0.962074 (0.000712) with: {'learning_rate': 0.3, 'max_depth': 9, 'n_estimators': 250}
0.962557 (0.000907) with: {'learning_rate': 0.3, 'max_depth': 11, 'n_estimators': 250}
CPU times: user 19 s, sys: 2.96 s, total: 21.9 s
Wall time: 45min 54s
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Best: <code>0.966389 using {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 250}</code></p>
<p>to our running best of : <code>0.966138 using {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 200}</code> is slightly better</p>
<p>to our comparitor score for this range of <code>0.964941 (0.000612) with: {'n_estimators': 450}</code> is much better</p>
<p>to our individual parameter tuning best of <code>0.966902 (0.000735) with: {'n_estimators': 1950}</code> is not yet better</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">time</span> xgboost_tune_n_estimators_and_max_depth_and_lr(to.xs, to.ys.values.ravel(), n_estimators_range[5:6], max_depth_range, learning_rate_range, n_splits, sampling_methods[1], random_seed, scoring)
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>range(300, 350, 50)
range(1, 12, 2)
[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]
Fitting 10 folds for each of 36 candidates, totalling 360 fits
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.2min
[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 10.1min
[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 26.5min
[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 54.5min finished
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Best: 0.966422 using {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 300}
0.752885 (0.002099) with: {'learning_rate': 0.0001, 'max_depth': 1, 'n_estimators': 300}
0.870359 (0.001705) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 300}
0.926303 (0.001948) with: {'learning_rate': 0.0001, 'max_depth': 5, 'n_estimators': 300}
0.943667 (0.001428) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 300}
0.951787 (0.001082) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 300}
0.955087 (0.001107) with: {'learning_rate': 0.0001, 'max_depth': 11, 'n_estimators': 300}
0.752885 (0.002099) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 300}
0.903583 (0.001910) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 300}
0.936403 (0.001723) with: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 300}
0.948150 (0.001116) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 300}
0.953947 (0.001114) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 300}
0.957204 (0.000949) with: {'learning_rate': 0.001, 'max_depth': 11, 'n_estimators': 300}
0.898274 (0.001634) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 300}
0.947948 (0.001315) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 300}
0.953170 (0.001111) with: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 300}
0.957402 (0.001064) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 300}
0.960462 (0.001027) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 300}
0.962134 (0.001052) with: {'learning_rate': 0.01, 'max_depth': 11, 'n_estimators': 300}
0.950505 (0.001150) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 300}
0.963692 (0.001049) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 300}
0.966168 (0.000944) with: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300}
0.966366 (0.000865) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 300}
0.965605 (0.000961) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 300}
0.964851 (0.000900) with: {'learning_rate': 0.1, 'max_depth': 11, 'n_estimators': 300}
0.954250 (0.001165) with: {'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 300}
0.965670 (0.000978) with: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 300}
0.966422 (0.001029) with: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 300}
0.964775 (0.000825) with: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 300}
0.963536 (0.000807) with: {'learning_rate': 0.2, 'max_depth': 9, 'n_estimators': 300}
0.963716 (0.000844) with: {'learning_rate': 0.2, 'max_depth': 11, 'n_estimators': 300}
0.956206 (0.001207) with: {'learning_rate': 0.3, 'max_depth': 1, 'n_estimators': 300}
0.966172 (0.001072) with: {'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 300}
0.965214 (0.000907) with: {'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 300}
0.962569 (0.001178) with: {'learning_rate': 0.3, 'max_depth': 7, 'n_estimators': 300}
0.962200 (0.000804) with: {'learning_rate': 0.3, 'max_depth': 9, 'n_estimators': 300}
0.962703 (0.000914) with: {'learning_rate': 0.3, 'max_depth': 11, 'n_estimators': 300}
CPU times: user 24.1 s, sys: 3.24 s, total: 27.3 s
Wall time: 54min 39s
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Best: <code>0.966422 using {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 300}</code></p>
<p>to our running best of : <code>0.966389 using {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 250}</code> is slightly better</p>
<p>to our comparitor score for this range of <code>0.964941 (0.000612) with: {'n_estimators': 450}</code> is much better</p>
<p>to our individual parameter tuning best of <code>0.966902 (0.000735) with: {'n_estimators': 1950}</code> is not quite better</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">time</span> xgboost_tune_n_estimators_and_max_depth_and_lr(to.xs, to.ys.values.ravel(), n_estimators_range[6:7], max_depth_range, learning_rate_range, n_splits, sampling_methods[1], random_seed, scoring)
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>range(350, 400, 50)
range(1, 12, 2)
[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]
Fitting 10 folds for each of 36 candidates, totalling 360 fits
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.3min
/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  "timeout or by a memory leak.", UserWarning
[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 11.7min
[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 30.7min
[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 63.3min finished
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Best: 0.966364 using {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 350}
0.752885 (0.002099) with: {'learning_rate': 0.0001, 'max_depth': 1, 'n_estimators': 350}
0.870359 (0.001705) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 350}
0.926368 (0.001834) with: {'learning_rate': 0.0001, 'max_depth': 5, 'n_estimators': 350}
0.943933 (0.001292) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 350}
0.951854 (0.001094) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 350}
0.955131 (0.001136) with: {'learning_rate': 0.0001, 'max_depth': 11, 'n_estimators': 350}
0.752885 (0.002099) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 350}
0.904082 (0.001852) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 350}
0.937111 (0.001732) with: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 350}
0.948413 (0.001086) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 350}
0.954211 (0.001119) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 350}
0.957343 (0.000912) with: {'learning_rate': 0.001, 'max_depth': 11, 'n_estimators': 350}
0.904170 (0.001252) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 350}
0.949571 (0.001256) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 350}
0.954626 (0.001114) with: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 350}
0.958239 (0.001046) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 350}
0.961222 (0.001022) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 350}
0.962727 (0.001037) with: {'learning_rate': 0.01, 'max_depth': 11, 'n_estimators': 350}
0.951543 (0.001098) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 350}
0.964213 (0.001006) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 350}
0.966355 (0.000945) with: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 350}
0.966364 (0.000883) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 350}
0.965457 (0.000911) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 350}
0.964757 (0.000847) with: {'learning_rate': 0.1, 'max_depth': 11, 'n_estimators': 350}
0.954959 (0.001214) with: {'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 350}
0.965975 (0.000986) with: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 350}
0.966346 (0.000999) with: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 350}
0.964533 (0.000798) with: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 350}
0.963490 (0.000848) with: {'learning_rate': 0.2, 'max_depth': 9, 'n_estimators': 350}
0.963839 (0.000856) with: {'learning_rate': 0.2, 'max_depth': 11, 'n_estimators': 350}
0.956761 (0.001212) with: {'learning_rate': 0.3, 'max_depth': 1, 'n_estimators': 350}
0.966328 (0.001051) with: {'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 350}
0.964919 (0.000865) with: {'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 350}
0.962367 (0.001163) with: {'learning_rate': 0.3, 'max_depth': 7, 'n_estimators': 350}
0.962323 (0.000792) with: {'learning_rate': 0.3, 'max_depth': 9, 'n_estimators': 350}
0.962775 (0.000912) with: {'learning_rate': 0.3, 'max_depth': 11, 'n_estimators': 350}
CPU times: user 28.9 s, sys: 4.1 s, total: 33 s
Wall time: 1h 3min 25s
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Best: <code>0.966364 using {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 350}</code></p>
<p>to our running best of : <code>0.966422 using {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 300}</code> is getting worse</p>
<p>to our comparitor score for this range of <code>0.964941 (0.000612) with: {'n_estimators': 450}</code> is much better</p>
<p>to our individual parameter tuning best of <code>0.966902 (0.000735) with: {'n_estimators': 1950}</code> is not quite better</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">time</span> xgboost_tune_n_estimators_and_max_depth_and_lr(to.xs, to.ys.values.ravel(), n_estimators_range[7:8], max_depth_range, learning_rate_range, n_splits, sampling_methods[1], random_seed, scoring)
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>range(400, 450, 50)
range(1, 12, 2)
[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]
Fitting 10 folds for each of 36 candidates, totalling 360 fits
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.5min
/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  "timeout or by a memory leak.", UserWarning
[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 13.4min
[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 35.0min
[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 71.9min finished
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Best: 0.966493 using {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400}
0.752885 (0.002099) with: {'learning_rate': 0.0001, 'max_depth': 1, 'n_estimators': 400}
0.870359 (0.001705) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 400}
0.926426 (0.001848) with: {'learning_rate': 0.0001, 'max_depth': 5, 'n_estimators': 400}
0.944073 (0.001318) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 400}
0.951963 (0.001093) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 400}
0.955188 (0.001187) with: {'learning_rate': 0.0001, 'max_depth': 11, 'n_estimators': 400}
0.752885 (0.002099) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 400}
0.904183 (0.001830) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 400}
0.937920 (0.001857) with: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 400}
0.948551 (0.001133) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 400}
0.954613 (0.001061) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 400}
0.957465 (0.000926) with: {'learning_rate': 0.001, 'max_depth': 11, 'n_estimators': 400}
0.904391 (0.001228) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 400}
0.950882 (0.001214) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 400}
0.955793 (0.001092) with: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 400}
0.959033 (0.001034) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 400}
0.961818 (0.001002) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 400}
0.963299 (0.001046) with: {'learning_rate': 0.01, 'max_depth': 11, 'n_estimators': 400}
0.952305 (0.001118) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 400}
0.964645 (0.001052) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 400}
0.966493 (0.000933) with: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400}
0.966339 (0.000897) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 400}
0.965312 (0.000938) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 400}
0.964762 (0.000891) with: {'learning_rate': 0.1, 'max_depth': 11, 'n_estimators': 400}
0.955576 (0.001221) with: {'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 400}
0.966138 (0.000939) with: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 400}
0.966203 (0.001033) with: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 400}
0.964334 (0.000765) with: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 400}
0.963454 (0.000863) with: {'learning_rate': 0.2, 'max_depth': 9, 'n_estimators': 400}
0.963840 (0.000869) with: {'learning_rate': 0.2, 'max_depth': 11, 'n_estimators': 400}
0.957199 (0.001185) with: {'learning_rate': 0.3, 'max_depth': 1, 'n_estimators': 400}
0.966369 (0.000997) with: {'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 400}
0.964662 (0.000825) with: {'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 400}
0.962201 (0.001143) with: {'learning_rate': 0.3, 'max_depth': 7, 'n_estimators': 400}
0.962400 (0.000791) with: {'learning_rate': 0.3, 'max_depth': 9, 'n_estimators': 400}
0.962836 (0.000900) with: {'learning_rate': 0.3, 'max_depth': 11, 'n_estimators': 400}
CPU times: user 28.3 s, sys: 4.08 s, total: 32.3 s
Wall time: 1h 11min 59s
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Best: <code>0.966493 using {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400}</code></p>
<p>to our running best of : <code>0.966422 using {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 300}</code> is a very slight improvement</p>
<p>to our comparitor score for this range of <code>0.964941 (0.000612) with: {'n_estimators': 450}</code> is better</p>
<p>to our individual parameter tuning best of <code>0.966902 (0.000735) with: {'n_estimators': 1950}</code> is not as good</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As we can see the most optimal parameters using XGBoost would be to do <code>n_estimators=300</code>, <code>max_depth=5</code>, <code>learning_rate=0.2</code>. Using default values of XGBoost from it's <a href="https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py#L362">source code</a>, Let's run both and compare results side by side</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">time</span> xgboost_tune_n_estimators_and_max_depth_and_lr(to.xs, to.ys.values.ravel(), n_estimators_range[8:9], max_depth_range, learning_rate_range, n_splits, sampling_methods[1], random_seed, scoring)
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>range(450, 500, 50)
range(1, 12, 2)
[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]
Fitting 10 folds for each of 36 candidates, totalling 360 fits
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  "timeout or by a memory leak.", UserWarning
[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.7min
[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 15.1min
[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 39.3min
[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 80.1min finished
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Best: 0.966590 using {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 450}
0.752885 (0.002099) with: {'learning_rate': 0.0001, 'max_depth': 1, 'n_estimators': 450}
0.870359 (0.001705) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 450}
0.926472 (0.001865) with: {'learning_rate': 0.0001, 'max_depth': 5, 'n_estimators': 450}
0.944154 (0.001364) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 450}
0.952004 (0.001115) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 450}
0.955253 (0.001215) with: {'learning_rate': 0.0001, 'max_depth': 11, 'n_estimators': 450}
0.752885 (0.002099) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 450}
0.904500 (0.001786) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 450}
0.938420 (0.001760) with: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 450}
0.948791 (0.001086) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 450}
0.954779 (0.001036) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 450}
0.957578 (0.000942) with: {'learning_rate': 0.001, 'max_depth': 11, 'n_estimators': 450}
0.904378 (0.001223) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 450}
0.952087 (0.001192) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 450}
0.956820 (0.001051) with: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 450}
0.959711 (0.001031) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 450}
0.962359 (0.000997) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 450}
0.963765 (0.001024) with: {'learning_rate': 0.01, 'max_depth': 11, 'n_estimators': 450}
0.952845 (0.001125) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 450}
0.964971 (0.001040) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 450}
0.966590 (0.000928) with: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 450}
0.966272 (0.000936) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 450}
0.965170 (0.000914) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 450}
0.964778 (0.000910) with: {'learning_rate': 0.1, 'max_depth': 11, 'n_estimators': 450}
0.956056 (0.001241) with: {'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 450}
0.966378 (0.000941) with: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 450}
0.966093 (0.000996) with: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 450}
0.964063 (0.000775) with: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 450}
0.963464 (0.000851) with: {'learning_rate': 0.2, 'max_depth': 9, 'n_estimators': 450}
0.963881 (0.000905) with: {'learning_rate': 0.2, 'max_depth': 11, 'n_estimators': 450}
0.957612 (0.001219) with: {'learning_rate': 0.3, 'max_depth': 1, 'n_estimators': 450}
0.966507 (0.000966) with: {'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 450}
0.964433 (0.000775) with: {'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 450}
0.962041 (0.001139) with: {'learning_rate': 0.3, 'max_depth': 7, 'n_estimators': 450}
0.962471 (0.000799) with: {'learning_rate': 0.3, 'max_depth': 9, 'n_estimators': 450}
0.962890 (0.000927) with: {'learning_rate': 0.3, 'max_depth': 11, 'n_estimators': 450}
CPU times: user 36.8 s, sys: 5.42 s, total: 42.2 s
Wall time: 1h 20min 12s
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Best: <code>0.966590 using {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 450}</code></p>
<p>to our running best of : <code>0.966493 using {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400}</code>is slightly better</p>
<p>to our comparitor score for this range of <code>0.964941 (0.000612) with: {'n_estimators': 450}</code> is better</p>
<p>to our individual parameter tuning best of <code>0.966902 (0.000735) with: {'n_estimators': 1950}</code> not quite as good</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Ending here for now, as the run times were just getting too long, and we're really really close to comparative performance with the best single-parameter tuned result</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Head-to-Head-Comparisons:-Default,-vs-Recommended-vs-Max-parameter-values">
<a class="anchor" href="#Head-to-Head-Comparisons:-Default,-vs-Recommended-vs-Max-parameter-values" aria-hidden="true"><span class="octicon octicon-link"></span></a>Head to Head Comparisons: Default, vs Recommended vs Max parameter values<a class="anchor-link" href="#Head-to-Head-Comparisons:-Default,-vs-Recommended-vs-Max-parameter-values"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_estimators_original</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">max_depth_original</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">learning_rate_original</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="n">n_estimators_recommended</span> <span class="o">=</span> <span class="mi">450</span>
<span class="n">max_depth_recommended</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">learning_rate_recommended</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">n_estimators_max</span> <span class="o">=</span> <span class="mi">1950</span>
<span class="n">subsample</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">enable_categorical</span><span class="o">=</span><span class="kc">True</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">to</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">xs</span><span class="p">,</span> <span class="n">to</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">ys</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">to</span><span class="o">.</span><span class="n">valid</span><span class="o">.</span><span class="n">xs</span><span class="p">,</span> <span class="n">to</span><span class="o">.</span><span class="n">valid</span><span class="o">.</span><span class="n">ys</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model_original</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="n">n_estimators_original</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth_original</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate_original</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="n">subsample</span><span class="p">,</span> 
                                  <span class="n">tree_method</span><span class="o">=</span><span class="s1">'gpu_hist'</span><span class="p">,</span> <span class="n">gpu_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">verbosity</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">enable_categorical</span><span class="o">=</span><span class="n">enable_categorical</span><span class="p">,</span> <span class="n">sampling_methods</span><span class="o">=</span><span class="n">sampling_methods</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="o">%</span><span class="k">time</span> xgb_model_original = model_original.fit(X_train, y_train)
<span class="n">xgb_preds_original</span> <span class="o">=</span> <span class="n">xgb_model_original</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_valid</span><span class="p">)</span>
<span class="n">xgb_preds_original</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>CPU times: user 2.24 s, sys: 246 ms, total: 2.49 s
Wall time: 2.47 s
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([[9.9646646e-01, 3.5335247e-03],
       [9.9989974e-01, 1.0023018e-04],
       [7.9425681e-01, 2.0574318e-01],
       ...,
       [9.8356736e-01, 1.6432654e-02],
       [4.6183008e-01, 5.3816992e-01],
       [9.6087682e-01, 3.9123163e-02]], dtype=float32)</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model_recommended</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="n">n_estimators_recommended</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth_recommended</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate_recommended</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="n">subsample</span><span class="p">,</span>
                                    <span class="n">tree_method</span><span class="o">=</span><span class="s1">'gpu_hist'</span><span class="p">,</span> <span class="n">gpu_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">verbosity</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">enable_categorical</span><span class="o">=</span><span class="n">enable_categorical</span><span class="p">,</span> <span class="n">sampling_methods</span><span class="o">=</span><span class="n">sampling_methods</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="o">%</span><span class="k">time</span> xgb_model_recommended = model_recommended.fit(X_train, y_train)
<span class="n">xgb_preds_recommended</span> <span class="o">=</span> <span class="n">xgb_model_recommended</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_valid</span><span class="p">)</span>
<span class="n">xgb_preds_recommended</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>CPU times: user 5.78 s, sys: 224 ms, total: 6.01 s
Wall time: 5.96 s
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([[9.9579751e-01, 4.2025056e-03],
       [9.9995327e-01, 4.6750749e-05],
       [7.4739861e-01, 2.5260136e-01],
       ...,
       [9.8437619e-01, 1.5623793e-02],
       [5.1231825e-01, 4.8768175e-01],
       [9.5093983e-01, 4.9060162e-02]], dtype=float32)</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model_max_e</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="n">n_estimators_max</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth_recommended</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate_recommended</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="n">subsample</span><span class="p">,</span> 
                                  <span class="n">tree_method</span><span class="o">=</span><span class="s1">'gpu_hist'</span><span class="p">,</span> <span class="n">gpu_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">verbosity</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">enable_categorical</span><span class="o">=</span><span class="n">enable_categorical</span><span class="p">,</span> <span class="n">sampling_methods</span><span class="o">=</span><span class="n">sampling_methods</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="o">%</span><span class="k">time</span> xgb_model_max_e = model_max_e.fit(X_train, y_train)
<span class="n">xgb_preds_max_e</span> <span class="o">=</span> <span class="n">xgb_model_max_e</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_valid</span><span class="p">)</span>
<span class="n">xgb_preds_max_e</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>CPU times: user 21.6 s, sys: 315 ms, total: 22 s
Wall time: 21.8 s
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([[9.9950659e-01, 4.9340865e-04],
       [9.9998891e-01, 1.1102343e-05],
       [8.0084145e-01, 1.9915855e-01],
       ...,
       [9.9154103e-01, 8.4589710e-03],
       [5.5457193e-01, 4.4542807e-01],
       [9.7199428e-01, 2.8005721e-02]], dtype=float32)</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">accuracy</span><span class="p">(</span><span class="n">tensor</span><span class="p">(</span><span class="n">xgb_preds_original</span><span class="p">),</span> <span class="n">tensor</span><span class="p">(</span><span class="n">y_valid</span><span class="p">)),</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">tensor</span><span class="p">(</span><span class="n">xgb_preds_recommended</span><span class="p">),</span> <span class="n">tensor</span><span class="p">(</span><span class="n">y_valid</span><span class="p">)),</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">tensor</span><span class="p">(</span><span class="n">xgb_preds_max_e</span><span class="p">),</span> <span class="n">tensor</span><span class="p">(</span><span class="n">y_valid</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(TensorBase(0.9254), TensorBase(0.9271), TensorBase(0.9255))</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_score</span><span class="o">=</span><span class="n">tensor</span><span class="p">(</span><span class="n">xgb_preds_original</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">]),</span> <span class="n">y_true</span><span class="o">=</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_valid</span><span class="p">)),</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_score</span><span class="o">=</span><span class="n">tensor</span><span class="p">(</span><span class="n">xgb_preds_recommended</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">]),</span> <span class="n">y_true</span><span class="o">=</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_valid</span><span class="p">)),</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_score</span><span class="o">=</span><span class="n">tensor</span><span class="p">(</span><span class="n">xgb_preds_max_e</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">]),</span> <span class="n">y_true</span><span class="o">=</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_valid</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(0.9651362820976437, 0.9657186586961237, 0.9643268132563287)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So as we can see, we get a slightly better performance both in accuracy and the <code>roc_auc_score</code> (used in Kaggle competition) when we use a lower <code>n_estimators</code> and tune together with <code>max_depth</code> and <code>learning_rate</code> parameters based on the recommendation, than either when we use just the defaults or the maximum value of <code>n_estimators</code></p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="redditech/team-fast-tabulous"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/team-fast-tabulous/kaggle/fastai/2020/07/02/Finding-Optimum-XGBoost-Parameters.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/team-fast-tabulous/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/team-fast-tabulous/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/team-fast-tabulous/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A fantastic team of FastAI learners, focused on a tabular data group project</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/redditech" title="redditech"><svg class="svg-icon grey"><use xlink:href="/team-fast-tabulous/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/redditech" title="redditech"><svg class="svg-icon grey"><use xlink:href="/team-fast-tabulous/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
